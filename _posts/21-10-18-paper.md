---
title: 💡 Paper
tags: paper 💡 🔥
mathjax_autoNumber: false
---

- **Self-training with combined unsupervised domain adaptation and semi-supervised learning**   
    (비지도 도메인 적응과 준지도 학습을 결합한 자가 학습)
- **Unsupervised domain adaptation with semi-supervised learning for semantic segmentation**   
    (의미론적 분할을 위해 반지도 학습 방법을 이용한 비지도 도메인 적응)
- Self-supervised learning with image based synthetic teacher model   
- Self-training 
- Synthetic 이미지 기반의 교사 모델을 이용한 Self-training
- Semi-supervised domain adaptation for semantic segmentation
- A Unified Approch to Semi-supervised learning and domain adaptation
- Self-training for Unsupervised domain adaptation on semantic segmentation
- Self-training with Semi-supervised domain adaptation for semantic segmentation
- Self-training that combines unsupervised domain adaptation and semi-supervised learning for semantic segmentation
- Unsupervised domain adaptation with semi-supervised methods for semantic segmentation
- 기존 UDA에 SSL 방법론 적용

<!--more-->

# 국문 요약 / 영문 요약

ImageNet의 데이터 공개를 시작으로 Data-driven 기반의 Supervised learning이 여러 visual task 분야에서 좋은 성과를 거두고 있다. AI모델을 만들기 위해서는 그에 맞는 양질의 데이터를 얼마나 많이 확보하느냐가 결국 모델의 성능을 좌우하게 되는데, 문제와 도메인에 맞는 annotation 데이터를 얻기 위한 비용은 매우 비싸다.

이 때 현실 세계와 비슷하게 구현된 시뮬레이션을 사용하면 가상의 공간에서 원하는 Ground Truth 데이터들을 비교적 쉽게 얻을 수 있다. 컴퓨터 그래픽과 계산식에 의하여 이미지, 센서, Bounding box, class 정보 등을 자동으로 생성하여 무한히 많은 synthetic 데이터들을 추출하고, 이 데이터들을 AI 학습에 이용할 수 있다면 human resource가 발생하는 annotation 작업 양이 현저히 줄어들 수 있을 것이다.

Synthetic 데이터로 학습한 AI 모델이 현실 세계에서 잘 동작하기 위해서는 실제 데이터와 synthetic 데이터 간 gap을 최소화 해야하는데, 이를 위해 Generative model, Adversarial learning, Pseudo-labeling 등의 기법들을 이용한 Unsupervised Domain Adaptation이 많이 연구 되고 있다. 하지만 대부분의 연구가 Domain shift를 줄이는 데에만 목적을 두고 있고 unlabeled 데이터 활용 방안에는 집중하고 있지 않다.

본 논문에서는 Synthetic 데이터를 이용한 Unsupervised Domain Adaptation (UDA)에 약간의 labeled 데이터가 있을 경우 사용할 수 있는 Semi-supervised learning (SSL)을 결합하여 domain 학습 데이터에 대한 의존성을 감소시키고, 갖고 있는 unlabeled 데이터까지 효과적으로 활용하는 UDAS (UDA + SSL) 모델을 제안한다. 

- 핵심어: Unsupervised Domain Adaptation, Semi-supervised learning, Self-training, Pseudo-Labeling, Consistency learning, Synthetic data

# 1. 서론

ImageNet의 데이터 공개를 시작으로 Data-driven 기반의 Supervised learning이 여러 visual task 분야에서 좋은 성과를 거뒀다. 검증된 좋은 모델들이 이미 존재하기 때문에, 특정 문제에 맞는 AI모델을 만들기 위해서는 그에 맞는 양질의 데이터를 얼마나 많이 확보하느냐가 결국 모델의 성능을 좌우하게 되었다.

하지만 특정 도메인의 task에 맞는 데이터를 확보하고, 그 데이터에 labeling 하는 것은 많은 시간과 비용을 필요로 한다. 이러한 데이터 의존적인 문제들을 해결하기 위해 데이터가 부족하더라도 효과적인 학습을 할 수 있는 다양한 연구들이 진행되고 있다. 유사한 task에서 학습된 pretrain 모델을 가져와 target task에 적용하는 Transfer Learning, Domain Adaptation, 그리고 labeled data가 적거나 없을 때 사용하는 Semi-Supervised Learning, Self-Supervised Learning 등의 연구들이 있다.

Labeling 작업의 비용이 큰 문제도 있지만 특정 상황에 대한 데이터 자체를 얻기 어려운 경우도 있다. 특히나 자율주행차량, 로보틱스 분야에서는 현실 세계에서 얻기 어려운 데이터들이 존재한다. 예를 들어 교차로 한가운데에 사람이 있는 데이터나, 로봇의 카메라 시점에서의 데이터 등은 쉽게 획득할 수가 없을 것이다.

이 경우 실제 세계를 digital twin 하여 시뮬레이션을 구축하는 방안이 있다. 시뮬레이션을 사용하게 되면 현실 세계에서 재현할 수 없는 상황들을 연출하고 환경을 자유자재로 변경하여 데이터들을 추출할 수가 있다. 컴퓨터 그래픽에 의해 RGB 이미지, depth 데이터, 객체의 bounding box, pixel 별 class, optical flow 등의 정확한 Ground Truth 데이터들이 자동으로 생성되도록 할 수 있기 때문에, 제작 비용이 저렴한 synthetic 데이터를 이용해 AI 학습에 사용할 수 있다.

Synthetic 데이터를 사용해 학습한 모델이 현실 세계에서 잘 동작하기 위해서는 실제 세계와 시뮬레이션 간의 격차가 작아야만 한다. 하지만 시뮬레이션을 최대한 사실적으로 만들기 위해서는 많은 노력이 들어가고, 실제 세계의 모든 데이터들을 모델링하기에는 쉽지 않은 일이다.

이러한 synthetic 데이터의 장단점을 이해하고 synthetic 데이터와 현실 데이터와의 domain distribution gap을 줄일 수 있는 Unsupervised Domain Adaptation 방법론들이 연구되고 있다. Generative 모델을 사용하거나 adversarial learning, self-training 등을 결합한 다양한 방법들이 제시되고 있는데, 대부분의 연구들이 Domain shift 자체를 감소시키는 연구에 집중하고 있다. Target 데이터가 없다는 가정하에 문제가 전개되기 때문에 좋은 성능이 나오기 어려워진다.

본 논문에서는 synthetic 데이터를 활용한 Unsupervised Domain Adaptation (UDA)을 수행하면서 소량의 labeled target 데이터와 다량의 unlabeled target데이터가 존재하는 경우 이를 효율적으로 사용할 수 있도록 Semi-supervised learning (SSL)을 결합한 UDAS (UDA + SSL) 프레임워크를 제안한다. 

# 2. 관련 연구

## 2.1. Semantic Segmentation

Semantic segmentation(SS) 는 이미지의 각 픽셀이 어느 클래스에 속하는 지 예측하는 것으로, 이미지의 전반적인 의미와 구조를 파악하여 더 깊이 있게 이해하는 작업이다. 픽셀 수준의 labeling이 수행되기 때문에 dense labeling task 라고 불리고, 이는 classification 이나 localization에 비해 어려운 문제로 분류된다.

SS 모델은 Supervised-learning 기반의 딥러닝 아키텍처가 나오면서 성능이 상당히 개선되었는데 최근에는 입력 공간 차원을 유지하며 전역의 semantic 을 추출하기 위해 encoder, decoder로 구성된 auto-encoder 구조가 많이 사용되고 있다. 대표적으로 FCN[1], PSPNet[2], DRN[3], DeepLab[4] 등과 같은 아키텍처들이 제안되었으며 좋은 성능을 보이고 있다.

하지만 대량의 labeled 데이터에 의존적이라 데이터셋을 확보하는 데에 많은 시간을 들여야 한다. 픽셀 별로 annotation 하는 것은 다른 visual task 에 비해 비용과 시간이 많이 소요되기 때문에 원하는 도메인의 데이터를 얻기가 쉽지 않다.

즉, synthetic 데이터에 대한 활용처가 높은 task로 분류될 수 있기 때문에 본 논문에서는 semantic segmentation을 downstream task로 설정하여 연구를 진행하고자 한다. 양이 많고 쉽게 얻을 수 있는 synthetic 데이터를 이용하여 지식을 추출하고 이를 활용하여 수동으로 labeling 하는 데이터의 양을 줄이고자 한다.

## 2.3. Unsupervised Domain Adaptation

일반적으로 labeled 데이터가 부족하면 pre-trained된 큰 모델을 이용하여 transfer learning을 수행하는 방법이 있다. Transfer learning을 하기 위해서도 해당 도메인에 맞는 labeled 데이터가 필요하지만 Domain Adaptation(DA)은 다른 도메인(Source)에서 배운 지식을 사용해 새로운 도메인(Target)의 labeled 데이터가 없는 상황에서도 모델의 성능을 올릴 수 있는 방법이다. Zero-shot learning, few-shot learning, self-supervised learning과 함께 sample-efficient learning의 한 유형이다.

![](/assets/images/21-09-24-paper-domain-adaptation.png)

Source 도메인과 target 도메인 간의 데이터 분포가 다르기 때문에 source 도메인에서 학습된 모델을 target 도메인에 직접적으로 transfer 하게 되면 domain shift(또는 domain gap)와 dataset bias에 의해 오차가 발생할 수 있다. 이 때 Domain Adaptation 방법론을 사용하면 source 와 target 도메인 간 격차를 줄이며 데이터 분포를 유사하게 학습할 수 있다. 

![](/assets/images/21-09-24-paper-adaptation-level.png)

DA 문제를 풀기 위한 주요 전략은 source 도메인과 target 도메인 사이의 격차를 줄임으로써 예측 모델의 성능 저하를 막는 방법이다. Input-level, feature-level, output-level에서 각각 adaptation 모듈을 추가하여 도메인 간 불일치성을 줄일 수 있다.

그 중 Unsupervised Domain Adaptation(UDA)는 Target 도메인의 unlabeled 데이터를 사용하기 위해 다른 도메인의 labeled 데이터를 사용하는 일종의 Semi-supervised learning의 변형이라고 볼 수 있다. Target 도메인의 labeled 데이터는 필요하지 않지만 충분한 양의 Unlabeled target 데이터가 필요하다.

Tzeng, Eric, et al.[5] 는 UDA에 GAN을 도입하여 adversarial adaptation을 처음 제안한 논문으로, feature level에 domain discriminator를 추가하고 adversarial loss를 사용하여 domain discrepancy를 최소화 하는 방법을 제안했다. 

Hoffman, Judy, et al.[6] 에서는 feature level에 adversarial learning을 그대로 사용하고, input-level에서 source 도메인 이미지를 target 이미지와 유사한 스타일로 생성하도록 generative 모델을 도입한 방법을 제안했다. Generative 모델을 도입함으로써 input 데이터의 숨겨진 확률분포를 찾아내고 유사한 확률분포의 데이터를 생성해 사용하는 방식이다. 

Tsai, Yi-Hsuan, et al.[7] 에서는 두 도메인의 이미지 스타일이 다르더라도 segmentation 결과에 포함되어 있는 공간 레이아웃, local context 등의 구조화된 특성은 유사하기 때문에 output-level에 adversarial learning을 도입하여 segmentation 예측 결과의 분포를 유사하게 학습하는 방법을 제안했다.

![](/assets/images/21-09-24-paper-da-methods.png)

정리하면 기존의 UDA연구는 source 도메인의 이미지를 target 이미지와 유사한 스타일로 생성하기 위해 generative 모델을 사용하거나, domain confusion을 위해 discriminator를 사용하거나, adversarial learning을 통해 두 도메인의 차이를 측정하는 방식으로 많이 연구되고 있다. 그 외에도 discrepancy-based methods, 분류기 불일치 분석, 엔트로피 최소화, 커리큘럼 학습 등의 방법론들을 변형하거나 적절이 섞어가며 다양한 방법들이 제안되고 있다. 

UDA 연구는 도메인 분포의 차별성에 따라 적용할 수 있는 도메인이 한정적일 수 있다. Supervised 정보가 전혀 없기 때문에 도메인 분포에 대해 파악하는 것이 보다 어려운 작업이 될 수 있고 특정 도메인 페어에 한해서만 잘 동작하는 모델이 될 수도 있다.

본 논문에서는 Unsupervised Domain Adaptation에 Semi-supervised learning을 결합하여 적은 Labeled target 데이터를 갖고 있는 경우에 쉽게 학습되고 실제 학습 데이터로 사용될 수 있도록 실용성이 강화된 방법을 제안한다.

## 2.3. Semi-supervised learning

Semi-supervised learning(SSL)은 소량의 labeled 데이터를 이용한 학습 방법이다. 소량의 labeled 데이터로만 학습하게 되면 overfitting과 같은 문제가 발생할 가능성이 크기 때문에 소량의 labeled 데이터와 대량의 unlabeled 데이터를 함께 사용하여 성능을 향상시키기 위해 semi-supervised learning이 사용된다.

![](/assets/images/21-09-24-paper-self-training.png)

Self-training은 가장 간단한 SSL방법으로, 소량의 labeled 데이터로 충분히 학습된 모델을 사용하여 unlabeled 데이터에 pseudo-labeling 을 하고, pseudo labeled 데이터와 labeled 데이터를 함께 학습에 사용하는 방식이다. Iteration을 반복할 수록 labeled 데이터가 늘어나고 확장된 데이터셋을 이용해 모델의 성능도 함께 향상될 수 있다. 하지만 처음 pseudo-labeling을 하는 모델의 성능이 좋지 않으면 error rate이 계속해서 증가하기 때문에 잘못된 방향으로 학습이 될 수 있다는 문제가 있다.

![](/assets/images/21-09-24-paper-entropy-minimization.png)

그래서 Unlabeled data를 학습시킬 때 pseudo-labeling을 그대로 사용하지 않고, pseudo-label의 maximum softmax probability를 기준으로 예측 확률의 confidence를 높이기 위해 entropy minimization을 사용한다. 주로 softmax temperature를 적용하여 unlabeled 데이터의 예측값을 계산할 때 decision boundary에서 멀어지도록 더 sharp하게 예측 확률값을 정한다. 단일 기법으로는 성능이 좋지 않지만 최신 연구에 많이 적용해 사용하고 있다. 

Xie, Qizhe, et al. [8]는 Teacher-student 기반의 Self-training으로, labeled 데이터로 학습된 teacher를 이용해 unlabeled 데이터에 pseudo-labeling 하고 이 데이터에 노이즈를 추가하여 student 모델을 학습하는 방법이다. Student 모델이 어느정도 학습이 되면 teacher 모델로 설정하고 다시 pseudo labeling을 수행하게 만들어 반복적인 학습 파이프라인을 구성했고 이를 통해 SOTA를 기록할 정도로 좋은 성능을 보인 연구이다.

Pham, Hieu, et al. [9]은 Teacher 모델과 Student 모델을 동시에 학습하는 방법으로, Teacher 모델이 잘 학습되어 있지 않더라도 student 모델과 유기적으로 학습할 수 있도록 Co-training 방식을 제안하였다.

최근 많이 사용되는 SSL 접근법은 Consistency regularization을 사용하는 것으로, unlabeled 데이터에 노이즈를 추가하더라도 예측 분포는 유지된다는 아이디어에 기안한 방법이다. 노이즈가 없는 원본 데이터와 노이즈가 주입된 데이터를 동일한 예측 분포로 학습하는 것이 consistency regularization의 목표이다.

Miyato, Takeru, et al. [10]은 입력 데이터의 adversarial transformation을 생성하고 예측 결과 간의 KL-divergence를 측정하여 학습시키는 방법을 제안했다. 

Xie, Qizhe, et al. [11]은 노이즈 대신 최신 augmentation 기법인 AutoAugment를 사용하여 예측 결과 간 KL-divergence를 loss로 사용한 방법으로, 적은 labeled 데이터만으로 supervised learning을 뛰어넘는 성능을 보였다.

그리고 Berthelot, David, et al. [12], Sohn, Kihyuk, et al. [13] 등 Pseudo-labeling과 함께 augmentation, entropy minimization, consistency regularization을 포함한 다양한 기법을 함께 사용하는 holistic methods 연구들도 나오고 있고 SSL 분야에서 SOTA를 기록하고 있다.
 
본 논문에서는 Unlabeled target 데이터에 K개의 augmentation 데이터를 생성하고, 가장 큰 confidence를 갖는 prediction 값만 취하는 maximum confidence map을 통해 신뢰도 높은 pseudo-labeling을 수행한다.

# 3. 제안 방법

## 3.1. 개요

본 논문에서는 Synthetic 데이터를 이용한 Unsupervised Domain Adaptation (UDA)과 소량의 labeled target 데이터를 사용하는 Semi-supervised learning (SSL)을 결합하여, 도메인 학습 데이터에 대한 의존도를 낮추고 semantic segmentation의 성능을 높이는 UDAS (UDA + SSL) 모델을 제안한다.

![](/assets/images/21-09-24-paper-paper-overview.png)

UDAS의 전체적인 프레임워크는 그림5과 같다. 크게 Domain Adaptation 을 위한 Teacher모델과 Teacher 모델로부터 Knowledge Distillation 을 받아 실제 task를 수행하는 Student 모델 두 부분으로 나눌 수 있다.

기본적인 구조는 UDA를 기반으로 하고 있어서 target labeled 데이터가 없더라도 target task를 수행할 수 있고, target labeled 데이터가 늘어남에 따라 Semi-Domain Adaptation 구조로 동작하면서 target task의 성능이 향상된다. 이 때 target labeled 데이터를 간접적으로 사용함으로써 target 데이터에 대한 의존성이 높아지지 않도록 한다. 

DA를 통해 학습된 모델을 이용하여 unlabeled target 데이터에 pseudo-labeling을 해주고 실제 target task를 수행하는 모델은 pseudo-labeling 된 데이터를 이용하여 Supervised learning 구조로 학습함으로써 target 도메인에 맞춰 fine-tuning 한다.

본 논문의 contribution은 다음과 같다.
    • Unsupervised Domain Adaptation와 Semi-supervised learning 을 결합한 모델 구조 제안
    • Labeled target 데이터 양을 줄였을 때 SL보다 안정적인 성능
    • 기존 UDA SOTA 대비 높은 mIoU 달성

## 3.2. Style Transfer

시각적으로 봤을 때 큰 차이가 있는 데이터는 그만큼 큰 도메인 gap 이 있을 것이라고 가정할 수 있다. Input-level에서의 도메인 gap을 줄이기 위해 source 도메인의 image appearance가 target domain과 유사하도록 생성해낸다. 대표적인 image-to-image translation 방법인 CycleGAN을 이용하여 target domain 스타일의 이미지로 변형시켜 domain adaptation 모델의 source 데이터로 사용하고자 한다.

$$ L(G, F, D_x, D_Y) = L_{GAN}(G, D_Y, X, Y) + L_{GAN}(F, D_x, Y, X) + \lambda L_{cyc}(G, F) $$

$$ G^*, F^* = arg min_{G, F} max_{D_x, D_y} L(G, F, D_x, D_y) $$

X가 source domain 샘플, Y가 target domain 샘플이라고 했을 때 XY로 가는 GAN의 adversarial loss와 YX로 가는 GAN의 adversarial loss에 각각 원본으로 복구하는 cycle consistency loss를 더해준 값이 총 Loss가 되고, 이 Loss를 최소화 하는 방향으로 G: XY 와 F: YX 를 학습시킨다.

학습 시에는 G와 F가 함께 학습되는 구조이지만, 전체 실험에서는 F: YX 모델은 사용하지 않고 G: XY 만 사용하기 때문에 GAN 부분만 도식화하면 아래와 같다.

![](/assets/images/21-09-24-paper-gan.png)

Source domain과 target domain의 분포를 구분할 수 없도록 학습시키는 방법이기 때문에 domain간 gap이 줄었을 것이라고 기대할 수 있고, G: XY 모델을 통해 생성된 데이터는 target domain분포에 가까운 형태가 될 수 있다. Source 도메인 데이터를 그대로 사용하는 것 보다 Target domain의 스타일에 맞게 이미지를 재생성해서 target task 모델에 적용하면 성능이 좋아지는 것을 볼 수 있었는데, 이에 관한 실험 결과는 4.3.에서 확인할 수 있다.

## 3.3. Unsupervised Domain Adaptation

UDA 모듈은 Adversarial Generative Model 구조로 구성된다. 3.2.에서 Style Transfer 된 이미지가 Source 이미지로 대체되어 입력으로 들어간다.

Source 이미지와 source labels을 이용하여 source 도메인 기반으로 Segmentation 을 학습한다. Segmentation loss는 일반적으로 많이 사용되는 pixel-wise cross-entropy loss를 사용한다.

![](/assets/images/21-09-24-paper-uda-module.png)

$$L_{seg} = -\sum_{h=1}^{H}\sum_{w=1}^{W}\sum_{c=1}^{C} y_s^i log(P(x_s^i)) $$

추가적으로 모델 학습 시에는 softmax temperature와 confidence-based masking을 추가하여 보다 정확한 예측을 하도록 유도한다.

학습하는 Segmentation 모델은 source 도메인으로부터 학습되기 때문에 target 도메인에서는 잘 동작하지 않는다. Target 도메인의 distribution이 source 도메인과 유사해지도록 하기 위해 prediction 결과에 adversarial loss를 추가한다. 이를 통해 source 도메인에서 학습한 지식들이 target 도메인 영역으로 전달되고 output-level에서도 domain adaptation이 수행된다.

$$ L_{adv} = \mathbb{E}[ log(D_p(P(x_s^i))) + log (1 - D_p(P(x_t^j))) ]
 $$

Discriminator는 prediction의 결과를 보고 source domain의 prediction인지 target domain의 prediction인지 구분하기 위해 노력할 것이다.


$$ L_{total} = L_{seg} + \lambda_{adv} L_{adv}$$

UDA로 동작하는 경우 전체 loss는 segmentation loss와 adversarial loss를 더한 값이다. Adversarial loss의 가중치를 조정하기 adversarial loss term에는 하이퍼파라미터 $\lambda_{adv}$ 를 곱해준다.

## 3.4. Combined Augmentation

Labeled target 데이터가 없는 경우에는 3.3.의 서술 내용처럼 UDA 구조로 동작할 수 있지만 소량의 labeled 데이터가 있는 경우 target 도메인에 대한 정보를 토대로 모델이 더 향상될 수 있다.

Labeled target 데이터를 Supervised learning 형태로 그대로 이용하게 되면 source 도메인 데이터를 adaptation 하는 정보와 상충될 수 있고, 추후 source 도메인 데이터가 추가로 확보되어 UDA를 진행한다고 할 때 target 도메인 데이터의 지식이 너무 강해서 추가적인 학습이 어려울 수 있다.

이러한 문제를 방지하기 위해 본 연구에서는 도메인과 독립적으로 모델을 학습시키기 위해 source 도메인 샘플과 target 도메인 샘플을 결합하여 학습 데이터로 사용한다. 

Semantic segmentation은 각 픽셀의 예측값과 더불어 주변 pixel에 대한 지역적인 예측 정보도 필요한데, 한 이미지에 source 도메인 이미지와 target 도메인 이미지가 결합되어 있으면 도메인과 관계 없이 이미지 자체의 representation을 학습할 수 있을 것이다. 이를 위해 기존의 CutMix[14]의 형태로 두 도메인의 이미지를 임의로 결합시켜 학습 데이터로 사용한다.

CutMix는 두 이미지를 섞어 하나의 이미지에 표현하는 Data augmentation 방법이다.  Regularization과 Localization 성능을 높이기 위한 간단한 방법으로, 모델이 이미지의 전체적인 영역을 보고 학습할 수 있도록 유도할 수 있다.

$$ r_x \sim Unif \{ 0, W \}, r_w = W \sqrt {1- \lambda}
\\
r_y \sim Unif \{ 0, H \}, r_h = H \sqrt {1- \lambda}
\\
B = \{ r_x , r_y, r_w, r_h \} $$

Cutmix 데이터 생성을 위해 매 배치마다 랜덤으로 bounding box B 사이즈를 결정하고, B 사이즈에 맞게 binary 마스크 M을 생성한다.  

Source 도메인의 샘플 $x_s$ 에서 M 영역만큼 지우고, 그 부분에 target 도메인 샘플 $x_t$를 채운다. 마찬가지로 source 도메인 샘플에 대한 label 데이터인 $y_s$ 에서도 (0, 1) 사이의 uniform distribution인 lambda를 통해 와 페어 되는 labeled cutmix 데이터를 만들어낸다.

![](/assets/images/21-09-24-paper-cutmix-data.png)

이렇게 생성된 cutmix 데이터에 augmentation을 취해 임의의 밝기, 대조, color jitter 들을 추가한 뒤 기존에 학습에 사용하던 source 이미지와 함께 입력 데이터로 segmentation 모델을 학습시킨다. Domain Randomization논문들[15][16][17]에 의거하면 랜덤하게 생성된 synthetic 데이터가 실제 세계에 적용할 때 도움이 된다는 것을 실험적으로 알 수 있다.

![](/assets/images/21-09-24-paper-cutmix-flow.png)

Semi-Domain adaptation 구조로 동작할 때 segmentation loss는 Source 도메인 이미지에 대한 Cross-entropy loss와 CutMix 이미지에 대한 Cross-entropy loss를 더한 값이다. CutmMix 데이터에 대한 가중치를 위하여 CutMix Loss term에는 하이퍼파라미터  를 곱해준다. 전체 Loss는 UDA에서 cutmix의 CE loss가 추가된 값이다.

$ L_{total} = L_seg + \lambda_{cm} L_{cm} + \lambda_{adv} L_{adv} $

## 3.5. Knowledge Distillation

Domain Adaptation 모듈을 통해 얻어진 Task model 이 있지만 Target 도메인에서의 Fine-tuning을 위해 Unlabeled Target 데이터에 Pseudo-labeling을 하여 학습 데이터로 사용한다. 앞서 학습한 segmentation모델은 이제 Teacher 모델이 되어 target 도메인에 적용할 모델인 Student 모델에게 Knowledge Distillation 하게 된다.

Teacher 모델이 예측한 결과를 그대로 Pseudo-labels로 사용하게 되면 teacher 모델이 갖고 있는 오차 정보가 그대로 전파되기 때문에 Sharpening predictions 해주는 작업이 필요하다. 

![](/assets/images/21-09-24-paper-pseudo-labeling.png)

Pseudo-labeling 방법은 다음과 같다. Unlabeled target 데이터에 K개의 augmentation 데이터를 생성하고 각각 teacher 모델을 통해 prediction probability를 얻는다. Confidence map을 비교했을 때 가장 높은 confidence의 예측값을 사용하기 위해 maximum confidence map을 구성하고 이를 통해 만들어진 pseudo-label 중에서 특정 confidence 값 이상의 예측 데이터만 학습 데이터로 사용한다.

![](/assets/images/21-09-24-paper-student-model.png)

Unlabeled 데이터에 pseudo-labeling이 된 이후에는 source 도메인 데이터는 사용하지 않는다. Pseudo-labeling된 데이터들을 이용해 Student 모델을 Supervised learning 방식으로 학습하고, Student 모델이 target 도메인에 대하여 fine-tuning 되어 더 나은 성능에 도달하게 되면 기존의 pseudo-labeling 데이터를 업데이트 해준다. 그리고 재생성된 pseudo-labeled 데이터를 이용하여 Student 모델이 견고하고 신뢰도 높은 방향으로 재학습되도록 구성하여 반복적인 학습이 이루어지도록 한다.

# 4. 실험 및 결과

## 4.1. 실험 설계

UDAS프레임워크를 코드는 Python 3.8과 Pytorch 1.9.0 버전을 기준으로 구현되었고, 모든 실험은 메모리 11GB의 1080ti 2개가 장착된 Ubuntu 20.04 환경에서 진행되었다

### 4.1.1. 데이터셋

![](/assets/images/21-09-24-paper-dataset.png)dataset

Cityscapes[18]는 유럽 50개 도시의 거리에서 차량 시점으로 캡쳐한 2048x1024 픽셀의 이미지 데이터셋이다. 2,975개의 training set, 500개의 validation set, 1,595개의 test set으로 총 5,000개의 이미지 데이터와 pixel-wise semantic label로 구성되어 있다. 추가로 label 되어 있지 않은 19,998 개의 raw 이미지 데이터도 제공된다.

GTA5[19]는 도시 운전을 위한 대규모 synthetic 데이터셋 중 하나이다. 사실적인 그래픽을 보여주는 GTA V 게임을 통해 렌더링 되었고 미국 스타일 도시에서의 자동차 관점에서 촬영되었다. 24,966개의 1914x1052 픽셀의 이미지와 pixel-wise semantic label로 구성되어 있다. Label은 Cityspcaes의 클래스에 맞게 19개 클래스로 재매핑 할 수 있다.

Cityscapes와 GTA5는 시각적인 appearance 차이가 존재하지만 비슷한 차량의 높이에서 도로 상황을 캡쳐한 데이터라는 점에서 구조가 유사하다고 볼 수 있다. GTA5 데이터가 Cityscapes 데이터 양에 비해 약 5배 가량 많기 때문에 GTA5의 데이터를 최대한 활용하여 이미지의 전반적인 의미와 구조를 학습하고, Cityscapes 환경에서 잘 동작하도록 Cityscapes의 unlabeled 데이터까지 적절히 이용한다.

|  Dataset   |   Type    | # of Labeled Data | # of Raw Data | # of Classes |
| :--------: | :-------: | :---------------: | :-----------: | :----------: |
| Cityscapes |   Real    |       5,000       |    19,998     |      19      |
|    GTA     | Synthetic |      24,966       |       -       |      19      |

### 4.1.2. 평가 Metric

평가 metric으로는 Semantic segmentation 평가 시에 주로 사용되는 클래스 별 IoU(Intersection over Union) 와 mIoU(Mean Intersection over Union)를 사용한다. $TP_i$, $FP_I$, $FN_i$ 는 특정 클래스 $i$의 True Positive, False Positive, False Negative 이고 $N$은 클래스의 갯수이다.

$$ {IoU}_i = \left( \frac{TP_i}{FP_i + FN_i + TP_i} \right) $$

$$ mIoU = \sum_{i=1}^N \left( \frac{IoU_i}{N} \right ) $$

## 4.2. Implementation Details

대부분의 UDA SOTA 모델은 Resnet101을 backbone으로 한 DeepLab v2 모델을 사용하고 source 도메인 이미지 사이즈는 (1280, 620), target 도메인 이미지 사이즈는 (1024, 512)를 사용한다. 아래 실험에서는 GPU 메모리 부족으로 모델은 동일하게 사용하나 source 도메인 이미지 사이즈는 (640, 360), target 도메인 이미지 사이즈는 (640, 360)으로 resize 하여 실험한다.

### 4.2.1. Semantic Segmentation

우선 일반적인 Supervised-learning 방식의 Semantic segmentation 모델과의 성능 비교를 위해 SOTA 모델인 FCN과 DeepLab v3+ 성능 비교를 수행하였다. Backbone은 동일하게 Resnet-50으로 설정하였고 Cityscapes의 Labeled 이미지들로 Supervised-learning 방식으로 학습시켰다.

![](/assets/images/21-09-24-paper-sl-learning-result.png)

|    model    | road | sidewalk | building | wall | fence | pole | traffic light | traffic sign | vegetation | terrain | sky  | person | rider | car  | truck | bus  | train | motorcycle | bicycle | Pixel Accuracy | mIoU  |
| :---------: | :--: | :------: | :------: | :--: | :---: | :--: | :-----------: | :----------: | :--------: | :-----: | :--: | :----: | :---: | :--: | :---: | :--: | :---: | :--------: | :-----: | :------------: | :---: |
|     FCN     | 0.97 |   0.81   |   0.9    | 0.26 | 0.45  | 0.54 |     0.67      |     0.75     |    0.91    |  0.53   | 0.93 |  0.78  | 0.57  | 0.93 | 0.48  | 0.7  | 0.36  |    0.56    |  0.75   |     94.693     | 68.03 |
| DeepLab v3+ | 0.98 |   0.84   |   0.92   | 0.56 | 0.59  | 0.64 |      0.7      |     0.78     |    0.92    |  0.64   | 0.95 |  0.81  | 0.63  | 0.94 | 0.69  | 0.76 | 0.43  |    0.63    |  0.76   |     95.826     | 74.97 |

사용하는 모델과 Backbone에 따라 mIoU 값 차이가 있으나, Supervised learning으로 모델을 학습시켰을 때 mIoU가 약 70 정도 나오는 것으로 확인했다. 

많은 domain adaptation 논문들이 Resnet101을 backbone으로 한 DeepLab v2를 Semantic segmentation 모델로 사용하고 있기 때문에, 동등한 비교를 위해 이후 실험에서는 Atrous Spatial Pyramid Pooling(ASPP) 모듈을 포함한 DeepLab v2 모델을 task 모델로 설정하여 진행한다. Cityscapes 데이터셋을 이용해 학습한 Resnet101 + DeepLab v2 모델의 기본 성능은 아래와 같다.

| Model                  | road | walk | build. | wall | fence | pole | light | sign | vege. | terr. | sky  | persn | rider | car  | truck | bus | train | mcyc | Bike | mIoU |
| :--------------------- | :--- | :--- | :----- | :--- | :---- | :--- | :---- | :--- | :---- | :---- | :--- | :---- | :---- | :--- | :---- | :-- | :---- | :--- | :--- | :--- |
| DeepLab v2 (Resnet101) | 96.7 | 76.2 | 88.6   | 47.2 | 47.6  | 45.1 | 57.9  | 67.6 | 89.1  | 58.2  | 90.4 | 70    | 54.8  | 92.2 | 71.8  | 78  | 56.6  | 56.7 | 67.8 | 69.1 |

### 4.2.2. Style Transfer

Source 이미지에서 Target 이미지 스타일로 재생성된 데이터가 domain gap을 줄이는데 얼마나 기여하는지 확인하기 위한 실험으로, 동일한 모델을 GTA 데이터와, Cityscapes 스타일로 재생성된 GTA->Cityscapes 데이터에 적용했을 때 mIoU를 비교한다.

![](/assets/images/21-09-24-paper-style-transfer.png)

GTA 이미지와 Cityscapes 이미지 간의 시각적 차이를 줄이기 위해 Labeled 데이터 없이 Cityscapes 이미지와 GTA 이미지만을 이용하여 CycleGAN을 학습하고, GTA이미지를 Cityscapes 스타일의 이미지로 재생성한 결과이다.

![](/assets/images/21-09-24-paper-style-transfer-prediction.png)

4.2.1장에서 Cityscapes 데이터로만 학습시킨 DeepLab v3+ 모델을 이용하여 GTA 샘플과 GTA->Cityscapes 샘플에 각각 예측한 결과이다. 

GTA5 샘플에 대한 예측 결과는 Cityscapes와 GTA5의 데이터 자체의 domain gap에 의해 Cityscapes 데이터에 적용했을 때 보다 더 많은 예측 오차가 발생하는 것을 볼 수 있다. 반면에 GTA5->Cityscapes 데이터를 예측한 결과는 GTA5 데이터에 비해 적은 오차를 보이고 있다.

| Data             | road | walk | building | wall | fence | pole | light | sign | vege. | terrain | sky  | person | rider | car  | truck | bus  | train | mCycle | bicycle | mIoU |
| :--------------- | :--- | :--- | :------- | :--- | :---- | :--- | :---- | :--- | :---- | :------ | :--- | :----- | :---- | :--- | :---- | :--- | :---- | :----- | :------ | :--- |
| GTA              | 70.2 | 19.3 | 60.4     | 23.3 | 13.4  | 25.3 | 31.8  | 17   | 53.5  | 24.3    | 85.6 | 41.2   | 1.7   | 55.4 | 19.1  | 16.8 | 2.4   | 6.4    | 5.4     | 30.1 |
| GTA → Cityscapes | 80.7 | 26.8 | 6.76     | 29.3 | 13.6  | 30.8 | 26.2  | 21.1 | 61.2  | 33.5    | 76.2 | 47.6   | 27    | 63   | 37.1  | 15.8 | 4.9   | 18.2   | 7.4     | 36.2 |
| Cityscapes       | 98   | 84.5 | 92.3     | 56   | 59.1  | 64.6 | 70.2  | 78.9 | 92.4  | 64      | 95   | 81.6   | 63.2  | 94.7 | 69    | 76.9 | 43.3  | 63.4   | 76.5    | 74.9 |

테스트에 사용한 데이터의 갯수는 24,966로 동일하게 설정하여 mIoU를 비교했을 때, GTA5 이미지를 그대로 사용하는 것에 비해 generative 모델을 통해 GTACityscapes 스타일로 생성된 이미지를 입력 데이터로 사용했을 때 mIoU가 +6% 증가하였고, 이에 따라 target domain과의 gap이 일부 감소한 것을 알 수 있다.

![](/assets/images/21-09-24-paper-2021-11-04-15-20-27.png)

Style transfer 모듈과 UDA 모듈을 end-to-end로 학습시키면 수렴하기가 더 어려워지기 때문에 GTACityscapes 데이터를 GTA 데이터 대신 source 도메인으로 설정하여 사용한다.

### 4.2.3. Combined Augmentation

![](/assets/images/21-09-24-paper-combined-augmentation.png)

그림7은 Cityscapes 이미지와 GTACityscapes 이미지를 혼합하기 위하여 임의 사이즈의 Mask를 만들어 특정 영역을 추출해내고, 다른 이미지에 결합하여 만든 새로운 입력 데이터이다. 해당 데이터에 대해 예측 결과를 봤을 때 이미지가 어색하게 결합이 되어 있더라도 모델이 일반화 되어 학습되었기 때문에 큰 오차 없이 예측하는 것을 볼 수 있다.

시각적인 부분에서 한단계 domain gap을 줄였지만 외관상으로 보이는 gap 이외에도 실제로 모델이 이미지를 이해하기 위해서는 보이지 않는 구조적인 부분 또한 학습해야 할 필요가 있다. 이를 위해 도메인을 혼합한 cutmix 데이터를 추가함으로써 구조적으로 이미지를 학습할 수 있도록 구성하였고, 이는 domain confusion을 야기하여 도메인 지식과 관계없이 task에 집중하여 학습할 수 있다.

### 4.2.4. Pseudo Labeling

![](/assets/images/21-09-24-paper-pseudo-labeling-result.png)

K=3으로 설정하고 Unlabeled target 데이터에 대해augmentation 데이터와 예측값의 confidence map을 얻어내고, maximum confidence map과 confidence-based masking을 통해 만들어진 pseudo-labeling 과정이다. Augmentation은 AutoAugment [20]를 사용했다. 

![](/assets/images/21-09-24-paper-pseudo-labeling-data.png)

이렇게 생성된 pseudo-labeling 데이터들을 이용해 student segmentation 모델을 학습시킨다.

## 4.3. Quantitative Comparison

### 4.3.1. Performance

![](/assets/images/21-09-24-paper-performance.png)

DA와 SSL 기법들이 적용됨에 따라 Target 도메인인 Cityscapes 데이터 대상으로 semantic segmentation예측을 했을 때 결과가 좋아짐을 볼 수 있다. 
Domain shift에 대한 고려없이 직접적으로 예측했을 때, 모델의 성능과 관계없이 많은 오차가 발생한다. 
Style Transfer를 적용한 데이터에는 어느 정도 semantic 구조를 파악한 것처럼 보이지만 여전히 많은 노이즈가 있다. 
Style Transfer 데이터를 source 데이터로 하여 UDA를 적용한 모델은 일반적인 UDA SOTA 수준의 성능을 보이고 있다. 
약간의 Labeled target 데이터가 있다는 가정하에 SSL 모듈을 도입하게 되면 target 도메인에 대한 정보들을 토대로 성능이 훨씬 향상된 것을 볼 수 있다. 

| Model               | road | walk | building | wall | fence | pole | light | sign | vege. | terrain | sky  | person | rider | car  | truck | bus  | train | mCycle | bicycle | mIoU |
| :------------------ | :--- | :--- | :------- | :--- | :---- | :--- | :---- | :--- | :---- | :------ | :--- | :----- | :---- | :--- | :---- | :--- | :---- | :----- | :------ | :--- |
| No Adaptation       | 83.9 | 10.2 | 74.3     | 4.4  | 3     | 10.3 | 11.8  | 11.4 | 84.2  | 15.8    | 68.7 | 39     | 0.1   | 79.1 | 26.1  | 17.3 | 0     | 2.7    | 0       | 28.5 |
| ST (Style Transfer) | 75.7 | 16.7 | 77.2     | 12.5 | 21    | 25.4 | 30    | 20.1 | 81.3  | 24.6    | 70.3 | 53.7   | 26.4  | 49.9 | 17.1  | 25.8 | 6.4   | 25.2   | 36      | 36.6 |
| ST + UDA            | 89.4 | 26.2 | 82.7     | 25.8 | 22.9  | 36.1 | 40.4  | 38.8 | 84.1  | 37.5    | 83.5 | 59     | 26.7  | 83.6 | 28.5  | 36.6 | 0.1   | 14.6   | 26.7    | 44.4 |
| ST + UDA + SSL      | 94.7 | 67.6 | 83.6     | 38.2 | 27.9  | 41.6 | 44.7  | 50   | 84.4  | 41.9    | 86.2 | 66.2   | 43.8  | 89.5 | 68.6  | 52.8 | 36.3  | 39.2   | 53.1    | 58.4 |

각 단계에 대한 클래스 별 IoU와 mIoU는 다음과 같다. SSL을 도입한 후 급격하게 성능이 향상되었고, Knowledge Distillation를 추가하여 불확실한 정보로 Supervised learning을 수행하더라도 성능이 상승한 것을 확인할 수 있다.

| Model           | road | walk | building | wall | fence | pole | light | sign | vege. | terrain | sky  | person | rider | car  | truck | bus  | train | mCycle | bicycle | mIoU |
| :-------------- | :--- | :--- | :------- | :--- | :---- | :--- | :---- | :--- | :---- | :------ | :--- | :----- | :---- | :--- | :---- | :--- | :---- | :----- | :------ | :--- |
| Cycada          | 79.1 | 33.1 | 77.9     | 23.4 | 17.3  | 32.1 | 33.3  | 31.8 | 81.5  | 26.7    | 69   | 62.8   | 14.7  | 74.5 | 20.9  | 25.6 | 6.9   | 18.8   | 20.4    | 39.5 |
| AdaptSegNet     | 86.5 | 36   | 79.9     | 23.4 | 23.3  | 23.9 | 35.2  | 14.8 | 83.4  | 33.3    | 75.6 | 58.5   | 27.6  | 73.7 | 32.5  | 35.4 | 3.9   | 30.1   | 28.1    | 42.4 |
| AdvEnt          | 89.4 | 33.1 | 81       | 26.6 | 26.8  | 27.2 | 33.5  | 24.7 | 83.9  | 36.7    | 78.8 | 58.7   | 30.5  | 84.8 | 38.5  | 44.5 | 1.7   | 31.6   | 32.4    | 45.5 |
| Seg-Uncertainty | 90.5 | 35   | 84.6     | 34.3 | 24    | 36.8 | 44.1  | 42.7 | 84.5  | 33.6    | 82.5 | 63.1   | 34.4  | 85.8 | 32.9  | 38.2 | 2     | 27.1   | 41.8    | 48.3 |
| Ours            | 94.7 | 67.6 | 83.6     | 38.2 | 27.9  | 41.6 | 44.7  | 50   | 84.4  | 41.9    | 86.2 | 66.2   | 43.8  | 89.5 | 68.6  | 52.8 | 36.3  | 39.2   | 53.1    | 58.4 |

UDA SOTA 모델들과 클래스 별 IoU와 mIoU를 비교한 결과이다. UDAS 모델이 월등하게 좋은 성능을 내고있다. 

### 4.3.2. Data Dependency

Supervised Learning(SL) 방법론의 경우 학습을 위해서는 많은 양의 Labeled 데이터가 필요하다. 적은 labeled 데이터로 학습시키게 되면 학습 데이터에 overfitting 되는 경향이 있다. 데이터가 적은 상황에서 제안 모델이 SL 방법에 비해 얼마나 안정적인 성능을 보이는지 비교하기 위한 Data Dependency실험이다.

Labeled target 이미지는 Cityscapes 데이터이고, 총 labeled 데이터 2,979장을 모두 사용하여 학습했을 때와 labeled 학습 데이터 양을 1000, 500, 100, 0으로 감소시키며 mIoU를 비교한 결과이다. 

| Model | 2975 | 1000         | 500           | 100           | 0             |
| :---- | :--- | :----------- | :------------ | :------------ | :------------ |
| SL    | 69.1 | 61.2 (-7.9%) | 55.4 (-13.7%) | 41.6 (-27.5%) | -             |
| UDAS  | 56.4 | 51.9 (-4.5%) | 52.8 (-3.6%)  | 49.2 (-7.2%)  | 43.9 (-12.5%) |

Labeled target 이미지가 2975장인 경우 UDAS모델이 -12.7% 의 성능을 보이고 있고, 1000장인 경우 -9.3%, 500장인 경우 -2.6%, 그리고 100장인 경우 +7.6%로, labeled target 데이터가 적어질 수록 UDAS가 SL 보다 좋은 성능을 보이는 것을 알 수 있다.

![](/assets/images/21-09-24-paper-udas-sl.png)

Labeled target 이미지에 따른 mIoU를 그래프로 시각화한 것이다. SL 모델은 labeled 데이터양이 적어짐에 따라 급격하게 성능이 하락하지만 UDAS 모델은 labeled 데이터양이 적어지더라도 안정적인 성능을 보이는 것을 알 수 있다. 이를 통해 제안한 UDAS 모델이 도메인 데이터에 대한 의존성이 낮아졌다고 미루어 짐작해볼 수 있다.

무엇보다 데이터가 전혀 존재하는 않는 경우, 제안 모델이 Source 도메인 데이터만으로 adaptation 하는 UDA 구조로 동작하기 때문에 labeled 데이터 없이는 무용지물이 되는 SL 모델에 비해 경쟁력 있다고 할 수 있다.

# 5. 결론

AI 학습에 있어서 항상 큰 장애물이 되는 데이터 부족 문제를 해결하기 위해 연구를 진행하였다. 컴퓨터 그래픽이 발전함에 따라 점차적으로 현실세계와 가상세계의 시각적 경계가 모호해지고 있기 때문에, 무한한 데이터를 창출해낼 수 있는 synthetic 데이터에 대한 활용도가 높아질 것이다. 또한 AI모델이 커지면서 점점 더 많은 데이터가 요구되는데 항상 데이터에 의존적인 AI 모델에서 벗어나 스스로 규칙을 찾아내는 모델을 만들고자 했다. 

본 논문에서는 Synthetic데이터를 이용하여 UDA와 SSL을 결합한 구조를 제안하였고 semantic segmentation 문제에 맞는 augmentation, pseudo-labeling 기법을 추가하여 데이터가 부족한 상황에서도 안정적인 성능을 보였다. 

UDA 방법론과 synthetic 데이터의 질이 향상함에 따라 제안한 UDAS 구조의 성능은 그와 비례하여 향상될 수 있을 것으로 예상한다.

# 6. 참고 문헌

[1] Long, Jonathan, Evan Shelhamer, and Trevor Darrell. "Fully convolutional networks for semantic segmentation." Proceedings of the IEEE conference on computer vision and pattern recognition. 2015.
[2] Zhao, Hengshuang, et al. "Pyramid scene parsing network." Proceedings of the IEEE conference on computer vision and pattern recognition. 2017.
[3] Yu, Fisher, Vladlen Koltun, and Thomas Funkhouser. "Dilated residual networks." Proceedings of the IEEE conference on computer vision and pattern recognition. 2017.
[4] Chen, Liang-Chieh, et al. "Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs." IEEE transactions on pattern analysis and machine intelligence 40.4 (2017): 834-848.
[5] Tzeng, Eric, et al. "Adversarial discriminative domain adaptation." Proceedings of the IEEE conference on computer vision and pattern recognition. 2017.
[6] Hoffman, Judy, et al. "Cycada: Cycle-consistent adversarial domain adaptation." International conference on machine learning. PMLR, 2018.
[7] Tsai, Yi-Hsuan, et al. "Learning to adapt structured output space for semantic segmentation." Proceedings of the IEEE conference on computer vision and pattern recognition. 2018.
[8] Xie, Qizhe, et al. "Self-training with noisy student improves imagenet classification." Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020.
[9] Pham, Hieu, et al. "Meta pseudo labels." Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021.
[10] Miyato, Takeru, et al. "Virtual adversarial training: a regularization method for supervised and semi-supervised learning." IEEE transactions on pattern analysis and machine intelligence 41.8 (2018): 1979-1993.
[11] Xie, Qizhe, et al. "Unsupervised data augmentation for consistency training." arXiv preprint arXiv:1904.12848 (2019).
[12] Berthelot, David, et al. "Mixmatch: A holistic approach to semi-supervised learning." arXiv preprint arXiv:1905.02249 (2019).
[13] Sohn, Kihyuk, et al. "Fixmatch: Simplifying semi-supervised learning with consistency and confidence." arXiv preprint arXiv:2001.07685 (2020).
[14] Yun, Sangdoo, et al. "Cutmix: Regularization strategy to train strong classifiers with localizable features." Proceedings of the IEEE/CVF International Conference on Computer Vision. 2019.
[15] Tremblay, Jonathan, et al. "Training deep networks with synthetic data: Bridging the reality gap by domain randomization." Proceedings of the IEEE conference on computer vision and pattern recognition workshops. 2018.
[16] Kar, Amlan, et al. "Meta-sim: Learning to generate synthetic datasets." Proceedings of the IEEE/CVF International Conference on Computer Vision. 2019.
[17] Andrychowicz, OpenAI: Marcin, et al. "Learning dexterous in-hand manipulation." The International Journal of Robotics Research 39.1 (2020): 3-20.
[18] Cordts, Marius, et al. "The cityscapes dataset for semantic urban scene understanding." Proceedings of the IEEE conference on computer vision and pattern recognition. 2016.
[19] Richter, Stephan R., et al. "Playing for data: Ground truth from computer games." European conference on computer vision. Springer, Cham, 2016.
[20] Cubuk, Ekin D., et al. "Autoaugment: Learning augmentation strategies from data." Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019.

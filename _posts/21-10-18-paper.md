---
title: 💡 Paper
tags: paper 💡 🔥
mathjax_autoNumber: false
---

- **Self-training with combined unsupervised domain adaptation and semi-supervised learning**   
    (비지도 도메인 적응과 준지도 학습을 결합한 자가 학습)
- **Unsupervised domain adaptation with semi-supervised learning for semantic segmentation**   
    (의미론적 분할을 위해 반지도 학습 방법을 적용한 비지도 도메인 적응)
- **Semi-Supervised Domain Adaptation for Semantic Segmentation**   
    (의미론적 분할을 위해 반지도 학습 방법을 적용한 비지도 도메인 적응)

<!--more-->

- Self-supervised learning with image based synthetic teacher model
- Synthetic 이미지 기반의 교사 모델을 이용한 Self-training
- Semi-supervised domain adaptation for semantic segmentation
- A Unified Approch to Semi-supervised learning and domain adaptation
- Self-training for Unsupervised domain adaptation on semantic segmentation
- Self-training with Semi-supervised domain adaptation for semantic segmentation
- Self-training that combines unsupervised domain adaptation and semi-supervised learning for semantic segmentation
- Unsupervised domain adaptation with semi-supervised methods for semantic segmentation
- 기존 UDA에 SSL 방법론 적용

# 국문 요약 / 영문 요약

Data-driven 기반의 Supervised learning이 여러 visual task 분야에서 좋은 성과를 거둬오고 있다. 학습 모델이 더욱 커지고 깊어짐에 따라 양질의 학습 데이터가 더욱 중요해지고, 그에 따라 모델의 성능이 좌우되고 있다. 하지만 문제와 도메인에 맞는 annotation 데이터를 얻기 위한 비용은 human resource가 많이 들어가고 여전히 매우 비싸다.

<!-- 이러한 데이터 부족 문제를 해결하기 위해 적은 데이터를 활용한 학습 방법인 Semi-supervised learning, Unsupervised learning나 비교적 얻기 쉬운 다른 도메인의 데이터를 이용한 Transfer learning 기반 방식이 활발하게 연구되고 있다. -->

이 때 현실과 유사하게 digital twin 세계를 구축하면 가상의 공간 내에서 원하는 Ground Truth 데이터를 무한하게 얻을 수 있고 더 나아가 현실 세계에서 얻기 어려운 데이터들도 비교적 쉽게 획득할 수 있다.

본 논문에서는 시뮬레이션을 통해 얻을 수 있는 synthetic 데이터를 이용해 데이터 획득 비용을 줄이고, 도메인 학습 데이터에 대한 의존성을 낮춰서 학습 데이터가 적은 환경에서도 높은 성능을 보이는 UDAS 프레임워크를 제안한다. 

<!-- 
본 논문에서는 시뮬레이션을 통해 얻을 수 있는 synthetic 데이터를 이용해  데이터 획득 비용을 줄이고, 유사한 도메인 데이터를 사용하여 학습하더라도 안정적인 성능을 보일 수 있도록 도메인 학습 데이터에 대한 의존성을 낮추는 UDAS 프레임워크를 제안한다. -->

UDAS 프레임워크는 Unsupervised Domain Adaptation 구조에 Semi-supervised learning 기법인 Self-training, Consistency regularization을 결합하였고 도메인 간 distribution gap을 줄이기 위해 Domain CutMix를 도입하여 기존 semantic segmentation UDA SOTA 대비 높은 mIoU를 달성하였다. 또한 Supervised learning 방식으로 학습된 모델의 성능과 크게 차이가 나지 않고, 오히려 학습 데이터가 적은 경우에는 Supervised learning 보다 안정적이고 높은 성능을 보이는 것을 확인하였다. 

<!-- Synthetic 데이터로 학습한 AI 모델이 현실 세계에서 잘 동작하기 위해서는 실제 데이터와 synthetic 데이터 간 distribution gap을 최소화 해야하는데, 이를 위해 Generative model, Adversarial learning, Pseudo-labeling 등의 기법들을 이용한 Unsupervised Domain Adaptation이 많이 연구 되고 있다. 하지만 대부분의 연구가 Domain shift를 줄이는 데에만 목적을 두고 있고 unlabeled 데이터 활용 방안에는 집중하고 있지 않다.

본 논문에서는 Synthetic 데이터를 이용한 Unsupervised Domain Adaptation (UDA)에 Semi-supervised learning (SSL)을 결합하여 domain 학습 데이터에 대한 의존성을 감소시키고, 갖고 있는 unlabeled 데이터까지 효과적으로 활용하는 UDAS (UDA + SSL) 모델을 제안한다.  -->

- 핵심어: Domain Adaptation, Semi-supervised learning, Data Augmentation, Pseudo-labeling, Self-Training, Consistency regularization, Synthetic data, Semantic Segmentation

# 1. 서론

ImageNet의 데이터 공개를 시작으로 Data-driven 기반의 Supervised learning이 여러 visual task 분야에서 좋은 성과를 거두고 있다. 검증된 좋은 모델들이 이미 존재하기 때문에, 특정 문제에 맞는 AI모델을 만들기 위해서는 그에 맞는 양질의 데이터를 얼마나 많이 확보하느냐가 결국 모델의 성능을 좌우하게 되었다.

하지만 특정 도메인의 task에 맞는 데이터를 확보하고, 그 데이터에 labeling 하는 것은 많은 시간과 비용을 필요로 한다. 이러한 데이터 의존적인 문제들을 해결하기 위해 데이터가 부족하더라도 효과적인 학습을 할 수 있는 다양한 연구들이 진행되고 있다. 유사한 task에서 학습된 pretrained 모델을 가져와 target task에 적용하는 Transfer Learning, 그리고 labeled data가 적거나 없을 때 사용하는 Semi-Supervised Learning, Self-Supervised Learning 등의 연구들이 있다.

Labeling 작업의 비용이 큰 문제도 있지만 특정 상황에 대한 데이터 자체를 얻기 어려운 경우도 있다. 특히나 자율주행차량, 로보틱스 분야에서는 현실 세계에서 얻기 어려운 데이터들이 존재한다. 예를 들어 교차로 한가운데에 사람이 있는 데이터나, 로봇의 카메라 시점에서의 데이터 등은 쉽게 획득할 수가 없을 것이다.

이 경우 실제 세계를 digital twin 하여 시뮬레이션을 구축하는 방안이 있다. 시뮬레이션을 사용하게 되면 현실 세계에서 얻기 어려운 상황들을 연출하고 물체의 위치, lighting, metarial 등과 같은 환경을 자유자재로 변경하여 데이터들을 추출할 수가 있다. 컴퓨터 그래픽과 계산식에 의해 이미지, depth, IMU, LiDAR, Radar와 같은 센서 데이터와 객체의 bounding box, 픽셀 별 클래스, optical flow 등의 정확한 Ground Truth 데이터들이 자동으로 생성되도록 프로그래밍 할 수 있다. 이러한 synthetic 데이터를 이용하면 human resource가 발생하는 annotation 작업을 줄이고 무한히 많은 데이터를 생성해 AI 학습에 사용할 수 있다.

물론 Synthetic 데이터를 사용해 학습한 모델이 현실 세계에서 잘 동작하기 위해서는 실제 세계와 시뮬레이션 간의 격차가 작아야만 한다. 하지만 시뮬레이션을 최대한 사실적으로 만들기 위해서는 많은 노력이 들어가고, 실제 세계의 모든 데이터들을 모델링하기에는 쉽지 않은 일이다.

Synthetic 데이터를 활용하여 모델을 학습하고 학습된 모델을 현실에서 사용하기 위해서는, 다른 도메인의 데이터와의 distribution gap을 줄이며 학습하는 방식의 domain adaptation 방법론을 사용할 수 있다. 다양한 방법들이 제안되고 있지만 대부분의 연구들은 target 도메인에 대한 정보 없이 domain shift 자체를 감소시키는 데에 집중하고 있다. 

본 논문에서는 이미지 기반의 synthetic 데이터를 활용하는 domain adaptation 모델에 semi-supervised learning 기법들을 도입하여 도메인 학습 데이터에 대한 의존성을 감소시키고 데이터가 부족한 상황에서도 안정적인 성능을 보이는 UDAS(UDA+SSL) 프레임워크를 제안한다. Visual task 중 dense labeling task로 분류되는 semantic segmentation을 downstream task로 설정하여 synthetic 데이터를 사용했음에도 불구하고 높은 성능을 보이는 것으로 제안한 방법을 증명한다.

# 2. 관련 연구

## 2.1. Semantic Segmentation

Semantic segmentation는 이미지의 각 픽셀이 어느 클래스에 속하는 지 예측하는 것으로, 이미지의 전반적인 의미와 구조를 파악하여 더 깊이 있게 이해하는 작업이다. 픽셀 수준의 labeling이 수행되기 때문에 dense labeling task 라고 불리고, 이는 classification 이나 localization에 비해  어려운 문제로 분류된다.

Semantic segmentation 모델은 Supervised-learning 기반의 딥러닝 아키텍처가 나오면서 성능이 상당히 개선되었는데 최근에는 입력 공간 차원을 유지하며 전역의 semantic 을 추출하기 위해 encoder, decoder로 구성된 auto-encoder 구조가 많이 사용되고 있다. 대표적으로 FCN[1], PSPNet[2], DRN[3], DeepLab[4] 등과 같은 아키텍처들이 제안되었으며 좋은 성능을 보이고 있다.

하지만 대량의 labeled 데이터에 의존적이라 데이터셋을 확보하는 데에 많은 시간을 들여야 한다. 픽셀 별로 annotation 하는 것은 다른 visual task 에 비해 비용과 시간이 더 많이 소요되기 때문에 원하는 도메인의 데이터를 얻기가 쉽지 않다.

## 2.2. Semi-supervised learning

Semi-supervised learning(SSL)은 소량의 labeled 데이터를 이용한 학습 방법이다. 소량의 labeled 데이터로만 학습하게 되면 overfitting과 같은 문제가 발생할 가능성이 크기 때문에 소량의 labeled 데이터와 대량의 unlabeled 데이터를 함께 사용하여 성능을 향상시키기 위해 semi-supervised learning이 사용된다.

![](/assets/images/21-09-24-paper-self-training.png)

Self-training은 가장 간단한 SSL방법으로, 소량의 labeled 데이터로 충분히 학습된 모델을 사용하여 unlabeled 데이터에 pseudo-labeling 을 하고, pseudo labeled 데이터와 labeled 데이터를 함께 학습에 사용하는 방식이다. Iteration을 반복할 수록 labeled 데이터가 늘어나고 확장된 데이터셋을 이용해 모델의 성능도 함께 향상될 수 있다. 하지만 처음 pseudo-labeling을 하는 모델의 성능이 좋지 않으면 error rate이 계속해서 증가하기 때문에 잘못된 방향으로 학습이 될 수 있다는 문제가 있다.

![](/assets/images/21-09-24-paper-entropy-minimization.png)

그래서 Unlabeled data를 학습시킬 때 pseudo-labeling을 그대로 사용하지 않고, pseudo-label의 maximum softmax probability를 기준으로 예측 확률의 confidence를 높이기 위해 entropy minimization을 사용한다. 주로 softmax temperature를 적용하여 unlabeled 데이터의 예측값을 계산할 때 decision boundary에서 멀어지도록 더 sharp하게 예측 확률값을 정한다. 단일 기법으로는 성능이 좋지 않지만 최신 연구에 많이 적용해 사용하고 있다. 

최근 많이 사용되는 SSL 접근법은 Consistency regularization을 사용하는 것으로, unlabeled 데이터에 노이즈를 추가하더라도 예측 분포는 유지된다는 아이디어에 기안한 방법이다. 노이즈가 없는 원본 데이터와 노이즈가 주입된 데이터를 동일한 예측 분포로 학습하는 것이 consistency regularization의 목표이다.

Xie, Qizhe, et al. [8]는 teacher-student 기반의 self-training으로, labeled 데이터로 학습된 teacher 모델을 이용해 unlabeled 데이터에 pseudo-labeling 하고 이 데이터에 노이즈를 추가하여 student 모델을 학습하는 방법이다. Student 모델이 어느정도 학습이 되면 teacher 모델로 설정하고 다시 pseudo labeling을 수행하게 만들어 반복적인 학습 파이프라인을 구성했고 이를 통해 SOTA를 기록할 정도로 좋은 성능을 보인 연구이다.

Pham, Hieu, et al. [9]은 teacher 모델과 student 모델을 동시에 학습하는 방법으로, teacher 모델이 잘 학습되어 있지 않더라도 student 모델과 유기적으로 학습할 수 있도록 Co-training 방식을 제안하였다.

Miyato, Takeru, et al. [10]은 입력 데이터의 adversarial transformation을 생성하고 예측 결과 간의 KL-divergence를 측정하여 학습시키는 방법을 제안했다. Xie, Qizhe, et al. [11]은 노이즈 대신 최신 augmentation 기법인 AutoAugment를 사용하여 예측 결과 간 KL-divergence를 loss로 사용한 방법으로, 적은 labeled 데이터만으로 supervised learning을 뛰어넘는 성능을 보였다.

그리고 Berthelot, David, et al. [12], Sohn, Kihyuk, et al. [13] 등 Pseudo-labeling과 함께 augmentation, entropy minimization, consistency regularization을 포함한 다양한 기법을 함께 사용한 연구들이 나오고 있다.

## 2.3. Unsupervised Domain Adaptation

일반적으로 labeled 데이터가 부족하면 pre-trained된 큰 모델을 이용하여 transfer learning을 수행하는 방법이 있다. Transfer learning을 하기 위해서도 해당 도메인에 맞는 일정 양의 labeled 데이터가 필수적으로 필요하나, Domain Adaptation(DA)은 labeled 데이터가 없더라도 이전 task 에서 배운 지식을 활용하여 새로운 상황에서도 잘 예측하도록 학습한다. 즉, 다른 도메인(Source)에서 배운 지식을 활용해 실제  도메인(Target) 모델에 적용하기 위한 방법으로, Zero-shot learning, few-shot learning, self-supervised learning과 함께 sample-efficient learning의 한 유형이다.

![](/assets/images/21-09-24-paper-domain-adaptation.png)

Source 도메인과 target 도메인 간의 데이터 분포가 다르기 때문에 source 도메인에서 학습된 모델을 target 도메인에 사용하게 되면 distribution shift(또는 domain gap)와 dataset bias에 의해 오차가 발생할 가능성이 크다. 그렇기 때문에 도메인이 다른 학습데이터를 활용하기 위해서는 두 도메인의 불일치성을 줄이면서 동시에 task loss도 최소화 하는 모델을 만들어야 한다.

![](/assets/images/21-09-24-paper-adaptation-level.png)

DA 문제를 풀기 위한 주요 전략은 source 도메인과 target 도메인 사이의 격차를 줄임으로써 예측 모델의 성능 저하를 막는 방법으로 Input-level, feature-level, output-level에서 각각 adaptation 모듈을 추가하여 도메인 간 불일치성을 줄일 수 있다.

그 중 Unsupervised Domain Adaptation(UDA)는 Target 도메인의 unlabeled 데이터를 사용하기 위해 다른 도메인의 labeled 데이터를 사용하는 일종의 Semi-supervised learning의 변형이라고 볼 수 있다. Target 도메인의 labeled 데이터는 필요하지 않지만 충분한 양의 Unlabeled target 데이터가 필요하다.

Tzeng, Eric, et al.[5] 는 UDA에 GAN을 도입하여 adversarial adaptation을 처음 제안한 논문으로, feature level에 domain discriminator를 추가하고 adversarial loss를 사용하여 domain discrepancy를 최소화 하는 방법을 제안했다. 

Hoffman, Judy, et al.[6] 에서는 feature level에 adversarial learning을 그대로 사용하고, input-level에서 source 도메인 이미지를 target 이미지와 유사한 스타일로 생성하도록 generative 모델을 도입한 방법을 제안했다. Generative 모델을 도입함으로써 input 데이터의 숨겨진 확률분포를 찾아내고 유사한 확률분포의 데이터를 생성해 사용하는 방식이다. 

Tsai, Yi-Hsuan, et al.[7] 에서는 두 도메인의 이미지 스타일이 다르더라도 segmentation 결과에 포함되어 있는 공간 레이아웃, local context 등의 구조화된 특성은 유사하기 때문에 output-level에 adversarial learning을 도입하여 segmentation 예측 결과의 분포를 유사하게 학습하는 방법을 제안했다.

![](/assets/images/21-09-24-paper-da-methods.png)

정리하면 기존의 UDA 연구는 (a) source 도메인과 target 도메인의 불일치성을 명시적으로 측정하여 loss로 사용하는 방법 (b) domain confusion을 위해 discriminator와 adversarial loss를 사용하는 방법 (c) source 도메인 이미지를 target 이미지와 유사한 스타일로 생성하기 위해 input-level에 generative 모델을 추가하여 appearance gap을 줄이는 방법 (d) task 모델과 self-supervised 모델을 함께 학습시키는 방법으로 분류될 수 있다. 그 외에도 Classifier discrepancy 분석, Entropy minimization, Curriculum learning, Multi-tasking learning 방법들이 있고 이러한 방법론들을 변형하거나 적절이 섞어가며 다양한 방법들이 제안되고 있다. 

다만 UDA 연구는 도메인 분포의 차별성에 따라 적용할 수 있는 도메인이 한정적일 수 있다. 타겟 도메인 데이터에 대한 정보가 전혀 없기 때문에 도메인 분포를 파악하는 것이 보다 어려운 작업이 될 수 있고 특정 도메인 페어에 한해서만 잘 동작하는 모델이 될 수도 있다.

# 3. 제안 방법

## 3.1. 개요

![](/assets/images/21-10-18-paper-udas-overview.png)

UDAS의 전체적인 프레임워크는 크게 Domain Adaptation(DA)과 Semi-Supervised Learning(SSL) 두 모듈로 나눌 수 있다. 

![](/assets/images/21-10-18-paper-udas-domain-adaptation.png)

DA 모듈은 source 도메인과 target 도메인을 함께 활용하여 도메인 간 distribution gap을 줄이고, 동시에 target 도메인에서 잘 동작하도록 task 모델을 학습한다. 기본적으로 target 도메인에 대한 label 데이터가 없다고 가정하고 UDA 구조로 구성되어 있다. Input-level 에서 GAN 기반의 모델을 통해 appearance gap을 줄이고 ouput-level 에서는 multi-level로 adversarial learning을 도입하여 prediction 결과에 대한 gap을 줄인다.

Labeled target 데이터가 없더라도 task를 수행할 수 있으나, labeled target 데이터가 있는 경우에는 Domain CutMix 방법을 이용하여 도메인이 서로 다른 데이터를 결합하고 새로운 source 데이터로 사용한다. 간접적으로 labeled target 데이터를 사용함으로써 target 데이터에 대한 의존성이 높아지지 않도록 설계되었다.

![](/assets/images/21-10-18-paper-udas-semi-supervised-learning.png)

SSL 모듈은 teacher-student 모델 구조로, DA를 통해 학습된 task 모델이 teacher 모델이 되어 unlabeled target 데이터에 pseudo-lableing 해준다. 실제 target task를 수행하는 student 모델은 pseudo-labeling 된 데이터들을 이용하여 supervised learning 구조로 학습한다. Student 모델이 일정 이상 학습이 되면 student 모델이 teacher 모델이 되어 pseudo-labeling 과정부터 반복하는 self-training 루프를 구성한다. 이를 통해 점진적으로 모델이 성장할 수 있도록 유도한다.

또한 신뢰도 높은 pseudo-labeling을 위하여 consistency regularization, maximum confidence map, confidence-based masking을 적용하여 knowledge distilation 시에 발생할 수 있는 오류 전파율을 낮춘다.

본 논문의 contribution은 다음과 같다.

-	Domain Adaptation에 SSL 기법인 Self-training과 Consistency regularization을 적용한 모델 구조 제안
-	Labeled target 데이터 양을 줄였을 때 Supervised learning보다 안정적인 성능
-	기존 UDA SOTA 대비 높은 mIoU 달성

## 3.2. Image Translation

![](/assets/images/21-10-18-paper-image-translation.png)

시각적으로 봤을 때 큰 차이가 있는 데이터는 그만큼 큰 도메인 gap 이 있을 것이라고 가정할 수 있다. Input-level에서의 도메인 gap을 줄이기 위해 source 도메인의 image appearance가 target domain과 유사하도록 생성해낸다. 대표적인 image-to-image translation 방법인 CycleGAN을 이용하여 target domain 스타일의 이미지로 변형시켜 domain adaptation 모델의 source 데이터로 사용한다.

$$ L(G, F, D_s, D_t) = L_{GAN}(G, D_t, S, T) + L_{GAN}(F, D_s, T, S) + \lambda L_{cyc}(G, F) $$

$$ G^*, F^* = arg min_{G, F} max_{D_s, D_t} L(G, F, D_s, D_t) $$

$S$가 source domain 샘플, $T$가 target domain 샘플이라고 했을 때 $S \rightarrow T$ 의 adversarial loss와 $T \rightarrow S$ adversarial loss에 각각 원본으로 복구하는 cycle consistency loss를 더해준 값이 총 Loss가 되고, 이 Loss를 최소화 하는 방향으로 $G: X \rightarrow Y$ 와 $F: Y \rightarrow X$ 를 학습시킨다.

Source 도메인과 target 도메인의 분포를 구분할 수 없도록 학습시키는 방법이기 때문에 도메인 간 gap이 줄었을 것이라고 기대할 수 있고, $G: S \rightarrow T$ 모델을 통해 생성된 데이터는 target 도메인 분포에 가까운 형태가 될 수 있다. 결과적으로 Source 도메인 데이터를 그대로 사용하는 것 보다 Target 도메인의 스타일에 맞게 이미지를 재생성해서 target task 모델에 적용했을 때 성능이 향상되었다.

## 3.3. Unsupervised Domain Adaptation

![](/assets/images/21-10-18-paper-uda.png)

UDA 모듈은 __모델을 본따 만든 Adversarial Generative Model 구조로, 입력으로 image translation 된 source 이미지와 label, 그리고 label 이 없는 target 이미지를 사용하고 output-level에 discriminator를 추가하여 input-level adaptation, output-level adaptation을 수행한다.

먼저 image translation 된 source 이미지와 labels을 이용하여 source 도메인 기반으로 segmentation 모델을 학습한다. 사용된 모델은 multi-level segmentation 모델로, 기존 segmentation 모델에서 마지막 레이어로 prediction하는 primary classifier와 그 직전 레이어로 prediction 하는 auxiliary classifier로 구성된다. 

$$ L_{seg}^p = -\sum_{h=1}^{H}\sum_{w=1}^{W}\sum_{c=1}^{C} y_s^i log(F_p(x_s^i))  $$

$$ L_{seg}^a = -\sum_{h=1}^{H}\sum_{w=1}^{W}\sum_{c=1}^{C} y_s^i log(F_a(x_s^i)) $$

$$ L_{seg} = -\sum_{h=1}^{H}\sum_{w=1}^{W}\sum_{c=1}^{C} y_s^i log(F_p(x_s^i)) \\ \qquad \qquad \qquad - \lambda_{seg} \sum_{h=1}^{H}\sum_{w=1}^{W}\sum_{c=1}^{C} y_s^i log(F_a(x_s^i)) $$

Primary classifier 와 auxiliary classifier를 모두 학습에 이용하기 위해 Primary classifier loss $L_{seg}^p$와 auxiliary classifier loss $L_{seg}^a$를 더하여 전체 segmentation loss로 사용한다. 여기서 $\lambda_{seg}$은 auxiliary classifier loss에 대한 가중치를 위한 하이퍼파라미터를 뜻한다. Segmentation loss function은 pixel-wise cross-entropy loss를 사용한다.

이렇게 학습된 Segmentation 모델은 source 도메인 데이터만을 이용하여 학습되기 때문에 데이터 distribution이 다른 target 도메인에서는 오차가 발생할 확률이 높다. Source 도메인과 target 도메인 간 불일치성을 줄이기 위한 방법으로 source domain classifier와 target domain classifier에 discriminator와 adversarial loss를 추가한다.

$$ L_{adv}^p = \mathbb{E}[ log(D_p(F_a(x_s^i))) + log (1 - D_p(F_p(x_t^j))) ] $$

$$ L_{adv}^a = \mathbb{E}[ log(D_a(F_a(x_s^i))) + log (1 - D_a(F_a(x_t^j))) ] $$

$$ L_{adv} = \mathbb{E}[ log(D_p(F_a(x_s^i))) + log (1 - D_p(F_p(x_t^j))) ] \\ \qquad  \qquad  \qquad + \lambda_{adv} \mathbb{E}[ log(D_a(F_a(x_s^i))) + log (1 - D_a(F_a(x_t^j))) ]$$

Discriminator $D$는 source prediction $F(x_s)$와 target prediction $F(x_t)$를 구분하지 못하도록 학습되어 domain confusion을 야기한다. Primary classifier와 auxiliary classifier에 모두 적용되기 때문에 multi-level로 adaptation이 일어나게 되고 전체 adversarial loss는 $L_{adv}^p$와 $L_{adv}^a$를 더한 값이다.

Source 도메인과 target 도메인 간의 불일치성을 줄인 이후에는 target 도메인 내의 불일치성을 줄여주는 작업이 필요하다. 이는 이후 pseudo-labeling 작업을 수행할 때 target 도메인 내의 불확실성을 줄이기 위함이다. Primary classifier와 auxiliary classifier의 prediction은 유사한 결과를 가져야하기 때문에 KL-divergence loss를 도입한다.

$$ L_{kl} = -\sum_{h=1}^{H}\sum_{w=1}^{W}\sum_{c=1}^{C} F_a(x_t^i) log(F_p(x_t^i)) \\ \qquad \qquad \qquad - \sum_{h=1}^{H}\sum_{w=1}^{W}\sum_{c=1}^{C} F_p(x_t^i) log(F_a(x_t^i)) $$

즉, UDA의 전체 loss는 segmentation loss와 adversarial loss, kl-divergence loss를 더한 값이다. 

$$ L_{total} = L_{seg} + L_{adv} + L_{kl}$$

이로써 adversarial learning과 multi-level adaptation을 통해 source 도메인에서 학습한 feature들이 target 도메인 영역으로 전달되고 inter-domain knowledge, intra-domain knowledge를 동시에 학습할 수 있다. 

## 3.4. Domain CutMix

Labeled target 데이터가 없는 경우에는 3.3.의 서술 내용처럼 UDA 구조로 동작할 수 있지만 소량의 labeled 데이터가 있는 경우 target 도메인에 대한 정보를 토대로 모델의 성능을 더욱 향상시킬 수 있다. Labeled target 데이터를 supervised learning 형태로 그대로 이용하게 되면 당장의 성능은 좋아질 수 있겠지만 target 학습 데이터에 대한 의존성이 강해져 학습하지 못한 데이터에 대한 추론 능력이 떨어지고, source 도메인 데이터가 추가 됐을 때 모델 학습에 혼란을 야기할 수 있다. 이러한 문제를 방지하기 위해 UDAS 프레임워크에서는 도메인 독립적으로 모델을 학습시키기 위해 source 도메인 샘플과 target 도메인 샘플을 결합하여 학습 데이터로 사용한다. 

![](/assets/images/21-10-18-paper-domain-cutmix.png)

Semantic segmentation은 각 픽셀의 예측값과 더불어 주변 pixel에 대한 지역적인 예측 정보도 필요한데, 한 이미지에 source 도메인 이미지와 target 도메인 이미지가 결합되어 있으면 도메인과 관계 없이 이미지 자체의 representation을 학습할 수 있다. 이를 위해 CutMix[14]의 형태로 두 도메인의 이미지를 임의로 결합시켜 학습 데이터로 사용한다.

CutMix는 두 이미지를 섞어 하나의 이미지에 표현하는 Data augmentation 방법으로, 이미지의 전체적인 영역을 보고 학습할 수 있도록 유도하기 때문에 Regularization과 Localization 성능을 높일 수 있다. CutMix의 아이디어에 기반한 Domain CutMix는 도메인이 다른 두 이미지를 섞어 하나의 이미지로 생성한다.

$$ r_x \sim Unif \{ 0, W \}, r_w = W \sqrt {1- \lambda}
\\
r_y \sim Unif \{ 0, H \}, r_h = H \sqrt {1- \lambda}
\\
B = \{ r_x , r_y, r_w, r_h \}
\\
M \in \left\{0, 1\right\}^{W \times H}
$$

우선 데이터 생성을 위해 매 배치마다 랜덤으로 bounding box $B$ 사이즈를 결정하고, $B$ 사이즈에 맞게 binary 마스크 $M$을 생성한다. $\lambda$는 영역의 비율을 뜻하는 파라미터이다. 

$$
\tilde {x} = M \odot x_s + (1-M) \odot x_t
\\
\tilde {y} = \lambda_{y_s} + (1- \lambda)y_t
$$

그 후, Source 도메인의 샘플 $x_s$ 에서 $M$ 영역만큼 지우고, 그 부분에 target 도메인 샘플 $x_t$를 채운다. 마찬가지로 source 도메인 샘플에 대한 label 데이터인 $y_s$에서도 $\tilde{x} $와 페어되는 labeled cutmix 데이터를 만들어낸다.

이렇게 생성된 cutmix 데이터에 augmentation 기법인 AutoAugment를 취해서 새로운 데이터를 생성하게 된다. 이 데이터는 기존에 학습에 사용되던 source 이미지 대신 입력 데이터로 사용하여 segmentation 모델을 학습시킨다. 이미지 데이터와 label이 모두 존재하기 때문에 segmentation 모델을 supervised learning 방식으로 학습시킬 수 있다. 

Domain CutMix 데이터는 랜덤하게 결합되고 augmentation 까지 취해졌기 때문에 현실과 거리가 먼 이질적인 데이터처럼 보인다. 그럼에도 불구하고 도메인의 차이보다는 지역적인 정보들에 집중하여 학습하기 때문에 모델의 일반화 능력이 향상되고, 무엇보다 값싸게 데이터를 생성할 수 있다는 장점이 있다. 

Sim-to-real을 위한 Domain Randomization논문들[15][16][17]에 의거하면 랜덤하게 생성된 synthetic 데이터를 사용하면 모델이 더욱 robust 하게 학습되기 때문에 실제 세계에 적용할 때 더 좋은 성능을 보인다는 것을 실험적으로 알 수 있다.

## 3.5. Self-Training

![](/assets/images/21-10-18-paper-consistency.png)

DA를 통해 학습된 segmentation 모델은 바로 target 데이터를 prediction 하는 task 모델이 되지 않고, 실제 task 모델인 student 모델에게 knowlege distilation 해주는 teacher 모델이 된다. Teacher 모델은 unlabeled target 데이터에 pseudo-labeling 하고 student 모델은 이 labeled 데이터들로 supervised learning 하며 자연스럽게 지식을 전파받을 수 있다. 이 때 학습에 사용하는 labeled 데이터는 pseudo-labeling 된 unlabeled target 데이터와 이미 가지고 있는 labeled 데이터를 섞어서 함께 학습한다. Student 모델이 어느정도 학습된 이후에는 student 모델이 teacher 모델이 되어 pseudo-labeling 데이터를 업데이트하고, 해당 데이터로 또다른 student 모델을 학습하는 과정을 반복하며 모델이 점차적으로 신뢰도 높은 방향으로 재학습되도록 iterative learning 한다.

Unlabeled target 데이터가 입력으로 들어오는 경우 pseudo-labeling 작업만 수행하지만, label이 있는 source 데이터 혹은 target 데이터가 들어오게 되면 pseudo-labeling 된 결과물과 GT 데이터의 차이를 KL-divergence loss로 추가하여 학습함으로써 일종의 consistency regularization 효과를 준다.

![](/assets/images/21-10-18-paper-self-training.png)

Teacher 모델이 예측한 결과를 그대로 Pseudo-labels로 사용하게 되면 teacher 모델이 갖고 있는 오차 정보가 그대로 전파되기 때문에 Sharpening predictions 해주는 작업이 필요하다. UDAS 프레임워크에서 사용하는 Pseudo-labeling 방법은 다음과 같다.

일단 K개의 augmentation 데이터를 생성하고 각각 teacher 모델을 통해 prediction probability를 얻는다. Confidence map을 비교했을 때 가장 높은 confidence의 예측값을 사용하기 위해 maximum confidence map을 구성하고 이를 통해 만들어진 pseudo-label 중에서 특정 confidence 값 이상의 예측 데이터만 학습 데이터로 사용하여 오류 전파율을 낮춘다.

<!-- Unlabeled 데이터에 pseudo-labeling이 된 이후에는 source 도메인 데이터는 사용하지 않는다. Pseudo-labeling된 데이터들을 이용해 student 모델을 supervised learning 방식으로 학습하고, student 모델이 target 도메인에 대하여 fine-tuning 되어 더 나은 성능에 도달하게 되면 student 모델이 teacher 모델로 치환되어 기존의 pseudo-labeling 데이터를 업데이트 해준다. 그리고 재생성된 pseudo-labeled 데이터를 이용하여 또다시 새로운 student 모델을 학습하는 방식으로, 모델이 점차적으로 신뢰도 높은 방향으로 재학습되도록 구성하여 반복적인 학습이 이루어지도록 한다. -->

Noisy Student 논문에서는 noisy한 데이터로 학습된 커다란 student 모델이 techer 모델보다 robust 하게 학습되어 성능이 좋아짐을 보였다. UDAS 프레임워크는 DA 모델을 이용해 unlabeled target 데이터와 수많은 synthetic 데이터로 student 모델을 학습한다. Augment된 synthetic 데이터는 noise의 일종으로 볼 수 있기 때문에 위의 논문과 같은 효과를 낼 수 있다.

# 4. 실험 및 결과

## 4.1. 실험 설계

UDAS 프레임워크 코드는 Ubuntu 20.04 환경에서 Python 3.8과 Pytorch 1.9.0 버전을 기준으로 구현되었고 모든 실험은 메모리 11GB의 GeForce GTX 1080 Ti 그래픽카드가 2개 장착된 PC에서 진행되었다.

### 4.1.1. 데이터셋

![](/assets/images/21-09-24-paper-dataset.png)

Cityscapes[18]는 유럽 50개 도시의 거리에서 차량 시점으로 캡쳐한 2048x1024 픽셀의 이미지 데이터셋이다. 2,975개의 training set, 500개의 validation set, 1,595개의 test set으로 총 5,000개의 이미지 데이터와 pixel-wise semantic label로 구성되어 있다. 추가로 label 되어 있지 않은 19,998 개의 raw 이미지 데이터도 제공된다.

GTA5[19]는 도시 운전을 위한 대규모 synthetic 데이터셋 중 하나이다. 사실적인 그래픽을 보여주는 GTA V 게임을 통해 렌더링 되었고 미국 스타일 도시에서의 자동차 관점에서 촬영되었다. 24,966개의 1914x1052 픽셀의 이미지와 pixel-wise semantic label로 구성되어 있다. Label은 Cityspcaes의 클래스에 맞게 19개의 클래스로 재매핑 할 수 있다.

Cityscapes와 GTA5는 시각적인 차이가 존재하지만 비슷한 차량의 높이에서 도로 상황을 캡쳐한 데이터라는 점에서 구조가 유사하다고 볼 수 있다. GTA5 데이터가 Cityscapes 데이터 양에 비해 약 5배 가량 많기 때문에 GTA5의 데이터를 최대한 활용하여 이미지의 전반적인 의미와 구조를 충분히 학습하고, Cityscapes 환경에서의 fine-tuning을 위하여 Cityscapes의 unlabeled 데이터를 함께 사용한다.

|  Dataset   |   Type    | # of Labeled Data | # of Raw Data | # of Classes |
| :--------: | :-------: | :---------------: | :-----------: | :----------: |
| Cityscapes |   Real    |       5,000       |    19,998     |      19      |
|    GTA     | Synthetic |      24,966       |       -       |      19      |

### 4.1.2. 평가 Metric

평가 metric으로는 Semantic segmentation 평가 시에 주로 사용되는 클래스 별 IoU(Intersection over Union) 와 mIoU(Mean Intersection over Union)를 사용한다. $TP_i$, $FP_I$, $FN_i$ 는 특정 클래스 $i$의 True Positive, False Positive, False Negative 이고 $N$은 클래스의 갯수이다.

$$ {IoU}_i = \left( \frac{TP_i}{FP_i + FN_i + TP_i} \right) $$

$$ mIoU = \sum_{i=1}^N \left( \frac{IoU_i}{N} \right ) $$

## 4.2. Implementation Details

### 4.2.1. Data Size

대부분의 UDA SOTA 모델은 source 도메인 이미지 사이즈는 1280 x 720, target 도메인 이미지 사이즈는 1024 x 512로 원본 사이즈를 그대로 사용하여 학습한다. 아래 실험에서는 GPU 메모리 부족으로 모델은 동일하게 사용하나 source 도메인과 target 이미지 사이즈는 640 x 360으로 resize하여 실험한다. 

![](/assets/images/21-10-18-paper-image-resize.png)

| Model      | Road | Sidewalk | building | wall | fence | pole | Traffic light | Traffic sign | vegetation | terrain | sky  | person | rider | car  | truck | Bus  | train | motorcycle | Bicycle | mIoU  |
| :--------- | :--- | :------- | :------- | :--- | :---- | :--- | :------------ | :----------- | :--------- | :------ | :--- | :----- | :---- | :--- | :---- | :--- | :---- | :--------- | :------ | :---- |
| 1280 x 720 | 89.1 | 23.7     | 82.4     | 19.6 | 20.2  | 33.0 | 42.4          | 39.7         | 85.3       | 33.1    | 76.9 | 60.5   | 33.0  | 85.7 | 36.2  | 43.6 | 5.0   | 22.3       | 30.0    | 42.35 |
| 640 x 360  | 83.9 | 10.2     | 74.3     | 4.4  | 3.0   | 10.3 | 11.8          | 11.4         | 84.2       | 15.8    | 68.7 | 39.0   | 0.1   | 79.1 | 26.1  | 17.3 | 0     | 2.7        | 0       | 28.59 |

표[]는 학습 이미지 사이즈에 따른 성능 저하를 확인하기 위해 실험 환경과 동일하게 이미지 샘플 사이즈를 640 x 360으로 설정한 뒤 UDA SOTA 모델을 학습시킨 결과이다. 640 x 360 사이즈의 이미지를 사용했을 때 원 이미지 사이즈를 사용했을 때의 성능 대비 약 14% mIoU가 감소한 것을 볼 수 있다. 이에 따라 이후에 진행되는 실험은 640 x 360 사이즈로 진행되나, 원 사이즈로 학습하게 되면 성능이 더욱 향상될 것으로 예상할 수 있다.

### 4.2.2. Semantic Segmentation

우선 일반적인 Supervised-learning 방식의 Semantic segmentation 모델과의 성능 비교를 위해 SOTA 모델인 FCN과 DeepLab v3+ 성능 비교를 수행하였다. Backbone은 Resnet-50으로 설정하였고 Cityscapes의 Labeled 이미지들로 Supervised-learning 방식으로 학습시켰다.

![](/assets/images/21-09-24-paper-sl-learning-result.png)

|    model    | road | sidewalk | building | wall | fence | pole | traffic light | traffic sign | vegetation | terrain | sky  | person | rider | car  | truck | bus  | train | motorcycle | bicycle | Pixel Accuracy | mIoU  |
| :---------: | :--: | :------: | :------: | :--: | :---: | :--: | :-----------: | :----------: | :--------: | :-----: | :--: | :----: | :---: | :--: | :---: | :--: | :---: | :--------: | :-----: | :------------: | :---: |
|     FCN     | 0.97 |   0.81   |   0.9    | 0.26 | 0.45  | 0.54 |     0.67      |     0.75     |    0.91    |  0.53   | 0.93 |  0.78  | 0.57  | 0.93 | 0.48  | 0.7  | 0.36  |    0.56    |  0.75   |     94.693     | 68.03 |
| DeepLab v3+ | 0.98 |   0.84   |   0.92   | 0.56 | 0.59  | 0.64 |      0.7      |     0.78     |    0.92    |  0.64   | 0.95 |  0.81  | 0.63  | 0.94 | 0.69  | 0.76 | 0.43  |    0.63    |  0.76   |     95.826     | 74.97 |

사용하는 모델과 Backbone에 따라 mIoU 값 차이가 있으나, Supervised learning으로 모델을 학습시켰을 때 mIoU가 약 70% 정도 나오는 것으로 확인했다. 

많은 domain adaptation 논문들이 Resnet101을 backbone으로 한 DeepLab v2를 Semantic segmentation 모델로 사용하고 있기 때문에, 동등한 비교를 위해 이후 실험에서는 Atrous Spatial Pyramid Pooling(ASPP) 모듈을 포함한 DeepLab v2 모델을 task 모델로 설정하여 진행한다. Cityscapes 데이터셋을 이용해 학습한 Resnet101 + DeepLab v2 모델의 기본 성능은 아래와 같다.

| Model                  | road | walk | build. | wall | fence | pole | light | sign | vege. | terr. | sky  | persn | rider | car  | truck | bus | train | mcyc | Bike | mIoU |
| :--------------------- | :--- | :--- | :----- | :--- | :---- | :--- | :---- | :--- | :---- | :---- | :--- | :---- | :---- | :--- | :---- | :-- | :---- | :--- | :--- | :--- |
| DeepLab v2 (Resnet101) | 96.7 | 76.2 | 88.6   | 47.2 | 47.6  | 45.1 | 57.9  | 67.6 | 89.1  | 58.2  | 90.4 | 70    | 54.8  | 92.2 | 71.8  | 78  | 56.6  | 56.7 | 67.8 | 69.1 |

### 4.2.3. Image Translation

Source 이미지에서 Target 이미지 스타일로 image translation된 데이터가 도메인 gap을 줄이는데 얼마나 기여하는지 확인하기 위한 실험으로, GTA 데이터와 Cityscapes 스타일로 재생성된 GTA->Cityscapes 데이터를 동일한 모델로 각각 예측했을 때 mIoU를 비교한다.

![](/assets/images/21-09-24-paper-style-transfer-prediction.png)

Labeled 데이터 없이 Cityscapes 이미지와 GTA 이미지만을 이용하여 CycleGAN 모델을 학습하고, CycleGAN 모델을 통해 GTA이미지를 Cityscapes 스타일의 이미지로 재생성한다. Image translation된 GTA->Cityscapes 샘플과 그에 대한 segmentation 예측 결과는 그림[]과 같다. 이 때 segmentation 모델은 4.2.1장에서 Cityscapes 데이터로만 학습시킨 DeepLab v3+ 모델을 사용했다.

| Data             | road | walk | building | wall | fence | pole | light | sign | vege. | terrain | sky  | person | rider | car  | truck | bus  | train | mCycle | bicycle | mIoU |
| :--------------- | :--- | :--- | :------- | :--- | :---- | :--- | :---- | :--- | :---- | :------ | :--- | :----- | :---- | :--- | :---- | :--- | :---- | :----- | :------ | :--- |
| GTA              | 70.2 | 19.3 | 60.4     | 23.3 | 13.4  | 25.3 | 31.8  | 17   | 53.5  | 24.3    | 85.6 | 41.2   | 1.7   | 55.4 | 19.1  | 16.8 | 2.4   | 6.4    | 5.4     | 30.1 |
| GTA → Cityscapes | 80.7 | 26.8 | 6.76     | 29.3 | 13.6  | 30.8 | 26.2  | 21.1 | 61.2  | 33.5    | 76.2 | 47.6   | 27    | 63   | 37.1  | 15.8 | 4.9   | 18.2   | 7.4     | 36.2 |
| Cityscapes       | 98   | 84.5 | 92.3     | 56   | 59.1  | 64.6 | 70.2  | 78.9 | 92.4  | 64      | 95   | 81.6   | 63.2  | 94.7 | 69    | 76.9 | 43.3  | 63.4   | 76.5    | 74.9 |

GTA에 대한 mIoU는 30.1% 로 Cityscapes에 적용했을 때의 mIoU인 74.9%에 비해 현저히 낮은 성능을 보이고 있다. 현실과 비슷한 시뮬레이션 환경을 구축하고 사실적인 렌더링을 하였음에도 불구하고 큰 도메인 gap이 존재하는 것이다.

Image translation한 GTA->Cityscapes의 mIoU는 36.2%로, GTA 데이터를 그대로 사용했을 때 보다 mIoU가 +6% 증가하였다. 이를 통해 appearance gap을 줄이는 방법을 통해 target 도메인과의 gap이 일부 줄어들었다고 판단할 수 있다. 하지만 여전히 target 도메인에 적용했을 때의 성능보다는 낮기 때문에 시각적으로 봤을 때 유사해보이더라도 여전히 보이지 않는 도메인 gap이 잔존하는 것을 알 수 있다.

이렇게 변환된 GTA->Cityscapes 데이터는 GTA 데이터 대신 source 도메인 데이터로 사용하여 input-level adaptation을 전체 학습 모델에 녹여낸다.

### 4.2.4. Domain CutMix

시각적인 부분에서 한단계 도메인 gap을 줄였지만 외관상으로 보이는 gap 이외에도 실제로 모델이 이미지를 이해하기 위해서는 보이지 않는 구조적인 부분 또한 학습해야 할 필요가 있다. 이를 위해 도메인을 혼합한 cutmix 데이터를 추가함으로써 구조적으로 이미지를 학습할 수 있도록 구성하였고, 이는 domain confusion을 야기하여 도메인 지식과 관계없이 task에 집중하여 학습할 수 있다.

![](/assets/images/21-09-24-paper-combined-augmentation.png)

그림7은 Cityscapes 이미지와 GTA->Cityscapes 이미지를 결합하여 만든 Domain CutMix 데이터이다. 이미지가 어색하게 결합이 되어 있음에도 불구하고 모델이 일반화 되어 학습되었기 때문에 큰 오차 없이 예측하는 것을 볼 수 있다.

### 4.2.5. Pseudo-labeling

만들어진 DA 모델은 Pseudo-labeling 하는 Teacher 모델이 된다. 신뢰도 높은 Pseudo-labeling을 위해 하나의 이미지에 대해 K개의 augmentation 샘플 데이터를 생성하고 이를 결합하여 사용하게 된다. 실험에서는 K를 3으로 설정하였고 Augmentation은 AutoAugment[20]를 사용했다.  Strong augmentation, weakly augmentation에 따라 prediction 한 결과의 confidence map은 다르게 나오는데, 이 중에서 가장 confidence가 높은 값만을 추출하여 maximum confidence map을 만들면 nosie가 감소되어 정제된 confidence map을 얻을 수 있다. 더욱 신뢰도 높은 pseudo-labeling을 위해 0.6 이상의 confidence만 masking 하여 사용한다.

![](/assets/images/21-09-24-paper-pseudo-labeling-result.png)

랜덤하게 augmentation 샘플들을 생성하여 maximum confidence map, confidence-based masking 작업을 수행하였을 때 신뢰도 높은 pseudo-labeling 데이터가 생성되는 것을 확인할 수 있다.

![](/assets/images/21-09-24-paper-pseudo-labeling-data.png)

생성된 pseudo-labeling 데이터를 통해 task 모델을 학습시키고, 모델이 어느정도 학습됐을 때에는 해당 모델을 다시 teacher 모델로 사용하여 새롭게 pseudo-labeling 하고 점진적으로 학습되어가는 self-training 루프를 이룬다.

## 4.3. Quantitative Comparison

### 4.3.1. Performance

DA와 Style Transfer, SSL 기법들이 차례대로 적용됨에 따라 Target 도메인인 Cityscapes에 대한 예측 결과가 향상된다.

![](/assets/images/21-10-18-paper-paper-performance.png)

DA 없이 학습한 모델은 Domain shift에 의한 많은 오차가 발생한다. Image Translation 즉, Style Transfer(ST)를 적용한 데이터에는 어느 정도 semantic 구조를 파악한 것처럼 보이지만 여전히 많은 노이즈가 포함되어 있다. Style Transfer 데이터를 source 데이터로 하여 UDA를 적용한 모델은 일반적인 UDA SOTA 수준의 성능을 보이고 있다. 여기에 SSL 기법으로 Domain CutMix, pseudo-labeling, self-training를 추가했을 때 깔끔하게 prediction 결과가 나오는 것을 볼 수 있다.

| Model         | road | walk | building | wall | fence | pole | light | sign | vege. | terrain | sky  | person | rider | car  | truck | bus  | train | mCycle | bicycle | mIoU |
| :------------ | :--- | :--- | :------- | :--- | :---- | :--- | :---- | :--- | :---- | :------ | :--- | :----- | :---- | :--- | :---- | :--- | :---- | :----- | :------ | :--- |
| No Adaptation | 83.9 | 10.2 | 74.3     | 4.4  | 3.0   | 10.3 | 11.8  | 11.4 | 84.2  | 15.8    | 68.7 | 39.0   | 0.1   | 79.1 | 26.1  | 17.3 | 0.0   | 2.7    | 0.0     | 28.5 |
| UDA           | 83.9 | 10.4 | 77.2     | 22.5 | 2.8   | 29.4 | 4.1   | 2.8  | 84.2  | 39.9    | 78.8 | 31.9   | 0     | 83   | 30.9  | 4.2  | 0     | 0      | 0.0     | 30.8 |
| ST            | 75.7 | 16.7 | 77.2     | 12.5 | 21.0  | 25.4 | 30.0  | 20.1 | 81.3  | 24.6    | 70.3 | 53.7   | 26.4  | 49.9 | 17.1  | 25.8 | 6.4   | 25.2   | 36.0    | 36.6 |
| ST + UDA      | 89.4 | 26.2 | 82.7     | 25.8 | 22.9  | 36.1 | 40.4  | 38.8 | 84.1  | 37.5    | 83.5 | 59.0   | 26.7  | 83.6 | 28.5  | 36.6 | 0.1   | 14.6   | 26.7    | 44.4 |
| ST + DA + SSL | 95.4 | 69.1 | 87.7     | 43.1 | 41.3  | 42.5 | 46.7  | 59.9 | 88.4  | 50.5    | 90.3 | 68.9   | 49.7  | 90.2 | 61.9  | 65.8 | 46.9  | 44.2   | 63.5    | 63.5 |

각 단계에 대한 클래스 별 IoU와 mIoU는 다음과 같다. DA 기법이 전혀 들어가지 않았을 때 28.5% mIoU, Style Transfer를 도입했을 때는 36.6% mIoU, ST와 UDA를 함께 사용했을 때 44.4% mIoU, 그리고 여기에 SSL 방법론을 추가하여 63.5%까지 성능을 향상시켰다.

| Model           | road | walk | building | wall | fence | pole | light | sign | vege. | terrain | sky  | person | rider | car  | truck | bus  | train | mCycle | bicycle | mIoU |
| :-------------- | :--- | :--- | :------- | :--- | :---- | :--- | :---- | :--- | :---- | :------ | :--- | :----- | :---- | :--- | :---- | :--- | :---- | :----- | :------ | :--- |
| Cycada          | 79.1 | 33.1 | 77.9     | 23.4 | 17.3  | 32.1 | 33.3  | 31.8 | 81.5  | 26.7    | 69.0 | 62.8   | 14.7  | 74.5 | 20.9  | 25.6 | 6.9   | 18.8   | 20.4    | 39.5 |
| AdaptSegNet     | 86.5 | 36.0 | 79.9     | 23.4 | 23.3  | 23.9 | 35.2  | 14.8 | 83.4  | 33.3    | 75.6 | 58.5   | 27.6  | 73.7 | 32.5  | 35.4 | 3.9   | 30.1   | 28.1    | 42.4 |
| AdvEnt          | 89.4 | 33.1 | 81.0     | 26.6 | 26.8  | 27.2 | 33.5  | 24.7 | 83.9  | 36.7    | 78.8 | 58.7   | 30.5  | 84.8 | 38.5  | 44.5 | 1.7   | 31.6   | 32.4    | 45.5 |
| Seg-Uncertainty | 90.5 | 35.0 | 84.6     | 34.3 | 24.0  | 36.8 | 44.1  | 42.7 | 84.5  | 33.6    | 82.5 | 63.1   | 34.4  | 85.8 | 32.9  | 38.2 | 2.0   | 27.1   | 41.8    | 48.3 |
| Ours            | 95.4 | 69.1 | 87.7     | 43.1 | 41.3  | 42.5 | 46.7  | 59.9 | 88.4  | 50.5    | 90.3 | 68.9   | 49.7  | 90.2 | 61.9  | 65.8 | 46.9  | 44.2   | 63.5    | 63.5 |

표 []는 UDA SOTA와 비교한 결과이다. UDA SOTA의 학습 데이터 사이즈가 원 사이즈인 것을 감안하더라도 제안한 모델이 월등하게 좋은 성능을 내고있다. Labeled 데이터를 일부 사용하긴 하지만 Target 도메인에 대한 지식 없이 distribution shift만을 줄이기 위해 노력하는 다른 연구들 대비 좋은 성능을 보이고 있기 때문에 더 실용성이 강화됐다고 볼 수 있다. 

### 4.3.2. Data Dependency

Supervised Learning(SL) 방법론의 경우 학습을 위해서는 많은 양의 Labeled 데이터가 필요하다. 적은 labeled 데이터로 학습시키게 되면 학습 데이터에 overfitting 되는 경향이 있다. Data dependency 실험은 데이터가 적은 상황에서 제안 모델이 SL 방법에 비해 얼마나 안정적인 성능을 보이는지 비교하기 위한 실험이다.

![](/assets/images/21-10-18-paper-data-dependency.png)

그림[]는 Labeled target 이미지 갯수에 따른 mIoU를 그래프로 시각화한 것이다. Cityscapes 데이터 2975장을 모두 사용하여 학습했을 때와 labeled target 학습 데이터 양을 1000, 500, 100, 0으로 감소시키며 mIoU를 비교하였다. SL은 labeled 데이터양이 적어짐에 따라 급격하게 성능이 하락하지만 UDAS는 labeled 데이터양이 적어지더라도 안정적인 성능을 보인다. 이를 통해 제안한 UDAS 프레임워크를 사용하면 도메인 데이터에 대한 의존성이 낮아졌다고 할 수 있다.

| Methods    | 2975 | 1000         | 500           | 100           | 0             |
| :--------- | :--- | :----------- | :------------ | :------------ | :------------ |
| Supervised | 69.1 | 61.2 (-7.9%) | 55.4 (-13.7%) | 41.6 (-27.5%) | -             |
| UDAS       | 63.5 | 60.6 (-2.9%) | 61.3 (-2.2%)  | 55.6 (-7.9%)  | 43.9 (-12.5%) |

수치적으로 봤을 때 Labeled target 이미지를 모두 사용한 경우 UDAS 프레임워크는 SL에 비해 -5.6% 의 성능을 보이고 있고, 1000장인 경우 -0.6%, 500장인 경우 +5.9%, 그리고 100장인 경우 +14.0% mIoU로, labeled target 데이터가 적어질 수록 UDAS가 SL 보다 좋은 성능을 보이는 것을 알 수 있다.

UDAS 프레임워크는 타겟 도메인의 학습 데이터 의존성이 줄어들어 synthetic 데이터를 이용한 다양한 상황들을 학습할 수 있게 되었고, 특히나 학습 데이터가 적거나 없는 상황에서는 SL 방법보다 더 안정적이고 높은 성능을 보인다.

# 5. 결론

AI 학습에 있어서 항상 큰 장애물이 되는 데이터 부족 문제를 해결하기 위해 연구를 진행하였다. 컴퓨터 그래픽이 발전함에 따라 점차적으로 현실세계와 가상세계의 시각적 경계가 모호해지고 있기 때문에, 무한한 데이터를 창출해낼 수 있는 synthetic 데이터에 대한 활용도가 높아질 것이다. 또한 AI 모델이 커지면서 점점 더 많은 데이터가 요구되는데 항상 데이터에 의존적인 AI 모델에서 벗어나 스스로 규칙을 찾아내는 모델을 만들고자 했다. 

본 논문에서는 Synthetic 데이터를 이용하여 DA와 SSL기법을 결합한 UDAS 프레임워크를 제안하였고 타겟 학습 데이터가 부족한 상황에서도 안정적고 높은 성능을 보이는 것을 확인했다.

DA 방법론과 synthetic 데이터의 질이 향상함에 따라 제안한 UDAS 구조의 성능은 그와 비례하여 향상될 수 있을 것으로 예상한다.

# 6. 참고 문헌

[1]	Long, Jonathan, Evan Shelhamer, and Trevor Darrell. "Fully convolutional networks for semantic segmentation." Proceedings of the IEEE conference on computer vision and pattern recognition. 2015.
[2]	Zhao, Hengshuang, et al. "Pyramid scene parsing network." Proceedings of the IEEE conference on computer vision and pattern recognition. 2017.
[3]	Yu, Fisher, Vladlen Koltun, and Thomas Funkhouser. "Dilated residual networks." Proceedings of the IEEE conference on computer vision and pattern recognition. 2017.
[4]	Chen, Liang-Chieh, et al. "Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs." IEEE transactions on pattern analysis and machine intelligence 40.4 (2017): 834-848.
[5]	Tzeng, Eric, et al. "Adversarial discriminative domain adaptation." Proceedings of the IEEE conference on computer vision and pattern recognition. 2017.
[6]	Hoffman, Judy, et al. "Cycada: Cycle-consistent adversarial domain adaptation." International conference on machine learning. PMLR, 2018.
[7]	Tsai, Yi-Hsuan, et al. "Learning to adapt structured output space for semantic segmentation." Proceedings of the IEEE conference on computer vision and pattern recognition. 2018.
[8]	Xie, Qizhe, et al. "Self-training with noisy student improves imagenet classification." Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020.
[9]	Pham, Hieu, et al. "Meta pseudo labels." Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021.
[10] Miyato, Takeru, et al. "Virtual adversarial training: a regularization method for supervised and semi-supervised learning." IEEE transactions on pattern analysis and machine intelligence 41.8 (2018): 1979-1993.
[11] Xie, Qizhe, et al. "Unsupervised data augmentation for consistency training." arXiv preprint arXiv:1904.12848 (2019).
[12] Berthelot, David, et al. "Mixmatch: A holistic approach to semi-supervised learning." arXiv preprint arXiv:1905.02249 (2019).
[13] Sohn, Kihyuk, et al. "Fixmatch: Simplifying semi-supervised learning with consistency and confidence." arXiv preprint arXiv:2001.07685 (2020).
[14] Zheng, Zhedong, and Yi Yang. "Unsupervised scene adaptation with memory regularization in vivo." arXiv preprint arXiv:1912.11164 (2019).
[15] Yun, Sangdoo, et al. "Cutmix: Regularization strategy to train strong classifiers with localizable features." Proceedings of the IEEE/CVF International Conference on Computer Vision. 2019.
[16] Tremblay, Jonathan, et al. "Training deep networks with synthetic data: Bridging the reality gap by domain randomization." Proceedings of the IEEE conference on computer vision and pattern recognition workshops. 2018.
[17] Kar, Amlan, et al. "Meta-sim: Learning to generate synthetic datasets." Proceedings of the IEEE/CVF International Conference on Computer Vision. 2019.
[18] Andrychowicz, OpenAI: Marcin, et al. "Learning dexterous in-hand manipulation." The International Journal of Robotics Research 39.1 (2020): 3-20.
[19] Cordts, Marius, et al. "The cityscapes dataset for semantic urban scene understanding." Proceedings of the IEEE conference on computer vision and pattern recognition. 2016.
[20] Richter, Stephan R., et al. "Playing for data: Ground truth from computer games." European conference on computer vision. Springer, Cham, 2016.
[21] Cubuk, Ekin D., et al. "Autoaugment: Learning augmentation strategies from data." Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019.
[22] Hoffman, Judy, et al. "Cycada: Cycle-consistent adversarial domain adaptation." International conference on machine learning. PMLR, 2018.
[23] Tsai, Yi-Hsuan, et al. "Learning to adapt structured output space for semantic segmentation." Proceedings of the IEEE conference on computer vision and pattern recognition. 2018.
[24] Vu, Tuan-Hung, et al. "Advent: Adversarial entropy minimization for domain adaptation in semantic segmentation." Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019.

---

하지만 대량의 labeled 데이터에 의존적이라 데이터셋을 확보하는 데에 많은 시간을 들여야 한다. 픽셀 별로 annotation 하는 것은 다른 visual task 에 비해 비용과 시간이 더 많이 소요되기 때문에 원하는 도메인의 데이터를 얻기가 쉽지 않다. 이에 따라 synthetic 데이터에 대한 활용처가 높은 task로 판단하고, semantic segmentation을 downstream task로 설정하여 연구를 진행한다. 기존의 잘 알려진 task 모델을 기반으로 사용하나 synthetic 데이터를 함께 활용하여 학습하기 때문에 도메인 학습 데이터의 양이 적을 때에도 robust하고 안정적인 성능의 모델을 만들 수 있다.

UDAS 프레임워크는 K개의 augmentation 샘플 데이터를 만들고 데이터 간 consistency regularization과 maximum confidence map, confidence-based를 적용하여 unlabeled target 데이터에 pseudo-labelnig을 한다. 이 때 pseudo-labeling 하는 Teacher 모델은 synthetic 데이터로 학습된 모델을 사용하고, pseudo-labeling 한 데이터와 함께 Studnet 모델을 학습한 후 일정 이상의 학습이 되면 student 모델을 teacher 모델로 사용하여 pseudo-labeling 하는 self-training 루프를 구성한다.

본 논문에서는 Unlabeled target 데이터에 K개의 augmentation 데이터를 생성하고, 각 데이터의 prediction confidence map을 비교하여 가장 큰 confidence를 갖는 prediction 값만 취하는 maximum confidence map을 만든다. 그렇게 만들어진 pseudo-label 중에서 특정 confidence 값 이상의 예측 데이터만 학습 데이터로 사용하는 confidence-based masking을 사용함으로써 오류 전파율을 낮춘다.

UDA 연구는 도메인 분포의 차별성에 따라 적용할 수 있는 도메인이 한정적일 수 있다. 타겟 도메인 데이터에 대한 정보가 전혀 없기 때문에 도메인 분포를 파악하는 것이 보다 어려운 작업이 될 수 있고 특정 도메인 페어에 한해서만 잘 동작하는 모델이 될 수도 있다.

본 논문에서는 Unsupervised Domain Adaptation에 Semi-supervised learning기법을 결합하여 적은 Labeled target 데이터를 갖고 있는 경우에는 더욱 쉽게 학습되고 높은 성능을 보이는 모델을 만드는  방법을 제안한다.

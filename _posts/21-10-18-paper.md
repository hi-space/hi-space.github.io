---
title: ğŸ’¡ Paper
tags: paper ğŸ’¡
mathjax_autoNumber: false
---

- **Domain Adaptation with semi-supervised learning for Semantic segmentation**
- **Self-training with combined unsupervised domain adaptation and semi-supervised learning**   
    (ë¹„ì§€ë„ ë„ë©”ì¸ ì ì‘ê³¼ ì¤€ì§€ë„ í•™ìŠµì„ ê²°í•©í•œ ìê°€ í•™ìŠµ)
- **Unsupervised domain adaptation with semi-supervised learning for semantic segmentation**   
    (ì˜ë¯¸ë¡ ì  ë¶„í• ì„ ìœ„í•´ ë°˜ì§€ë„ í•™ìŠµ ë°©ë²•ì„ ì ìš©í•œ ë¹„ì§€ë„ ë„ë©”ì¸ ì ì‘)

<!--more-->

- Self-supervised learning with image based synthetic teacher model
- Synthetic ì´ë¯¸ì§€ ê¸°ë°˜ì˜ êµì‚¬ ëª¨ë¸ì„ ì´ìš©í•œ Self-training
- Semi-supervised domain adaptation for semantic segmentation
- A Unified Approch to Semi-supervised learning and domain adaptation
- Self-training for Unsupervised domain adaptation on semantic segmentation
- Self-training with Semi-supervised domain adaptation for semantic segmentation
- Self-training that combines unsupervised domain adaptation and semi-supervised learning for semantic segmentation
- Unsupervised domain adaptation with semi-supervised methods for semantic segmentation
- ê¸°ì¡´ UDAì— SSL ë°©ë²•ë¡  ì ìš©

# êµ­ë¬¸ ìš”ì•½

Data-driven ê¸°ë°˜ì˜ Supervised learningì´ ì—¬ëŸ¬ visual task ë¶„ì•¼ì—ì„œ ì¢‹ì€ ì„±ê³¼ë¥¼ ê±°ë‘¬ì˜¤ê³  ìˆë‹¤. í•™ìŠµ ëª¨ë¸ì´ ë”ìš± ì»¤ì§€ê³  ê¹Šì–´ì§ì— ë”°ë¼ ì–‘ì§ˆì˜ í•™ìŠµ ë°ì´í„°ê°€ ë”ìš± ì¤‘ìš”í•´ì§€ê³ , ê·¸ì— ë”°ë¼ ëª¨ë¸ì˜ ì„±ëŠ¥ì´ ì¢Œìš°ë˜ê³  ìˆë‹¤. í•˜ì§€ë§Œ ë¬¸ì œì™€ ë„ë©”ì¸ì— ë§ëŠ” annotation ë°ì´í„°ë¥¼ ì–»ê¸° ìœ„í•œ ë¹„ìš©ì€ human resourceê°€ ë§ì´ ë“¤ì–´ê°€ê³  ì—¬ì „íˆ ë§¤ìš° ë¹„ì‹¸ë‹¤.

<!-- ì´ëŸ¬í•œ ë°ì´í„° ë¶€ì¡± ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ì ì€ ë°ì´í„°ë¥¼ í™œìš©í•œ í•™ìŠµ ë°©ë²•ì¸ Semi-supervised learning, Unsupervised learningë‚˜ ë¹„êµì  ì–»ê¸° ì‰¬ìš´ ë‹¤ë¥¸ ë„ë©”ì¸ì˜ ë°ì´í„°ë¥¼ ì´ìš©í•œ Transfer learning ê¸°ë°˜ ë°©ì‹ì´ í™œë°œí•˜ê²Œ ì—°êµ¬ë˜ê³  ìˆë‹¤. -->

ì´ ë•Œ í˜„ì‹¤ê³¼ ìœ ì‚¬í•˜ê²Œ digital twin ì„¸ê³„ë¥¼ êµ¬ì¶•í•˜ë©´ ê°€ìƒì˜ ê³µê°„ ë‚´ì—ì„œ ì›í•˜ëŠ” Ground Truth ë°ì´í„°ë¥¼ ë¬´í•œí•˜ê²Œ ì–»ì„ ìˆ˜ ìˆê³  ë” ë‚˜ì•„ê°€ í˜„ì‹¤ ì„¸ê³„ì—ì„œ ì–»ê¸° ì–´ë ¤ìš´ ë°ì´í„°ë“¤ë„ ë¹„êµì  ì‰½ê²Œ íšë“í•  ìˆ˜ ìˆë‹¤. ì‹œë®¬ë ˆì´ì…˜ì„ í†µí•´ ì–»ì„ ìˆ˜ ìˆëŠ” synthetic ë°ì´í„°ëŠ” í˜„ì‹¤ ë°ì´í„°ì™€ distribution shiftê°€ ì¡´ì¬í•˜ê¸° ë•Œë¬¸ì— ë°ì´í„°ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ë©´ ì¢‹ì€ ì„±ëŠ¥ì˜ ëª¨ë¸ì„ ì–»ì„ ìˆ˜ ì—†ë‹¤.

ë³¸ ë…¼ë¬¸ì—ì„œëŠ” synthetic ë°ì´í„°ë¥¼ ì´ìš©í•´ ë°ì´í„° íšë“ ë¹„ìš©ì„ ì¤„ì´ê³ , ë„ë©”ì¸ í•™ìŠµ ë°ì´í„°ì— ëŒ€í•œ ì˜ì¡´ì„±ì„ ë‚®ì¶°ì„œ í•™ìŠµ ë°ì´í„°ê°€ ì ì€ í™˜ê²½ì—ì„œë„ ë†’ì€ ì„±ëŠ¥ì„ ë³´ì´ëŠ” UDAS í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•œë‹¤. 

<!-- 
ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ì‹œë®¬ë ˆì´ì…˜ì„ í†µí•´ ì–»ì„ ìˆ˜ ìˆëŠ” synthetic ë°ì´í„°ë¥¼ ì´ìš©í•´  ë°ì´í„° íšë“ ë¹„ìš©ì„ ì¤„ì´ê³ , ìœ ì‚¬í•œ ë„ë©”ì¸ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ í•™ìŠµí•˜ë”ë¼ë„ ì•ˆì •ì ì¸ ì„±ëŠ¥ì„ ë³´ì¼ ìˆ˜ ìˆë„ë¡ ë„ë©”ì¸ í•™ìŠµ ë°ì´í„°ì— ëŒ€í•œ ì˜ì¡´ì„±ì„ ë‚®ì¶”ëŠ” UDAS í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•œë‹¤. -->

UDAS í”„ë ˆì„ì›Œí¬ëŠ” Unsupervised Domain Adaptation êµ¬ì¡°ì— Semi-supervised learning ê¸°ë²•ì¸ Self-training, Consistency regularizationì„ ê²°í•©í•˜ì˜€ê³  ë„ë©”ì¸ ê°„ distribution gapì„ ì¤„ì´ê¸° ìœ„í•´ Domain CutMixë¥¼ ë„ì…í•˜ì—¬ ê¸°ì¡´ semantic segmentation UDA SOTA ëŒ€ë¹„ ë†’ì€ mIoUë¥¼ ë‹¬ì„±í•˜ì˜€ë‹¤. ë˜í•œ Supervised learning ë°©ì‹ìœ¼ë¡œ í•™ìŠµëœ ëª¨ë¸ì˜ ì„±ëŠ¥ê³¼ í¬ê²Œ ì°¨ì´ê°€ ë‚˜ì§€ ì•Šê³ , ì˜¤íˆë ¤ í•™ìŠµ ë°ì´í„°ê°€ ì ì€ ê²½ìš°ì—ëŠ” Supervised learning ë³´ë‹¤ ì•ˆì •ì ì´ê³  ë†’ì€ ì„±ëŠ¥ì„ ë³´ì´ëŠ” ê²ƒì„ í™•ì¸í•˜ì˜€ë‹¤. 

<!-- Synthetic ë°ì´í„°ë¡œ í•™ìŠµí•œ AI ëª¨ë¸ì´ í˜„ì‹¤ ì„¸ê³„ì—ì„œ ì˜ ë™ì‘í•˜ê¸° ìœ„í•´ì„œëŠ” ì‹¤ì œ ë°ì´í„°ì™€ synthetic ë°ì´í„° ê°„ distribution gapì„ ìµœì†Œí™” í•´ì•¼í•˜ëŠ”ë°, ì´ë¥¼ ìœ„í•´ Generative model, Adversarial learning, Pseudo-labeling ë“±ì˜ ê¸°ë²•ë“¤ì„ ì´ìš©í•œ Unsupervised Domain Adaptationì´ ë§ì´ ì—°êµ¬ ë˜ê³  ìˆë‹¤. í•˜ì§€ë§Œ ëŒ€ë¶€ë¶„ì˜ ì—°êµ¬ê°€ Domain shiftë¥¼ ì¤„ì´ëŠ” ë°ì—ë§Œ ëª©ì ì„ ë‘ê³  ìˆê³  unlabeled ë°ì´í„° í™œìš© ë°©ì•ˆì—ëŠ” ì§‘ì¤‘í•˜ê³  ìˆì§€ ì•Šë‹¤.

ë³¸ ë…¼ë¬¸ì—ì„œëŠ” Synthetic ë°ì´í„°ë¥¼ ì´ìš©í•œ Unsupervised Domain Adaptation (UDA)ì— Semi-supervised learning (SSL)ì„ ê²°í•©í•˜ì—¬ domain í•™ìŠµ ë°ì´í„°ì— ëŒ€í•œ ì˜ì¡´ì„±ì„ ê°ì†Œì‹œí‚¤ê³ , ê°–ê³  ìˆëŠ” unlabeled ë°ì´í„°ê¹Œì§€ íš¨ê³¼ì ìœ¼ë¡œ í™œìš©í•˜ëŠ” UDAS (UDA + SSL) ëª¨ë¸ì„ ì œì•ˆí•œë‹¤.  -->

- í•µì‹¬ì–´: Domain Adaptation, Semi-supervised learning, Data Augmentation, Pseudo-labeling, Self-Training, Consistency regularization, Synthetic data, Semantic Segmentation

# Absstract

Data-driven-based supervision learning has been performed well in various visual task fields. As the models grow larger and deeper, large amounts and high-quality training data becomes more important. However, it is very expensive to obtain labeled data suitable for the task and domain.
At this time, if the digital twin world is constructed similarly to real world, not only can the desired ground truth in virtual space be infinitely obtained, but also data that is difficult to obtain in the real world can be obtained easily. Of course, a model trained with synthetic data that can be obtained through simulation does not perform well because of the distribution shift between real data. 
In this paper, we propose a UDAS framework that achieves high performance despite the small amount of training data by using synthetic data to reduce data acquisition cost and reduce dependence on domain training data.
The UDAS framework combines Semi-supervised learning techniques such as Self-training and Consistency regulation to the Unsupervised Domain Adaptation structure. In addition, Domain CutMix is added to reduce the domain distribution gaps. 
We achieved high mIoU compared to the UDA for semantic segmentation SOTA using this methods. Moreover, it has only little difference from the model trained by the supervised learning methods and it shows more stable and higher performance than the supervised learning methods when the training data is small.

Keyword : Domain Adaptation, Semi-supervised learning, Data Augmentation, Pseudo-labeling, Self-Training, Consistency regularization, Synthetic data, Semantic Segmentation

# 1. ì„œë¡ 

ImageNetì˜ ë°ì´í„° ê³µê°œë¥¼ ì‹œì‘ìœ¼ë¡œ Data-driven ê¸°ë°˜ì˜ Supervised learningì´ ì—¬ëŸ¬ visual task ë¶„ì•¼ì—ì„œ ì¢‹ì€ ì„±ê³¼ë¥¼ ê±°ë‘ê³  ìˆë‹¤. ê²€ì¦ëœ ì¢‹ì€ ëª¨ë¸ë“¤ì´ ì´ë¯¸ ì¡´ì¬í•˜ê¸° ë•Œë¬¸ì—, íŠ¹ì • ë¬¸ì œì— ë§ëŠ” AIëª¨ë¸ì„ ë§Œë“¤ê¸° ìœ„í•´ì„œëŠ” ê·¸ì— ë§ëŠ” ì–‘ì§ˆì˜ ë°ì´í„°ë¥¼ ì–¼ë§ˆë‚˜ ë§ì´ í™•ë³´í•˜ëŠëƒê°€ ê²°êµ­ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ì¢Œìš°í•˜ê²Œ ë˜ì—ˆë‹¤.

í•˜ì§€ë§Œ íŠ¹ì • ë„ë©”ì¸ì˜ taskì— ë§ëŠ” ë°ì´í„°ë¥¼ í™•ë³´í•˜ê³ , ê·¸ ë°ì´í„°ì— labeling í•˜ëŠ” ê²ƒì€ ë§ì€ ì‹œê°„ê³¼ ë¹„ìš©ì„ í•„ìš”ë¡œ í•œë‹¤. ì´ëŸ¬í•œ ë°ì´í„° ì˜ì¡´ì ì¸ ë¬¸ì œë“¤ì„ í•´ê²°í•˜ê¸° ìœ„í•´ ë°ì´í„°ê°€ ë¶€ì¡±í•˜ë”ë¼ë„ íš¨ê³¼ì ì¸ í•™ìŠµì„ í•  ìˆ˜ ìˆëŠ” ë‹¤ì–‘í•œ ì—°êµ¬ë“¤ì´ ì§„í–‰ë˜ê³  ìˆë‹¤. ìœ ì‚¬í•œ taskì—ì„œ í•™ìŠµëœ pretrained ëª¨ë¸ì„ ê°€ì ¸ì™€ target taskì— ì ìš©í•˜ëŠ” Transfer Learning, ê·¸ë¦¬ê³  labeled dataê°€ ì ê±°ë‚˜ ì—†ì„ ë•Œ ì‚¬ìš©í•˜ëŠ” Semi-Supervised Learning, Self-Supervised Learning ë“±ì˜ ì—°êµ¬ë“¤ì´ ìˆë‹¤.

Labeling ì‘ì—…ì˜ ë¹„ìš©ì´ í° ë¬¸ì œë„ ìˆì§€ë§Œ íŠ¹ì • ìƒí™©ì— ëŒ€í•œ ë°ì´í„° ìì²´ë¥¼ ì–»ê¸° ì–´ë ¤ìš´ ê²½ìš°ë„ ìˆë‹¤. íŠ¹íˆë‚˜ ììœ¨ì£¼í–‰ì°¨ëŸ‰, ë¡œë³´í‹±ìŠ¤ ë¶„ì•¼ì—ì„œëŠ” í˜„ì‹¤ ì„¸ê³„ì—ì„œ ì–»ê¸° ì–´ë ¤ìš´ ë°ì´í„°ë“¤ì´ ì¡´ì¬í•œë‹¤. ì˜ˆë¥¼ ë“¤ì–´ êµì°¨ë¡œ í•œê°€ìš´ë°ì— ì‚¬ëŒì´ ìˆëŠ” ë°ì´í„°ë‚˜, ë¡œë´‡ì˜ ì¹´ë©”ë¼ ì‹œì ì—ì„œì˜ ë°ì´í„° ë“±ì€ ì‰½ê²Œ íšë“í•  ìˆ˜ê°€ ì—†ì„ ê²ƒì´ë‹¤.

ì´ ê²½ìš° ì‹¤ì œ ì„¸ê³„ë¥¼ digital twin í•˜ì—¬ ì‹œë®¬ë ˆì´ì…˜ì„ êµ¬ì¶•í•˜ëŠ” ë°©ì•ˆì´ ìˆë‹¤. ì‹œë®¬ë ˆì´ì…˜ì„ ì‚¬ìš©í•˜ê²Œ ë˜ë©´ í˜„ì‹¤ ì„¸ê³„ì—ì„œ ì–»ê¸° ì–´ë ¤ìš´ ìƒí™©ë“¤ì„ ì—°ì¶œí•˜ê³  ë¬¼ì²´ì˜ ìœ„ì¹˜, lighting, materials ë“±ê³¼ ê°™ì€ í™˜ê²½ì„ ììœ ìì¬ë¡œ ë³€ê²½í•˜ì—¬ ë°ì´í„°ë“¤ì„ ì¶”ì¶œí•  ìˆ˜ê°€ ìˆë‹¤. ì»´í“¨í„° ê·¸ë˜í”½ê³¼ ê³„ì‚°ì‹ì— ì˜í•´ ì´ë¯¸ì§€, depth, IMU, LiDAR, Radarì™€ ê°™ì€ ì„¼ì„œ ë°ì´í„°ì™€ ê°ì²´ì˜ bounding box, í”½ì…€ ë³„ í´ë˜ìŠ¤, optical flow ë“±ì˜ ì •í™•í•œ Ground Truth ë°ì´í„°ë“¤ì´ ìë™ìœ¼ë¡œ ìƒì„±ë˜ë„ë¡ í”„ë¡œê·¸ë˜ë° í•  ìˆ˜ ìˆë‹¤. ì´ëŸ¬í•œ synthetic ë°ì´í„°ë¥¼ ì´ìš©í•˜ë©´ human resourceê°€ ë°œìƒí•˜ëŠ” annotation ì‘ì—…ì„ ì¤„ì´ê³  ë¬´í•œíˆ ë§ì€ ë°ì´í„°ë¥¼ ìƒì„±í•´ AI í•™ìŠµì— ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.

ë¬¼ë¡  Synthetic ë°ì´í„°ë¥¼ ì‚¬ìš©í•´ í•™ìŠµí•œ ëª¨ë¸ì´ í˜„ì‹¤ ì„¸ê³„ì—ì„œ ì˜ ë™ì‘í•˜ê¸° ìœ„í•´ì„œëŠ” ì‹¤ì œ ì„¸ê³„ì™€ ì‹œë®¬ë ˆì´ì…˜ ê°„ì˜ ê²©ì°¨ê°€ ì‘ì•„ì•¼ë§Œ í•œë‹¤. í•˜ì§€ë§Œ ì‹œë®¬ë ˆì´ì…˜ì„ ìµœëŒ€í•œ ì‚¬ì‹¤ì ìœ¼ë¡œ ë§Œë“¤ê¸° ìœ„í•´ì„œëŠ” ë§ì€ ë…¸ë ¥ì´ ë“¤ì–´ê°€ê³ , ì‹¤ì œ ì„¸ê³„ì˜ ëª¨ë“  ë°ì´í„°ë“¤ì„ ëª¨ë¸ë§í•˜ê¸°ì—ëŠ” ì‰½ì§€ ì•Šì€ ì¼ì´ë‹¤.

Synthetic ë°ì´í„°ë¥¼ í™œìš©í•˜ì—¬ ëª¨ë¸ì„ í•™ìŠµí•˜ê³  í•™ìŠµëœ ëª¨ë¸ì„ í˜„ì‹¤ì—ì„œ ì‚¬ìš©í•˜ê¸° ìœ„í•´ì„œëŠ”, ë‹¤ë¥¸ ë„ë©”ì¸ì˜ ë°ì´í„°ì™€ì˜ distribution gapì„ ì¤„ì´ë©° í•™ìŠµí•˜ëŠ” ë°©ì‹ì˜ domain adaptation ë°©ë²•ë¡ ì„ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤. ë‹¤ì–‘í•œ ë°©ë²•ë“¤ì´ ì œì•ˆë˜ê³  ìˆì§€ë§Œ ëŒ€ë¶€ë¶„ì˜ ì—°êµ¬ë“¤ì€ target ë„ë©”ì¸ì— ëŒ€í•œ ì •ë³´ ì—†ì´ domain shift ìì²´ë¥¼ ê°ì†Œì‹œí‚¤ëŠ” ë°ì— ì§‘ì¤‘í•˜ê³  ìˆë‹¤. 

ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ì´ë¯¸ì§€ ê¸°ë°˜ì˜ synthetic ë°ì´í„°ë¥¼ í™œìš©í•˜ëŠ” domain adaptation ëª¨ë¸ì— semi-supervised learning ê¸°ë²•ë“¤ì„ ë„ì…í•˜ì—¬ ë„ë©”ì¸ í•™ìŠµ ë°ì´í„°ì— ëŒ€í•œ ì˜ì¡´ì„±ì„ ê°ì†Œì‹œí‚¤ê³  ë°ì´í„°ê°€ ë¶€ì¡±í•œ ìƒí™©ì—ì„œë„ ì•ˆì •ì ì¸ ì„±ëŠ¥ì„ ë³´ì´ëŠ” UDAS(UDA+SSL) í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•œë‹¤. Visual task ì¤‘ dense labeling taskë¡œ ë¶„ë¥˜ë˜ëŠ” semantic segmentationì„ downstream taskë¡œ ì„¤ì •í•˜ì˜€ê³ , synthetic ë°ì´í„°ë¥¼ ì‚¬ìš©í–ˆìŒì—ë„ ë¶ˆêµ¬í•˜ê³  ë†’ì€ ì„±ëŠ¥ì„ ë³´ì´ëŠ” ê²ƒìœ¼ë¡œ ì œì•ˆí•œ ë°©ë²•ì„ ì¦ëª…í•œë‹¤.

# 2. ê´€ë ¨ ì—°êµ¬

## 2.1. Semantic Segmentation

Semantic segmentationëŠ” ì´ë¯¸ì§€ì˜ ê° í”½ì…€ì´ ì–´ëŠ í´ë˜ìŠ¤ì— ì†í•˜ëŠ” ì§€ ì˜ˆì¸¡í•˜ëŠ” ê²ƒìœ¼ë¡œ, ì´ë¯¸ì§€ì˜ ì „ë°˜ì ì¸ ì˜ë¯¸ì™€ êµ¬ì¡°ë¥¼ íŒŒì•…í•˜ì—¬ ë” ê¹Šì´ ìˆê²Œ ì´í•´í•˜ëŠ” ì‘ì—…ì´ë‹¤. í”½ì…€ ìˆ˜ì¤€ì˜ labelingì´ ìˆ˜í–‰ë˜ê¸° ë•Œë¬¸ì— dense labeling task ë¼ê³  ë¶ˆë¦¬ê³ , ì´ëŠ” classification ì´ë‚˜ localizationì— ë¹„í•´  ì–´ë ¤ìš´ ë¬¸ì œë¡œ ë¶„ë¥˜ëœë‹¤.

Semantic segmentation ëª¨ë¸ì€ Supervised-learning ê¸°ë°˜ì˜ ë”¥ëŸ¬ë‹ ì•„í‚¤í…ì²˜ê°€ ë‚˜ì˜¤ë©´ì„œ ì„±ëŠ¥ì´ ìƒë‹¹íˆ ê°œì„ ë˜ì—ˆëŠ”ë° ìµœê·¼ì—ëŠ” ì…ë ¥ ê³µê°„ ì°¨ì›ì„ ìœ ì§€í•˜ë©° ì „ì—­ì˜ semantic ì„ ì¶”ì¶œí•˜ê¸° ìœ„í•´ encoder, decoderë¡œ êµ¬ì„±ëœ auto-encoder êµ¬ì¡°ê°€ ë§ì´ ì‚¬ìš©ë˜ê³  ìˆë‹¤. ëŒ€í‘œì ìœ¼ë¡œ FCN[1], PSPNet[2], DRN[3], DeepLab[4] ë“±ê³¼ ê°™ì€ ì•„í‚¤í…ì²˜ë“¤ì´ ì œì•ˆë˜ì—ˆìœ¼ë©° ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì´ê³  ìˆë‹¤.

í•˜ì§€ë§Œ ëŒ€ëŸ‰ì˜ labeled ë°ì´í„°ì— ì˜ì¡´ì ì´ë¼ ë°ì´í„°ì…‹ì„ í™•ë³´í•˜ëŠ” ë°ì— ë§ì€ ì‹œê°„ì„ ë“¤ì—¬ì•¼ í•œë‹¤. í”½ì…€ ë³„ë¡œ annotation í•˜ëŠ” ê²ƒì€ ë‹¤ë¥¸ visual task ì— ë¹„í•´ ë¹„ìš©ê³¼ ì‹œê°„ì´ ë” ë§ì´ ì†Œìš”ë˜ê¸° ë•Œë¬¸ì— ì›í•˜ëŠ” ë„ë©”ì¸ì˜ ë°ì´í„°ë¥¼ ì–»ê¸°ê°€ ì‰½ì§€ ì•Šë‹¤.

## 2.2. Semi-supervised learning

Semi-supervised learning(SSL)ì€ ì†ŒëŸ‰ì˜ labeled ë°ì´í„°ë¥¼ ì´ìš©í•œ í•™ìŠµ ë°©ë²•ì´ë‹¤. ì†ŒëŸ‰ì˜ labeled ë°ì´í„°ë¡œë§Œ í•™ìŠµí•˜ê²Œ ë˜ë©´ overfittingê³¼ ê°™ì€ ë¬¸ì œê°€ ë°œìƒí•  ê°€ëŠ¥ì„±ì´ í¬ê¸° ë•Œë¬¸ì— ì†ŒëŸ‰ì˜ labeled ë°ì´í„°ì™€ ëŒ€ëŸ‰ì˜ unlabeled ë°ì´í„°ë¥¼ í•¨ê»˜ ì‚¬ìš©í•˜ì—¬ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•´ semi-supervised learningì´ ì‚¬ìš©ëœë‹¤.

![](/assets/images/21-09-24-paper-self-training.png)

Self-trainingì€ ê°€ì¥ ê°„ë‹¨í•œ SSLë°©ë²•ìœ¼ë¡œ, ì†ŒëŸ‰ì˜ labeled ë°ì´í„°ë¡œ ì¶©ë¶„íˆ í•™ìŠµëœ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ unlabeled ë°ì´í„°ì— pseudo-labeling ì„ í•˜ê³ , pseudo labeled ë°ì´í„°ì™€ labeled ë°ì´í„°ë¥¼ í•¨ê»˜ í•™ìŠµì— ì‚¬ìš©í•˜ëŠ” ë°©ì‹ì´ë‹¤. Iterationì„ ë°˜ë³µí•  ìˆ˜ë¡ labeled ë°ì´í„°ê°€ ëŠ˜ì–´ë‚˜ê³  í™•ì¥ëœ ë°ì´í„°ì…‹ì„ ì´ìš©í•´ ëª¨ë¸ì˜ ì„±ëŠ¥ë„ í•¨ê»˜ í–¥ìƒë  ìˆ˜ ìˆë‹¤. í•˜ì§€ë§Œ ì²˜ìŒ pseudo-labelingì„ í•˜ëŠ” ëª¨ë¸ì˜ ì„±ëŠ¥ì´ ì¢‹ì§€ ì•Šìœ¼ë©´ error rateì´ ê³„ì†í•´ì„œ ì¦ê°€í•˜ê¸° ë•Œë¬¸ì— ì˜ëª»ëœ ë°©í–¥ìœ¼ë¡œ í•™ìŠµì´ ë  ìˆ˜ ìˆë‹¤ëŠ” ë¬¸ì œê°€ ìˆë‹¤.

![](/assets/images/21-09-24-paper-entropy-minimization.png)

ê·¸ë˜ì„œ Unlabeled dataë¥¼ í•™ìŠµì‹œí‚¬ ë•Œ pseudo-labelingì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ì§€ ì•Šê³ , pseudo-labelì˜ maximum softmax probabilityë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì˜ˆì¸¡ í™•ë¥ ì˜ confidenceë¥¼ ë†’ì´ê¸° ìœ„í•´ entropy minimizationì„ ì‚¬ìš©í•œë‹¤. ì£¼ë¡œ softmax temperatureë¥¼ ì ìš©í•˜ì—¬ unlabeled ë°ì´í„°ì˜ ì˜ˆì¸¡ê°’ì„ ê³„ì‚°í•  ë•Œ decision boundaryì—ì„œ ë©€ì–´ì§€ë„ë¡ ë” sharpí•˜ê²Œ ì˜ˆì¸¡ í™•ë¥ ê°’ì„ ì •í•œë‹¤. ë‹¨ì¼ ê¸°ë²•ìœ¼ë¡œëŠ” ì„±ëŠ¥ì´ ì¢‹ì§€ ì•Šì§€ë§Œ ìµœì‹  ì—°êµ¬ì— ë§ì´ ì ìš©í•´ ì‚¬ìš©í•˜ê³  ìˆë‹¤. 

ìµœê·¼ ë§ì´ ì‚¬ìš©ë˜ëŠ” SSL ì ‘ê·¼ë²•ì€ Consistency regularizationì„ ì‚¬ìš©í•˜ëŠ” ê²ƒìœ¼ë¡œ, unlabeled ë°ì´í„°ì— ë…¸ì´ì¦ˆë¥¼ ì¶”ê°€í•˜ë”ë¼ë„ ì˜ˆì¸¡ ë¶„í¬ëŠ” ìœ ì§€ëœë‹¤ëŠ” ì•„ì´ë””ì–´ì— ê¸°ì•ˆí•œ ë°©ë²•ì´ë‹¤. ë…¸ì´ì¦ˆê°€ ì—†ëŠ” ì›ë³¸ ë°ì´í„°ì™€ ë…¸ì´ì¦ˆê°€ ì£¼ì…ëœ ë°ì´í„°ë¥¼ ë™ì¼í•œ ì˜ˆì¸¡ ë¶„í¬ë¡œ í•™ìŠµí•˜ëŠ” ê²ƒì´ consistency regularizationì˜ ëª©í‘œì´ë‹¤.

Xie, Qizhe, et al. [8]ëŠ” teacher-student ê¸°ë°˜ì˜ self-trainingìœ¼ë¡œ, labeled ë°ì´í„°ë¡œ í•™ìŠµëœ teacher ëª¨ë¸ì„ ì´ìš©í•´ unlabeled ë°ì´í„°ì— pseudo-labeling í•˜ê³  ì´ ë°ì´í„°ì— ë…¸ì´ì¦ˆë¥¼ ì¶”ê°€í•˜ì—¬ student ëª¨ë¸ì„ í•™ìŠµí•˜ëŠ” ë°©ë²•ì´ë‹¤. Student ëª¨ë¸ì´ ì–´ëŠì •ë„ í•™ìŠµì´ ë˜ë©´ teacher ëª¨ë¸ë¡œ ì„¤ì •í•˜ê³  ë‹¤ì‹œ pseudo labelingì„ ìˆ˜í–‰í•˜ê²Œ ë§Œë“¤ì–´ ë°˜ë³µì ì¸ í•™ìŠµ íŒŒì´í”„ë¼ì¸ì„ êµ¬ì„±í–ˆê³  ì´ë¥¼ í†µí•´ SOTAë¥¼ ê¸°ë¡í•  ì •ë„ë¡œ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì¸ ì—°êµ¬ì´ë‹¤.

Pham, Hieu, et al. [9]ì€ teacher ëª¨ë¸ê³¼ student ëª¨ë¸ì„ ë™ì‹œì— í•™ìŠµí•˜ëŠ” ë°©ë²•ìœ¼ë¡œ, teacher ëª¨ë¸ì´ ì˜ í•™ìŠµë˜ì–´ ìˆì§€ ì•Šë”ë¼ë„ student ëª¨ë¸ê³¼ ìœ ê¸°ì ìœ¼ë¡œ í•™ìŠµí•  ìˆ˜ ìˆë„ë¡ Co-training ë°©ì‹ì„ ì œì•ˆí•˜ì˜€ë‹¤.

Miyato, Takeru, et al. [10]ì€ ì…ë ¥ ë°ì´í„°ì˜ adversarial transformationì„ ìƒì„±í•˜ê³  ì˜ˆì¸¡ ê²°ê³¼ ê°„ì˜ KL-divergenceë¥¼ ì¸¡ì •í•˜ì—¬ í•™ìŠµì‹œí‚¤ëŠ” ë°©ë²•ì„ ì œì•ˆí–ˆë‹¤. Xie, Qizhe, et al. [11]ì€ ë…¸ì´ì¦ˆ ëŒ€ì‹  ìµœì‹  augmentation ê¸°ë²•ì¸ AutoAugmentë¥¼ ì‚¬ìš©í•˜ì—¬ ì˜ˆì¸¡ ê²°ê³¼ ê°„ KL-divergenceë¥¼ lossë¡œ ì‚¬ìš©í•œ ë°©ë²•ìœ¼ë¡œ, ì ì€ labeled ë°ì´í„°ë§Œìœ¼ë¡œ supervised learningì„ ë›°ì–´ë„˜ëŠ” ì„±ëŠ¥ì„ ë³´ì˜€ë‹¤.

ê·¸ë¦¬ê³  Berthelot, David, et al. [12], Sohn, Kihyuk, et al. [13] ë“± Pseudo-labelingê³¼ í•¨ê»˜ augmentation, entropy minimization, consistency regularizationì„ í¬í•¨í•œ ë‹¤ì–‘í•œ ê¸°ë²•ì„ í•¨ê»˜ ì‚¬ìš©í•œ ì—°êµ¬ë“¤ì´ ë‚˜ì˜¤ê³  ìˆë‹¤.

## 2.3. Unsupervised Domain Adaptation

ì¼ë°˜ì ìœ¼ë¡œ labeled ë°ì´í„°ê°€ ë¶€ì¡±í•˜ë©´ pre-trainedëœ í° ëª¨ë¸ì„ ì´ìš©í•˜ì—¬ transfer learningì„ ìˆ˜í–‰í•˜ëŠ” ë°©ë²•ì´ ìˆë‹¤. Transfer learningì„ í•˜ê¸° ìœ„í•´ì„œë„ í•´ë‹¹ ë„ë©”ì¸ì— ë§ëŠ” ì¼ì • ì–‘ì˜ labeled ë°ì´í„°ê°€ í•„ìˆ˜ì ìœ¼ë¡œ í•„ìš”í•˜ë‚˜, Domain Adaptation(DA)ì€ labeled ë°ì´í„°ê°€ ì—†ë”ë¼ë„ ì´ì „ task ì—ì„œ ë°°ìš´ ì§€ì‹ì„ í™œìš©í•˜ì—¬ ìƒˆë¡œìš´ ìƒí™©ì—ì„œë„ ì˜ ì˜ˆì¸¡í•˜ë„ë¡ í•™ìŠµí•œë‹¤. ì¦‰, ë‹¤ë¥¸ ë„ë©”ì¸(Source)ì—ì„œ ë°°ìš´ ì§€ì‹ì„ í™œìš©í•´ ì‹¤ì œ  ë„ë©”ì¸(Target) ëª¨ë¸ì— ì ìš©í•˜ê¸° ìœ„í•œ ë°©ë²•ìœ¼ë¡œ, Zero-shot learning, few-shot learning, self-supervised learningê³¼ í•¨ê»˜ sample-efficient learningì˜ í•œ ìœ í˜•ì´ë‹¤.

![](/assets/images/21-09-24-paper-domain-adaptation.png)

Source ë„ë©”ì¸ê³¼ target ë„ë©”ì¸ ê°„ì˜ ë°ì´í„° ë¶„í¬ê°€ ë‹¤ë¥´ê¸° ë•Œë¬¸ì— source ë„ë©”ì¸ì—ì„œ í•™ìŠµëœ ëª¨ë¸ì„ target ë„ë©”ì¸ì— ì‚¬ìš©í•˜ê²Œ ë˜ë©´ distribution shift(ë˜ëŠ” domain gap)ì™€ dataset biasì— ì˜í•´ ì˜¤ì°¨ê°€ ë°œìƒí•  ê°€ëŠ¥ì„±ì´ í¬ë‹¤. ê·¸ë ‡ê¸° ë•Œë¬¸ì— ë„ë©”ì¸ì´ ë‹¤ë¥¸ í•™ìŠµë°ì´í„°ë¥¼ í™œìš©í•˜ê¸° ìœ„í•´ì„œëŠ” ë‘ ë„ë©”ì¸ì˜ ë¶ˆì¼ì¹˜ì„±ì„ ì¤„ì´ë©´ì„œ ë™ì‹œì— task lossë„ ìµœì†Œí™” í•˜ëŠ” ëª¨ë¸ì„ ë§Œë“¤ì–´ì•¼ í•œë‹¤.

![](/assets/images/21-09-24-paper-adaptation-level.png)

DA ë¬¸ì œë¥¼ í’€ê¸° ìœ„í•œ ì£¼ìš” ì „ëµì€ source ë„ë©”ì¸ê³¼ target ë„ë©”ì¸ ì‚¬ì´ì˜ ê²©ì°¨ë¥¼ ì¤„ì„ìœ¼ë¡œì¨ ì˜ˆì¸¡ ëª¨ë¸ì˜ ì„±ëŠ¥ ì €í•˜ë¥¼ ë§‰ëŠ” ë°©ë²•ìœ¼ë¡œ Input-level, feature-level, output-levelì—ì„œ ê°ê° adaptation ëª¨ë“ˆì„ ì¶”ê°€í•˜ì—¬ ë„ë©”ì¸ ê°„ ë¶ˆì¼ì¹˜ì„±ì„ ì¤„ì¼ ìˆ˜ ìˆë‹¤.

ê·¸ ì¤‘ Unsupervised Domain Adaptation(UDA)ëŠ” Target ë„ë©”ì¸ì˜ unlabeled ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´ ë‹¤ë¥¸ ë„ë©”ì¸ì˜ labeled ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ëŠ” ì¼ì¢…ì˜ Semi-supervised learningì˜ ë³€í˜•ì´ë¼ê³  ë³¼ ìˆ˜ ìˆë‹¤. Target ë„ë©”ì¸ì˜ labeled ë°ì´í„°ëŠ” í•„ìš”í•˜ì§€ ì•Šì§€ë§Œ ì¶©ë¶„í•œ ì–‘ì˜ Unlabeled target ë°ì´í„°ê°€ í•„ìš”í•˜ë‹¤.

Tzeng, Eric, et al.[5] ëŠ” UDAì— GANì„ ë„ì…í•˜ì—¬ adversarial adaptationì„ ì²˜ìŒ ì œì•ˆí•œ ë…¼ë¬¸ìœ¼ë¡œ, feature levelì— domain discriminatorë¥¼ ì¶”ê°€í•˜ê³  adversarial lossë¥¼ ì‚¬ìš©í•˜ì—¬ domain discrepancyë¥¼ ìµœì†Œí™” í•˜ëŠ” ë°©ë²•ì„ ì œì•ˆí–ˆë‹¤. 

Hoffman, Judy, et al.[6] ì—ì„œëŠ” feature levelì— adversarial learningì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ê³ , input-levelì—ì„œ source ë„ë©”ì¸ ì´ë¯¸ì§€ë¥¼ target ì´ë¯¸ì§€ì™€ ìœ ì‚¬í•œ ìŠ¤íƒ€ì¼ë¡œ ìƒì„±í•˜ë„ë¡ generative ëª¨ë¸ì„ ë„ì…í•œ ë°©ë²•ì„ ì œì•ˆí–ˆë‹¤. Generative ëª¨ë¸ì„ ë„ì…í•¨ìœ¼ë¡œì¨ input ë°ì´í„°ì˜ ìˆ¨ê²¨ì§„ í™•ë¥ ë¶„í¬ë¥¼ ì°¾ì•„ë‚´ê³  ìœ ì‚¬í•œ í™•ë¥ ë¶„í¬ì˜ ë°ì´í„°ë¥¼ ìƒì„±í•´ ì‚¬ìš©í•˜ëŠ” ë°©ì‹ì´ë‹¤. 

Tsai, Yi-Hsuan, et al.[7] ì—ì„œëŠ” ë‘ ë„ë©”ì¸ì˜ ì´ë¯¸ì§€ ìŠ¤íƒ€ì¼ì´ ë‹¤ë¥´ë”ë¼ë„ segmentation ê²°ê³¼ì— í¬í•¨ë˜ì–´ ìˆëŠ” ê³µê°„ ë ˆì´ì•„ì›ƒ, local context ë“±ì˜ êµ¬ì¡°í™”ëœ íŠ¹ì„±ì€ ìœ ì‚¬í•˜ê¸° ë•Œë¬¸ì— output-levelì— adversarial learningì„ ë„ì…í•˜ì—¬ segmentation ì˜ˆì¸¡ ê²°ê³¼ì˜ ë¶„í¬ë¥¼ ìœ ì‚¬í•˜ê²Œ í•™ìŠµí•˜ëŠ” ë°©ë²•ì„ ì œì•ˆí–ˆë‹¤.

![](/assets/images/21-09-24-paper-da-methods.png)

ì •ë¦¬í•˜ë©´ ê¸°ì¡´ì˜ UDA ì—°êµ¬ëŠ” (a) source ë„ë©”ì¸ê³¼ target ë„ë©”ì¸ì˜ ë¶ˆì¼ì¹˜ì„±ì„ ëª…ì‹œì ìœ¼ë¡œ ì¸¡ì •í•˜ì—¬ lossë¡œ ì‚¬ìš©í•˜ëŠ” ë°©ë²• (b) domain confusionì„ ìœ„í•´ discriminatorì™€ adversarial lossë¥¼ ì‚¬ìš©í•˜ëŠ” ë°©ë²• (c) source ë„ë©”ì¸ ì´ë¯¸ì§€ë¥¼ target ì´ë¯¸ì§€ì™€ ìœ ì‚¬í•œ ìŠ¤íƒ€ì¼ë¡œ ìƒì„±í•˜ê¸° ìœ„í•´ input-levelì— generative ëª¨ë¸ì„ ì¶”ê°€í•˜ì—¬ appearance gapì„ ì¤„ì´ëŠ” ë°©ë²• (d) task ëª¨ë¸ê³¼ self-supervised ëª¨ë¸ì„ í•¨ê»˜ í•™ìŠµì‹œí‚¤ëŠ” ë°©ë²•ìœ¼ë¡œ ë¶„ë¥˜ë  ìˆ˜ ìˆë‹¤. ê·¸ ì™¸ì—ë„ Classifier discrepancy ë¶„ì„, Entropy minimization, Curriculum learning, Multi-tasking learning ë°©ë²•ë“¤ì´ ìˆê³  ì´ëŸ¬í•œ ë°©ë²•ë¡ ë“¤ì„ ë³€í˜•í•˜ê±°ë‚˜ ì ì ˆì´ ì„ì–´ê°€ë©° ë‹¤ì–‘í•œ ë°©ë²•ë“¤ì´ ì œì•ˆë˜ê³  ìˆë‹¤. 

ë‹¤ë§Œ UDA ì—°êµ¬ëŠ” ë„ë©”ì¸ ë¶„í¬ì˜ ì°¨ë³„ì„±ì— ë”°ë¼ ì ìš©í•  ìˆ˜ ìˆëŠ” ë„ë©”ì¸ì´ í•œì •ì ì¼ ìˆ˜ ìˆë‹¤. íƒ€ê²Ÿ ë„ë©”ì¸ ë°ì´í„°ì— ëŒ€í•œ ì •ë³´ê°€ ì „í˜€ ì—†ê¸° ë•Œë¬¸ì— ë„ë©”ì¸ ë¶„í¬ë¥¼ íŒŒì•…í•˜ëŠ” ê²ƒì´ ë³´ë‹¤ ì–´ë ¤ìš´ ì‘ì—…ì´ ë  ìˆ˜ ìˆê³  íŠ¹ì • ë„ë©”ì¸ í˜ì–´ì— í•œí•´ì„œë§Œ ì˜ ë™ì‘í•˜ëŠ” ëª¨ë¸ì´ ë  ìˆ˜ë„ ìˆë‹¤.

# 3. ì œì•ˆ ë°©ë²•

## 3.1. ê°œìš”

![](/assets/images/21-10-18-paper-udas-overview.png)

UDASì˜ ì „ì²´ì ì¸ í”„ë ˆì„ì›Œí¬ëŠ” í¬ê²Œ Domain Adaptation(DA)ê³¼ Semi-Supervised Learning(SSL) ë‘ ëª¨ë“ˆë¡œ ë‚˜ëˆŒ ìˆ˜ ìˆë‹¤. 

![](/assets/images/21-10-18-paper-udas-domain-adaptation.png)

DA ëª¨ë“ˆì€ source ë„ë©”ì¸ê³¼ target ë„ë©”ì¸ì„ í•¨ê»˜ í™œìš©í•˜ì—¬ ë„ë©”ì¸ ê°„ distribution gapì„ ì¤„ì´ê³ , ë™ì‹œì— target ë„ë©”ì¸ì—ì„œ ì˜ ë™ì‘í•˜ë„ë¡ task ëª¨ë¸ì„ í•™ìŠµí•œë‹¤. ê¸°ë³¸ì ìœ¼ë¡œ target ë„ë©”ì¸ì— ëŒ€í•œ label ë°ì´í„°ê°€ ì—†ë‹¤ê³  ê°€ì •í•˜ê³  UDA êµ¬ì¡°ë¡œ êµ¬ì„±ë˜ì–´ ìˆë‹¤. Input-level ì—ì„œ GAN ê¸°ë°˜ì˜ ëª¨ë¸ì„ í†µí•´ appearance gapì„ ì¤„ì´ê³  ouput-level ì—ì„œëŠ” multi-levelë¡œ adversarial learningì„ ë„ì…í•˜ì—¬ prediction ê²°ê³¼ì— ëŒ€í•œ gapì„ ì¤„ì¸ë‹¤.

Labeled target ë°ì´í„°ê°€ ì—†ë”ë¼ë„ taskë¥¼ ìˆ˜í–‰í•  ìˆ˜ ìˆìœ¼ë‚˜, labeled target ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°ì—ëŠ” Domain CutMix ë°©ë²•ì„ ì´ìš©í•˜ì—¬ ë„ë©”ì¸ì´ ì„œë¡œ ë‹¤ë¥¸ ë°ì´í„°ë¥¼ ê²°í•©í•˜ê³  ìƒˆë¡œìš´ source ë°ì´í„°ë¡œ ì‚¬ìš©í•œë‹¤. ê°„ì ‘ì ìœ¼ë¡œ labeled target ë°ì´í„°ë¥¼ ì‚¬ìš©í•¨ìœ¼ë¡œì¨ target ë°ì´í„°ì— ëŒ€í•œ ì˜ì¡´ì„±ì´ ë†’ì•„ì§€ì§€ ì•Šë„ë¡ ì„¤ê³„ë˜ì—ˆë‹¤.

![](/assets/images/21-10-18-paper-udas-semi-supervised-learning.png)

SSL ëª¨ë“ˆì€ teacher-student ëª¨ë¸ êµ¬ì¡°ë¡œ, DAë¥¼ í†µí•´ í•™ìŠµëœ task ëª¨ë¸ì´ teacher ëª¨ë¸ì´ ë˜ì–´ unlabeled target ë°ì´í„°ì— pseudo-lableing í•´ì¤€ë‹¤. ì‹¤ì œ target taskë¥¼ ìˆ˜í–‰í•˜ëŠ” student ëª¨ë¸ì€ pseudo-labeling ëœ ë°ì´í„°ë“¤ì„ ì´ìš©í•˜ì—¬ supervised learning êµ¬ì¡°ë¡œ í•™ìŠµí•œë‹¤. Student ëª¨ë¸ì´ ì¼ì • ì´ìƒ í•™ìŠµì´ ë˜ë©´ student ëª¨ë¸ì´ teacher ëª¨ë¸ì´ ë˜ì–´ pseudo-labeling ê³¼ì •ë¶€í„° ë°˜ë³µí•˜ëŠ” self-training ë£¨í”„ë¥¼ êµ¬ì„±í•œë‹¤. ì´ë¥¼ í†µí•´ ì ì§„ì ìœ¼ë¡œ ëª¨ë¸ì´ ì„±ì¥í•  ìˆ˜ ìˆë„ë¡ ìœ ë„í•œë‹¤.

ë˜í•œ ì‹ ë¢°ë„ ë†’ì€ pseudo-labelingì„ ìœ„í•˜ì—¬ consistency regularization, maximum confidence map, confidence-based maskingì„ ì ìš©í•˜ì—¬ knowledge distilation ì‹œì— ë°œìƒí•  ìˆ˜ ìˆëŠ” ì˜¤ë¥˜ ì „íŒŒìœ¨ì„ ë‚®ì¶˜ë‹¤.

ë³¸ ë…¼ë¬¸ì˜ contributionì€ ë‹¤ìŒê³¼ ê°™ë‹¤.

-	Domain Adaptationì— SSL ê¸°ë²•ì¸ Self-trainingê³¼ Consistency regularizationì„ ì ìš©í•œ ëª¨ë¸ êµ¬ì¡° ì œì•ˆ
-	Labeled target ë°ì´í„° ì–‘ì„ ì¤„ì˜€ì„ ë•Œ Supervised learningë³´ë‹¤ ì•ˆì •ì ì¸ ì„±ëŠ¥
-	ê¸°ì¡´ UDA SOTA ëŒ€ë¹„ ë†’ì€ mIoU ë‹¬ì„±

## 3.2. Image Translation

![](/assets/images/21-10-18-paper-image-translation.png)

ì‹œê°ì ìœ¼ë¡œ ë´¤ì„ ë•Œ í° ì°¨ì´ê°€ ìˆëŠ” ë°ì´í„°ëŠ” ê·¸ë§Œí¼ í° ë„ë©”ì¸ gap ì´ ìˆì„ ê²ƒì´ë¼ê³  ê°€ì •í•  ìˆ˜ ìˆë‹¤. Input-levelì—ì„œì˜ ë„ë©”ì¸ gapì„ ì¤„ì´ê¸° ìœ„í•´ source ë„ë©”ì¸ì˜ image appearanceê°€ target domainê³¼ ìœ ì‚¬í•˜ë„ë¡ ìƒì„±í•´ë‚¸ë‹¤. ëŒ€í‘œì ì¸ image-to-image translation ë°©ë²•ì¸ CycleGANì„ ì´ìš©í•˜ì—¬ target domain ìŠ¤íƒ€ì¼ì˜ ì´ë¯¸ì§€ë¡œ ë³€í˜•ì‹œì¼œ domain adaptation ëª¨ë¸ì˜ source ë°ì´í„°ë¡œ ì‚¬ìš©í•œë‹¤.

$$ L(G, F, D_s, D_t) = L_{GAN}(G, D_t, S, T) + L_{GAN}(F, D_s, T, S) + \lambda L_{cyc}(G, F) $$

$$ G^*, F^* = arg min_{G, F} max_{D_s, D_t} L(G, F, D_s, D_t) $$

$S$ê°€ source domain ìƒ˜í”Œ, $T$ê°€ target domain ìƒ˜í”Œì´ë¼ê³  í–ˆì„ ë•Œ $S \rightarrow T$ ì˜ adversarial lossì™€ $T \rightarrow S$ adversarial lossì— ê°ê° ì›ë³¸ìœ¼ë¡œ ë³µêµ¬í•˜ëŠ” cycle consistency lossë¥¼ ë”í•´ì¤€ ê°’ì´ ì´ Lossê°€ ë˜ê³ , ì´ Lossë¥¼ ìµœì†Œí™” í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ $G: X \rightarrow Y$ ì™€ $F: Y \rightarrow X$ ë¥¼ í•™ìŠµì‹œí‚¨ë‹¤.

Source ë„ë©”ì¸ê³¼ target ë„ë©”ì¸ì˜ ë¶„í¬ë¥¼ êµ¬ë¶„í•  ìˆ˜ ì—†ë„ë¡ í•™ìŠµì‹œí‚¤ëŠ” ë°©ë²•ì´ê¸° ë•Œë¬¸ì— ë„ë©”ì¸ ê°„ gapì´ ì¤„ì—ˆì„ ê²ƒì´ë¼ê³  ê¸°ëŒ€í•  ìˆ˜ ìˆê³ , $G: S \rightarrow T$ ëª¨ë¸ì„ í†µí•´ ìƒì„±ëœ ë°ì´í„°ëŠ” target ë„ë©”ì¸ ë¶„í¬ì— ê°€ê¹Œìš´ í˜•íƒœê°€ ë  ìˆ˜ ìˆë‹¤. ê²°ê³¼ì ìœ¼ë¡œ Source ë„ë©”ì¸ ë°ì´í„°ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ëŠ” ê²ƒ ë³´ë‹¤ Target ë„ë©”ì¸ì˜ ìŠ¤íƒ€ì¼ì— ë§ê²Œ ì´ë¯¸ì§€ë¥¼ ì¬ìƒì„±í•´ì„œ target task ëª¨ë¸ì— ì ìš©í–ˆì„ ë•Œ ì„±ëŠ¥ì´ í–¥ìƒë˜ì—ˆë‹¤.

## 3.3. Unsupervised Domain Adaptation

![](/assets/images/21-10-18-paper-uda.png)

UDA ëª¨ë“ˆì€ __ëª¨ë¸ì„ ë³¸ë”° ë§Œë“  Adversarial Generative Model êµ¬ì¡°ë¡œ, ì…ë ¥ìœ¼ë¡œ image translation ëœ source ì´ë¯¸ì§€ì™€ label, ê·¸ë¦¬ê³  label ì´ ì—†ëŠ” target ì´ë¯¸ì§€ë¥¼ ì‚¬ìš©í•˜ê³  output-levelì— discriminatorë¥¼ ì¶”ê°€í•˜ì—¬ input-level adaptation, output-level adaptationì„ ìˆ˜í–‰í•œë‹¤.

ë¨¼ì € image translation ëœ source ì´ë¯¸ì§€ì™€ labelsì„ ì´ìš©í•˜ì—¬ source ë„ë©”ì¸ ê¸°ë°˜ìœ¼ë¡œ segmentation ëª¨ë¸ì„ í•™ìŠµí•œë‹¤. ì‚¬ìš©ëœ ëª¨ë¸ì€ multi-level segmentation ëª¨ë¸ë¡œ, ê¸°ì¡´ segmentation ëª¨ë¸ì—ì„œ ë§ˆì§€ë§‰ ë ˆì´ì–´ë¡œ predictioní•˜ëŠ” primary classifierì™€ ê·¸ ì§ì „ ë ˆì´ì–´ë¡œ prediction í•˜ëŠ” auxiliary classifierë¡œ êµ¬ì„±ëœë‹¤. 

$$ L_{seg}^p = -\sum_{h=1}^{H}\sum_{w=1}^{W}\sum_{c=1}^{C} y_s^i log(F_p(x_s^i))  $$

$$ L_{seg}^a = -\sum_{h=1}^{H}\sum_{w=1}^{W}\sum_{c=1}^{C} y_s^i log(F_a(x_s^i)) $$

$$ L_{seg} = -\sum_{h=1}^{H}\sum_{w=1}^{W}\sum_{c=1}^{C} y_s^i log(F_p(x_s^i)) \\ \qquad \qquad \qquad - \lambda_{seg} \sum_{h=1}^{H}\sum_{w=1}^{W}\sum_{c=1}^{C} y_s^i log(F_a(x_s^i)) $$

Primary classifier ì™€ auxiliary classifierë¥¼ ëª¨ë‘ í•™ìŠµì— ì´ìš©í•˜ê¸° ìœ„í•´ Primary classifier loss $L_{seg}^p$ì™€ auxiliary classifier loss $L_{seg}^a$ë¥¼ ë”í•˜ì—¬ ì „ì²´ segmentation lossë¡œ ì‚¬ìš©í•œë‹¤. ì—¬ê¸°ì„œ $\lambda_{seg}$ì€ auxiliary classifier lossì— ëŒ€í•œ ê°€ì¤‘ì¹˜ë¥¼ ìœ„í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ëœ»í•œë‹¤. Segmentation loss functionì€ pixel-wise cross-entropy lossë¥¼ ì‚¬ìš©í•œë‹¤.

ì´ë ‡ê²Œ í•™ìŠµëœ Segmentation ëª¨ë¸ì€ source ë„ë©”ì¸ ë°ì´í„°ë§Œì„ ì´ìš©í•˜ì—¬ í•™ìŠµë˜ê¸° ë•Œë¬¸ì— ë°ì´í„° distributionì´ ë‹¤ë¥¸ target ë„ë©”ì¸ì—ì„œëŠ” ì˜¤ì°¨ê°€ ë°œìƒí•  í™•ë¥ ì´ ë†’ë‹¤. Source ë„ë©”ì¸ê³¼ target ë„ë©”ì¸ ê°„ ë¶ˆì¼ì¹˜ì„±ì„ ì¤„ì´ê¸° ìœ„í•œ ë°©ë²•ìœ¼ë¡œ source domain classifierì™€ target domain classifierì— discriminatorì™€ adversarial lossë¥¼ ì¶”ê°€í•œë‹¤.

$$ L_{adv}^p = \mathbb{E}[ log(D_p(F_a(x_s^i))) + log (1 - D_p(F_p(x_t^j))) ] $$

$$ L_{adv}^a = \mathbb{E}[ log(D_a(F_a(x_s^i))) + log (1 - D_a(F_a(x_t^j))) ] $$

$$ L_{adv} = \mathbb{E}[ log(D_p(F_a(x_s^i))) + log (1 - D_p(F_p(x_t^j))) ] \\ \qquad  \qquad  \qquad + \lambda_{adv} \mathbb{E}[ log(D_a(F_a(x_s^i))) + log (1 - D_a(F_a(x_t^j))) ]$$

Discriminator $D$ëŠ” source prediction $F(x_s)$ì™€ target prediction $F(x_t)$ë¥¼ êµ¬ë¶„í•˜ì§€ ëª»í•˜ë„ë¡ í•™ìŠµë˜ì–´ domain confusionì„ ì•¼ê¸°í•œë‹¤. Primary classifierì™€ auxiliary classifierì— ëª¨ë‘ ì ìš©ë˜ê¸° ë•Œë¬¸ì— multi-levelë¡œ adaptationì´ ì¼ì–´ë‚˜ê²Œ ë˜ê³  ì „ì²´ adversarial lossëŠ” $L_{adv}^p$ì™€ $L_{adv}^a$ë¥¼ ë”í•œ ê°’ì´ë‹¤.

Source ë„ë©”ì¸ê³¼ target ë„ë©”ì¸ ê°„ì˜ ë¶ˆì¼ì¹˜ì„±ì„ ì¤„ì¸ ì´í›„ì—ëŠ” target ë„ë©”ì¸ ë‚´ì˜ ë¶ˆì¼ì¹˜ì„±ì„ ì¤„ì—¬ì£¼ëŠ” ì‘ì—…ì´ í•„ìš”í•˜ë‹¤. ì´ëŠ” ì´í›„ pseudo-labeling ì‘ì—…ì„ ìˆ˜í–‰í•  ë•Œ target ë„ë©”ì¸ ë‚´ì˜ ë¶ˆí™•ì‹¤ì„±ì„ ì¤„ì´ê¸° ìœ„í•¨ì´ë‹¤. Primary classifierì™€ auxiliary classifierì˜ predictionì€ ìœ ì‚¬í•œ ê²°ê³¼ë¥¼ ê°€ì ¸ì•¼í•˜ê¸° ë•Œë¬¸ì— KL-divergence lossë¥¼ ë„ì…í•œë‹¤.

$$ L_{kl} = -\sum_{h=1}^{H}\sum_{w=1}^{W}\sum_{c=1}^{C} F_a(x_t^i) log(F_p(x_t^i)) \\ \qquad \qquad \qquad - \sum_{h=1}^{H}\sum_{w=1}^{W}\sum_{c=1}^{C} F_p(x_t^i) log(F_a(x_t^i)) $$

ì¦‰, UDAì˜ ì „ì²´ lossëŠ” segmentation lossì™€ adversarial loss, kl-divergence lossë¥¼ ë”í•œ ê°’ì´ë‹¤. 

$$ L_{total} = L_{seg} + L_{adv} + L_{kl}$$

ì´ë¡œì¨ adversarial learningê³¼ multi-level adaptationì„ í†µí•´ source ë„ë©”ì¸ì—ì„œ í•™ìŠµí•œ featureë“¤ì´ target ë„ë©”ì¸ ì˜ì—­ìœ¼ë¡œ ì „ë‹¬ë˜ê³  inter-domain knowledge, intra-domain knowledgeë¥¼ ë™ì‹œì— í•™ìŠµí•  ìˆ˜ ìˆë‹¤. 

## 3.4. Domain CutMix

Labeled target ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš°ì—ëŠ” 3.3.ì˜ ì„œìˆ  ë‚´ìš©ì²˜ëŸ¼ UDA êµ¬ì¡°ë¡œ ë™ì‘í•  ìˆ˜ ìˆì§€ë§Œ ì†ŒëŸ‰ì˜ labeled ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš° target ë„ë©”ì¸ì— ëŒ€í•œ ì •ë³´ë¥¼ í† ëŒ€ë¡œ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ë”ìš± í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆë‹¤. Labeled target ë°ì´í„°ë¥¼ supervised learning í˜•íƒœë¡œ ê·¸ëŒ€ë¡œ ì´ìš©í•˜ê²Œ ë˜ë©´ ë‹¹ì¥ì˜ ì„±ëŠ¥ì€ ì¢‹ì•„ì§ˆ ìˆ˜ ìˆê² ì§€ë§Œ target í•™ìŠµ ë°ì´í„°ì— ëŒ€í•œ ì˜ì¡´ì„±ì´ ê°•í•´ì ¸ í•™ìŠµí•˜ì§€ ëª»í•œ ë°ì´í„°ì— ëŒ€í•œ ì¶”ë¡  ëŠ¥ë ¥ì´ ë–¨ì–´ì§€ê³ , source ë„ë©”ì¸ ë°ì´í„°ê°€ ì¶”ê°€ ëì„ ë•Œ ëª¨ë¸ í•™ìŠµì— í˜¼ë€ì„ ì•¼ê¸°í•  ìˆ˜ ìˆë‹¤. ì´ëŸ¬í•œ ë¬¸ì œë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•´ UDAS í”„ë ˆì„ì›Œí¬ì—ì„œëŠ” ë„ë©”ì¸ ë…ë¦½ì ìœ¼ë¡œ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ê¸° ìœ„í•´ source ë„ë©”ì¸ ìƒ˜í”Œê³¼ target ë„ë©”ì¸ ìƒ˜í”Œì„ ê²°í•©í•˜ì—¬ í•™ìŠµ ë°ì´í„°ë¡œ ì‚¬ìš©í•œë‹¤. 

![](/assets/images/21-10-18-paper-domain-cutmix.png)

Semantic segmentationì€ ê° í”½ì…€ì˜ ì˜ˆì¸¡ê°’ê³¼ ë”ë¶ˆì–´ ì£¼ë³€ pixelì— ëŒ€í•œ ì§€ì—­ì ì¸ ì˜ˆì¸¡ ì •ë³´ë„ í•„ìš”í•œë°, í•œ ì´ë¯¸ì§€ì— source ë„ë©”ì¸ ì´ë¯¸ì§€ì™€ target ë„ë©”ì¸ ì´ë¯¸ì§€ê°€ ê²°í•©ë˜ì–´ ìˆìœ¼ë©´ ë„ë©”ì¸ê³¼ ê´€ê³„ ì—†ì´ ì´ë¯¸ì§€ ìì²´ì˜ representationì„ í•™ìŠµí•  ìˆ˜ ìˆë‹¤. ì´ë¥¼ ìœ„í•´ CutMix[14]ì˜ í˜•íƒœë¡œ ë‘ ë„ë©”ì¸ì˜ ì´ë¯¸ì§€ë¥¼ ì„ì˜ë¡œ ê²°í•©ì‹œì¼œ í•™ìŠµ ë°ì´í„°ë¡œ ì‚¬ìš©í•œë‹¤.

CutMixëŠ” ë‘ ì´ë¯¸ì§€ë¥¼ ì„ì–´ í•˜ë‚˜ì˜ ì´ë¯¸ì§€ì— í‘œí˜„í•˜ëŠ” Data augmentation ë°©ë²•ìœ¼ë¡œ, ì´ë¯¸ì§€ì˜ ì „ì²´ì ì¸ ì˜ì—­ì„ ë³´ê³  í•™ìŠµí•  ìˆ˜ ìˆë„ë¡ ìœ ë„í•˜ê¸° ë•Œë¬¸ì— Regularizationê³¼ Localization ì„±ëŠ¥ì„ ë†’ì¼ ìˆ˜ ìˆë‹¤. CutMixì˜ ì•„ì´ë””ì–´ì— ê¸°ë°˜í•œ Domain CutMixëŠ” ë„ë©”ì¸ì´ ë‹¤ë¥¸ ë‘ ì´ë¯¸ì§€ë¥¼ ì„ì–´ í•˜ë‚˜ì˜ ì´ë¯¸ì§€ë¡œ ìƒì„±í•œë‹¤.

$$ r_x \sim Unif \{ 0, W \}, r_w = W \sqrt {1- \lambda}
\\
r_y \sim Unif \{ 0, H \}, r_h = H \sqrt {1- \lambda}
\\
B = \{ r_x , r_y, r_w, r_h \}
\\
M \in \left\{0, 1\right\}^{W \times H}
$$

ìš°ì„  ë°ì´í„° ìƒì„±ì„ ìœ„í•´ ë§¤ ë°°ì¹˜ë§ˆë‹¤ ëœë¤ìœ¼ë¡œ bounding box $B$ ì‚¬ì´ì¦ˆë¥¼ ê²°ì •í•˜ê³ , $B$ ì‚¬ì´ì¦ˆì— ë§ê²Œ binary ë§ˆìŠ¤í¬ $M$ì„ ìƒì„±í•œë‹¤. $\lambda$ëŠ” ì˜ì—­ì˜ ë¹„ìœ¨ì„ ëœ»í•˜ëŠ” íŒŒë¼ë¯¸í„°ì´ë‹¤. 

$$
\tilde {x} = M \odot x_s + (1-M) \odot x_t
\\
\tilde {y} = \lambda_{y_s} + (1- \lambda)y_t
$$

ê·¸ í›„, Source ë„ë©”ì¸ì˜ ìƒ˜í”Œ $x_s$ ì—ì„œ $M$ ì˜ì—­ë§Œí¼ ì§€ìš°ê³ , ê·¸ ë¶€ë¶„ì— target ë„ë©”ì¸ ìƒ˜í”Œ $x_t$ë¥¼ ì±„ìš´ë‹¤. ë§ˆì°¬ê°€ì§€ë¡œ source ë„ë©”ì¸ ìƒ˜í”Œì— ëŒ€í•œ label ë°ì´í„°ì¸ $y_s$ì—ì„œë„ $\tilde{x} $ì™€ í˜ì–´ë˜ëŠ” labeled cutmix ë°ì´í„°ë¥¼ ë§Œë“¤ì–´ë‚¸ë‹¤.

ì´ë ‡ê²Œ ìƒì„±ëœ cutmix ë°ì´í„°ì— augmentation ê¸°ë²•ì¸ AutoAugmentë¥¼ ì·¨í•´ì„œ ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ ìƒì„±í•˜ê²Œ ëœë‹¤. ì´ ë°ì´í„°ëŠ” ê¸°ì¡´ì— í•™ìŠµì— ì‚¬ìš©ë˜ë˜ source ì´ë¯¸ì§€ ëŒ€ì‹  ì…ë ¥ ë°ì´í„°ë¡œ ì‚¬ìš©í•˜ì—¬ segmentation ëª¨ë¸ì„ í•™ìŠµì‹œí‚¨ë‹¤. ì´ë¯¸ì§€ ë°ì´í„°ì™€ labelì´ ëª¨ë‘ ì¡´ì¬í•˜ê¸° ë•Œë¬¸ì— segmentation ëª¨ë¸ì„ supervised learning ë°©ì‹ìœ¼ë¡œ í•™ìŠµì‹œí‚¬ ìˆ˜ ìˆë‹¤. 

Domain CutMix ë°ì´í„°ëŠ” ëœë¤í•˜ê²Œ ê²°í•©ë˜ê³  augmentation ê¹Œì§€ ì·¨í•´ì¡Œê¸° ë•Œë¬¸ì— í˜„ì‹¤ê³¼ ê±°ë¦¬ê°€ ë¨¼ ì´ì§ˆì ì¸ ë°ì´í„°ì²˜ëŸ¼ ë³´ì¸ë‹¤. ê·¸ëŸ¼ì—ë„ ë¶ˆêµ¬í•˜ê³  ë„ë©”ì¸ì˜ ì°¨ì´ë³´ë‹¤ëŠ” ì§€ì—­ì ì¸ ì •ë³´ë“¤ì— ì§‘ì¤‘í•˜ì—¬ í•™ìŠµí•˜ê¸° ë•Œë¬¸ì— ëª¨ë¸ì˜ ì¼ë°˜í™” ëŠ¥ë ¥ì´ í–¥ìƒë˜ê³ , ë¬´ì—‡ë³´ë‹¤ ê°’ì‹¸ê²Œ ë°ì´í„°ë¥¼ ìƒì„±í•  ìˆ˜ ìˆë‹¤ëŠ” ì¥ì ì´ ìˆë‹¤. 

Sim-to-realì„ ìœ„í•œ Domain Randomizationë…¼ë¬¸ë“¤[15][16][17]ì— ì˜ê±°í•˜ë©´ ëœë¤í•˜ê²Œ ìƒì„±ëœ synthetic ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ë©´ ëª¨ë¸ì´ ë”ìš± robust í•˜ê²Œ í•™ìŠµë˜ê¸° ë•Œë¬¸ì— ì‹¤ì œ ì„¸ê³„ì— ì ìš©í•  ë•Œ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì¸ë‹¤ëŠ” ê²ƒì„ ì‹¤í—˜ì ìœ¼ë¡œ ì•Œ ìˆ˜ ìˆë‹¤.

## 3.5. Self-Training

![](/assets/images/21-10-18-paper-consistency.png)

DAë¥¼ í†µí•´ í•™ìŠµëœ segmentation ëª¨ë¸ì€ ë°”ë¡œ target ë°ì´í„°ë¥¼ prediction í•˜ëŠ” task ëª¨ë¸ì´ ë˜ì§€ ì•Šê³ , ì‹¤ì œ task ëª¨ë¸ì¸ student ëª¨ë¸ì—ê²Œ knowlege distilation í•´ì£¼ëŠ” teacher ëª¨ë¸ì´ ëœë‹¤. Teacher ëª¨ë¸ì€ unlabeled target ë°ì´í„°ì— pseudo-labeling í•˜ê³  student ëª¨ë¸ì€ ì´ labeled ë°ì´í„°ë“¤ë¡œ supervised learning í•˜ë©° ìì—°ìŠ¤ëŸ½ê²Œ ì§€ì‹ì„ ì „íŒŒë°›ì„ ìˆ˜ ìˆë‹¤. ì´ ë•Œ í•™ìŠµì— ì‚¬ìš©í•˜ëŠ” labeled ë°ì´í„°ëŠ” pseudo-labeling ëœ unlabeled target ë°ì´í„°ì™€ ì´ë¯¸ ê°€ì§€ê³  ìˆëŠ” labeled ë°ì´í„°ë¥¼ ì„ì–´ì„œ í•¨ê»˜ í•™ìŠµí•œë‹¤. Student ëª¨ë¸ì´ ì–´ëŠì •ë„ í•™ìŠµëœ ì´í›„ì—ëŠ” student ëª¨ë¸ì´ teacher ëª¨ë¸ì´ ë˜ì–´ pseudo-labeling ë°ì´í„°ë¥¼ ì—…ë°ì´íŠ¸í•˜ê³ , í•´ë‹¹ ë°ì´í„°ë¡œ ë˜ë‹¤ë¥¸ student ëª¨ë¸ì„ í•™ìŠµí•˜ëŠ” ê³¼ì •ì„ ë°˜ë³µí•˜ë©° ëª¨ë¸ì´ ì ì°¨ì ìœ¼ë¡œ ì‹ ë¢°ë„ ë†’ì€ ë°©í–¥ìœ¼ë¡œ ì¬í•™ìŠµë˜ë„ë¡ iterative learning í•œë‹¤.

Unlabeled target ë°ì´í„°ê°€ ì…ë ¥ìœ¼ë¡œ ë“¤ì–´ì˜¤ëŠ” ê²½ìš° pseudo-labeling ì‘ì—…ë§Œ ìˆ˜í–‰í•˜ì§€ë§Œ, labelì´ ìˆëŠ” source ë°ì´í„° í˜¹ì€ target ë°ì´í„°ê°€ ë“¤ì–´ì˜¤ê²Œ ë˜ë©´ pseudo-labeling ëœ ê²°ê³¼ë¬¼ê³¼ GT ë°ì´í„°ì˜ ì°¨ì´ë¥¼ KL-divergence lossë¡œ ì¶”ê°€í•˜ì—¬ í•™ìŠµí•¨ìœ¼ë¡œì¨ ì¼ì¢…ì˜ consistency regularization íš¨ê³¼ë¥¼ ì¤€ë‹¤.

![](/assets/images/21-10-18-paper-self-training.png)

Teacher ëª¨ë¸ì´ ì˜ˆì¸¡í•œ ê²°ê³¼ë¥¼ ê·¸ëŒ€ë¡œ Pseudo-labelsë¡œ ì‚¬ìš©í•˜ê²Œ ë˜ë©´ teacher ëª¨ë¸ì´ ê°–ê³  ìˆëŠ” ì˜¤ì°¨ ì •ë³´ê°€ ê·¸ëŒ€ë¡œ ì „íŒŒë˜ê¸° ë•Œë¬¸ì— Sharpening predictions í•´ì£¼ëŠ” ì‘ì—…ì´ í•„ìš”í•˜ë‹¤. UDAS í”„ë ˆì„ì›Œí¬ì—ì„œ ì‚¬ìš©í•˜ëŠ” Pseudo-labeling ë°©ë²•ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.

ì¼ë‹¨ Kê°œì˜ augmentation ë°ì´í„°ë¥¼ ìƒì„±í•˜ê³  ê°ê° teacher ëª¨ë¸ì„ í†µí•´ prediction probabilityë¥¼ ì–»ëŠ”ë‹¤. Confidence mapì„ ë¹„êµí–ˆì„ ë•Œ ê°€ì¥ ë†’ì€ confidenceì˜ ì˜ˆì¸¡ê°’ì„ ì‚¬ìš©í•˜ê¸° ìœ„í•´ maximum confidence mapì„ êµ¬ì„±í•˜ê³  ì´ë¥¼ í†µí•´ ë§Œë“¤ì–´ì§„ pseudo-label ì¤‘ì—ì„œ íŠ¹ì • confidence ê°’ ì´ìƒì˜ ì˜ˆì¸¡ ë°ì´í„°ë§Œ í•™ìŠµ ë°ì´í„°ë¡œ ì‚¬ìš©í•˜ì—¬ ì˜¤ë¥˜ ì „íŒŒìœ¨ì„ ë‚®ì¶˜ë‹¤.

<!-- Unlabeled ë°ì´í„°ì— pseudo-labelingì´ ëœ ì´í›„ì—ëŠ” source ë„ë©”ì¸ ë°ì´í„°ëŠ” ì‚¬ìš©í•˜ì§€ ì•ŠëŠ”ë‹¤. Pseudo-labelingëœ ë°ì´í„°ë“¤ì„ ì´ìš©í•´ student ëª¨ë¸ì„ supervised learning ë°©ì‹ìœ¼ë¡œ í•™ìŠµí•˜ê³ , student ëª¨ë¸ì´ target ë„ë©”ì¸ì— ëŒ€í•˜ì—¬ fine-tuning ë˜ì–´ ë” ë‚˜ì€ ì„±ëŠ¥ì— ë„ë‹¬í•˜ê²Œ ë˜ë©´ student ëª¨ë¸ì´ teacher ëª¨ë¸ë¡œ ì¹˜í™˜ë˜ì–´ ê¸°ì¡´ì˜ pseudo-labeling ë°ì´í„°ë¥¼ ì—…ë°ì´íŠ¸ í•´ì¤€ë‹¤. ê·¸ë¦¬ê³  ì¬ìƒì„±ëœ pseudo-labeled ë°ì´í„°ë¥¼ ì´ìš©í•˜ì—¬ ë˜ë‹¤ì‹œ ìƒˆë¡œìš´ student ëª¨ë¸ì„ í•™ìŠµí•˜ëŠ” ë°©ì‹ìœ¼ë¡œ, ëª¨ë¸ì´ ì ì°¨ì ìœ¼ë¡œ ì‹ ë¢°ë„ ë†’ì€ ë°©í–¥ìœ¼ë¡œ ì¬í•™ìŠµë˜ë„ë¡ êµ¬ì„±í•˜ì—¬ ë°˜ë³µì ì¸ í•™ìŠµì´ ì´ë£¨ì–´ì§€ë„ë¡ í•œë‹¤. -->

Noisy Student ë…¼ë¬¸ì—ì„œëŠ” noisyí•œ ë°ì´í„°ë¡œ í•™ìŠµëœ ì»¤ë‹¤ë€ student ëª¨ë¸ì´ techer ëª¨ë¸ë³´ë‹¤ robust í•˜ê²Œ í•™ìŠµë˜ì–´ ì„±ëŠ¥ì´ ì¢‹ì•„ì§ì„ ë³´ì˜€ë‹¤. UDAS í”„ë ˆì„ì›Œí¬ëŠ” DA ëª¨ë¸ì„ ì´ìš©í•´ unlabeled target ë°ì´í„°ì™€ ìˆ˜ë§ì€ synthetic ë°ì´í„°ë¡œ student ëª¨ë¸ì„ í•™ìŠµí•œë‹¤. Augmentëœ synthetic ë°ì´í„°ëŠ” noiseì˜ ì¼ì¢…ìœ¼ë¡œ ë³¼ ìˆ˜ ìˆê¸° ë•Œë¬¸ì— ìœ„ì˜ ë…¼ë¬¸ê³¼ ê°™ì€ íš¨ê³¼ë¥¼ ë‚¼ ìˆ˜ ìˆë‹¤.

# 4. ì‹¤í—˜ ë° ê²°ê³¼

## 4.1. ì‹¤í—˜ ì„¤ê³„

êµ¬í˜„ ì½”ë“œëŠ” Ubuntu 20.04 í™˜ê²½ì—ì„œ Python 3.8ê³¼ Pytorch 1.9.0 ë²„ì „ì„ ê¸°ì¤€ìœ¼ë¡œ êµ¬í˜„ë˜ì—ˆê³  ëª¨ë“  ì‹¤í—˜ì€ ë©”ëª¨ë¦¬ 11GBì˜ GeForce GTX 1080 Ti ê·¸ë˜í”½ì¹´ë“œê°€ 2ê°œ ì¥ì°©ëœ PCì—ì„œ ì§„í–‰ë˜ì—ˆë‹¤.

### 4.1.1. ë°ì´í„°ì…‹

![](/assets/images/21-09-24-paper-dataset.png)

Cityscapes[18]ëŠ” ìœ ëŸ½ 50ê°œ ë„ì‹œì˜ ê±°ë¦¬ì—ì„œ ì°¨ëŸ‰ ì‹œì ìœ¼ë¡œ ìº¡ì³í•œ 2048x1024 í”½ì…€ì˜ ì´ë¯¸ì§€ ë°ì´í„°ì…‹ì´ë‹¤. 2,975ê°œì˜ training set, 500ê°œì˜ validation set, 1,595ê°œì˜ test setìœ¼ë¡œ ì´ 5,000ê°œì˜ ì´ë¯¸ì§€ ë°ì´í„°ì™€ pixel-wise semantic labelë¡œ êµ¬ì„±ë˜ì–´ ìˆë‹¤. ì¶”ê°€ë¡œ label ë˜ì–´ ìˆì§€ ì•Šì€ 19,998 ê°œì˜ raw ì´ë¯¸ì§€ ë°ì´í„°ë„ ì œê³µëœë‹¤.

GTA5[19]ëŠ” ë„ì‹œ ìš´ì „ì„ ìœ„í•œ ëŒ€ê·œëª¨ synthetic ë°ì´í„°ì…‹ ì¤‘ í•˜ë‚˜ì´ë‹¤. ì‚¬ì‹¤ì ì¸ ê·¸ë˜í”½ì„ ë³´ì—¬ì£¼ëŠ” GTA V ê²Œì„ì„ í†µí•´ ë Œë”ë§ ë˜ì—ˆê³  ë¯¸êµ­ ìŠ¤íƒ€ì¼ ë„ì‹œì—ì„œì˜ ìë™ì°¨ ê´€ì ì—ì„œ ì´¬ì˜ë˜ì—ˆë‹¤. 24,966ê°œì˜ 1914x1052 í”½ì…€ì˜ ì´ë¯¸ì§€ì™€ pixel-wise semantic labelë¡œ êµ¬ì„±ë˜ì–´ ìˆë‹¤. Labelì€ Cityspcaesì˜ í´ë˜ìŠ¤ì— ë§ê²Œ 19ê°œì˜ í´ë˜ìŠ¤ë¡œ ì¬ë§¤í•‘ í•  ìˆ˜ ìˆë‹¤.

Cityscapesì™€ GTA5ëŠ” ì‹œê°ì ì¸ ì°¨ì´ê°€ ì¡´ì¬í•˜ì§€ë§Œ ë¹„ìŠ·í•œ ì°¨ëŸ‰ì˜ ë†’ì´ì—ì„œ ë„ë¡œ ìƒí™©ì„ ìº¡ì³í•œ ë°ì´í„°ë¼ëŠ” ì ì—ì„œ êµ¬ì¡°ê°€ ìœ ì‚¬í•˜ë‹¤ê³  ë³¼ ìˆ˜ ìˆë‹¤. GTA5 ë°ì´í„°ê°€ Cityscapes ë°ì´í„° ì–‘ì— ë¹„í•´ ì•½ 5ë°° ê°€ëŸ‰ ë§ê¸° ë•Œë¬¸ì— GTA5ì˜ ë°ì´í„°ë¥¼ ìµœëŒ€í•œ í™œìš©í•˜ì—¬ ì´ë¯¸ì§€ì˜ ì „ë°˜ì ì¸ ì˜ë¯¸ì™€ êµ¬ì¡°ë¥¼ ì¶©ë¶„íˆ í•™ìŠµí•˜ê³ , Cityscapes í™˜ê²½ì—ì„œì˜ fine-tuningì„ ìœ„í•˜ì—¬ Cityscapesì˜ unlabeled ë°ì´í„°ë¥¼ í•¨ê»˜ ì‚¬ìš©í•œë‹¤.

|  Dataset   |   Type    | # of Labeled Data | # of Raw Data | # of Classes |
| :--------: | :-------: | :---------------: | :-----------: | :----------: |
| Cityscapes |   Real    |       5,000       |    19,998     |      19      |
|    GTA     | Synthetic |      24,966       |       -       |      19      |

### 4.1.2. í‰ê°€ Metric

í‰ê°€ metricìœ¼ë¡œëŠ” Semantic segmentation í‰ê°€ ì‹œì— ì£¼ë¡œ ì‚¬ìš©ë˜ëŠ” í´ë˜ìŠ¤ ë³„ IoU(Intersection over Union) ì™€ mIoU(Mean Intersection over Union)ë¥¼ ì‚¬ìš©í•œë‹¤. $TP_i$, $FP_I$, $FN_i$ ëŠ” íŠ¹ì • í´ë˜ìŠ¤ $i$ì˜ True Positive, False Positive, False Negative ì´ê³  $N$ì€ í´ë˜ìŠ¤ì˜ ê°¯ìˆ˜ì´ë‹¤.

$$ {IoU}_i = \left( \frac{TP_i}{FP_i + FN_i + TP_i} \right) $$

$$ mIoU = \sum_{i=1}^N \left( \frac{IoU_i}{N} \right ) $$

## 4.2. Implementation Details

### 4.2.1. Data Size

ëŒ€ë¶€ë¶„ì˜ UDA SOTA ëª¨ë¸ì€ source ë„ë©”ì¸ ì´ë¯¸ì§€ ì‚¬ì´ì¦ˆëŠ” 1280 x 720, target ë„ë©”ì¸ ì´ë¯¸ì§€ ì‚¬ì´ì¦ˆëŠ” 1024 x 512ë¡œ ì›ë³¸ ì‚¬ì´ì¦ˆë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ì—¬ í•™ìŠµí•œë‹¤. ì•„ë˜ ì‹¤í—˜ì—ì„œëŠ” GPU ë©”ëª¨ë¦¬ ë¶€ì¡±ìœ¼ë¡œ ëª¨ë¸ì€ ë™ì¼í•˜ê²Œ ì‚¬ìš©í•˜ë‚˜ source ë„ë©”ì¸ê³¼ target ì´ë¯¸ì§€ ì‚¬ì´ì¦ˆëŠ” 640 x 360ìœ¼ë¡œ resizeí•˜ì—¬ ì‹¤í—˜í•œë‹¤. 

![](/assets/images/21-10-18-paper-image-resize.png)

| Model      | Road | Sidewalk | building | wall | fence | pole | Traffic light | Traffic sign | vegetation | terrain | sky  | person | rider | car  | truck | Bus  | train | motorcycle | Bicycle | mIoU  |
| :--------- | :--- | :------- | :------- | :--- | :---- | :--- | :------------ | :----------- | :--------- | :------ | :--- | :----- | :---- | :--- | :---- | :--- | :---- | :--------- | :------ | :---- |
| 1280 x 720 | 89.1 | 23.7     | 82.4     | 19.6 | 20.2  | 33.0 | 42.4          | 39.7         | 85.3       | 33.1    | 76.9 | 60.5   | 33.0  | 85.7 | 36.2  | 43.6 | 5.0   | 22.3       | 30.0    | 42.35 |
| 640 x 360  | 83.9 | 10.2     | 74.3     | 4.4  | 3.0   | 10.3 | 11.8          | 11.4         | 84.2       | 15.8    | 68.7 | 39.0   | 0.1   | 79.1 | 26.1  | 17.3 | 0     | 2.7        | 0       | 28.59 |

í‘œ[]ëŠ” í•™ìŠµ ì´ë¯¸ì§€ ì‚¬ì´ì¦ˆì— ë”°ë¥¸ ì„±ëŠ¥ ì €í•˜ë¥¼ í™•ì¸í•˜ê¸° ìœ„í•´ ì‹¤í—˜ í™˜ê²½ê³¼ ë™ì¼í•˜ê²Œ ì´ë¯¸ì§€ ìƒ˜í”Œ ì‚¬ì´ì¦ˆë¥¼ 640 x 360ìœ¼ë¡œ ì„¤ì •í•œ ë’¤ UDA SOTA ëª¨ë¸ì„ í•™ìŠµì‹œí‚¨ ê²°ê³¼ì´ë‹¤. 640 x 360 ì‚¬ì´ì¦ˆì˜ ì´ë¯¸ì§€ë¥¼ ì‚¬ìš©í–ˆì„ ë•Œ ì› ì´ë¯¸ì§€ ì‚¬ì´ì¦ˆë¥¼ ì‚¬ìš©í–ˆì„ ë•Œì˜ ì„±ëŠ¥ ëŒ€ë¹„ ì•½ 14% mIoUê°€ ê°ì†Œí•œ ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤. ì´ì— ë”°ë¼ ì´í›„ì— ì§„í–‰ë˜ëŠ” ì‹¤í—˜ì€ 640 x 360 ì‚¬ì´ì¦ˆë¡œ ì§„í–‰ë˜ë‚˜, ì› ì‚¬ì´ì¦ˆë¡œ í•™ìŠµí•˜ê²Œ ë˜ë©´ ì„±ëŠ¥ì´ ë”ìš± í–¥ìƒë  ê²ƒìœ¼ë¡œ ì˜ˆìƒí•  ìˆ˜ ìˆë‹¤.

### 4.2.2. Semantic Segmentation

ìš°ì„  ì¼ë°˜ì ì¸ Supervised-learning ë°©ì‹ì˜ Semantic segmentation ëª¨ë¸ê³¼ì˜ ì„±ëŠ¥ ë¹„êµë¥¼ ìœ„í•´ SOTA ëª¨ë¸ì¸ FCNê³¼ DeepLab v3+ ì„±ëŠ¥ ë¹„êµë¥¼ ìˆ˜í–‰í•˜ì˜€ë‹¤. Backboneì€ Resnet-50ìœ¼ë¡œ ì„¤ì •í•˜ì˜€ê³  Cityscapesì˜ Labeled ì´ë¯¸ì§€ë“¤ë¡œ Supervised-learning ë°©ì‹ìœ¼ë¡œ í•™ìŠµì‹œì¼°ë‹¤.

![](/assets/images/21-09-24-paper-sl-learning-result.png)

|    model    | road | sidewalk | building | wall | fence | pole | traffic light | traffic sign | vegetation | terrain | sky  | person | rider | car  | truck | bus  | train | motorcycle | bicycle | Pixel Accuracy | mIoU  |
| :---------: | :--: | :------: | :------: | :--: | :---: | :--: | :-----------: | :----------: | :--------: | :-----: | :--: | :----: | :---: | :--: | :---: | :--: | :---: | :--------: | :-----: | :------------: | :---: |
|     FCN     | 0.97 |   0.81   |   0.9    | 0.26 | 0.45  | 0.54 |     0.67      |     0.75     |    0.91    |  0.53   | 0.93 |  0.78  | 0.57  | 0.93 | 0.48  | 0.7  | 0.36  |    0.56    |  0.75   |     94.693     | 68.03 |
| DeepLab v3+ | 0.98 |   0.84   |   0.92   | 0.56 | 0.59  | 0.64 |      0.7      |     0.78     |    0.92    |  0.64   | 0.95 |  0.81  | 0.63  | 0.94 | 0.69  | 0.76 | 0.43  |    0.63    |  0.76   |     95.826     | 74.97 |

ì‚¬ìš©í•˜ëŠ” ëª¨ë¸ê³¼ Backboneì— ë”°ë¼ mIoU ê°’ ì°¨ì´ê°€ ìˆìœ¼ë‚˜, Supervised learningìœ¼ë¡œ ëª¨ë¸ì„ í•™ìŠµì‹œì¼°ì„ ë•Œ mIoUê°€ ì•½ 70% ì •ë„ ë‚˜ì˜¤ëŠ” ê²ƒìœ¼ë¡œ í™•ì¸í–ˆë‹¤. 

ë§ì€ domain adaptation ë…¼ë¬¸ë“¤ì´ Resnet101ì„ backboneìœ¼ë¡œ í•œ DeepLab v2ë¥¼ Semantic segmentation ëª¨ë¸ë¡œ ì‚¬ìš©í•˜ê³  ìˆê¸° ë•Œë¬¸ì—, ë™ë“±í•œ ë¹„êµë¥¼ ìœ„í•´ ì´í›„ ì‹¤í—˜ì—ì„œëŠ” Atrous Spatial Pyramid Pooling(ASPP) ëª¨ë“ˆì„ í¬í•¨í•œ DeepLab v2 ëª¨ë¸ì„ task ëª¨ë¸ë¡œ ì„¤ì •í•˜ì—¬ ì§„í–‰í•œë‹¤. Cityscapes ë°ì´í„°ì…‹ì„ ì´ìš©í•´ í•™ìŠµí•œ Resnet101 + DeepLab v2 ëª¨ë¸ì˜ ê¸°ë³¸ ì„±ëŠ¥ì€ ì•„ë˜ì™€ ê°™ë‹¤.

| Model                  | road | walk | build. | wall | fence | pole | light | sign | vege. | terr. | sky  | persn | rider | car  | truck | bus | train | mcyc | Bike | mIoU |
| :--------------------- | :--- | :--- | :----- | :--- | :---- | :--- | :---- | :--- | :---- | :---- | :--- | :---- | :---- | :--- | :---- | :-- | :---- | :--- | :--- | :--- |
| DeepLab v2 (Resnet101) | 96.7 | 76.2 | 88.6   | 47.2 | 47.6  | 45.1 | 57.9  | 67.6 | 89.1  | 58.2  | 90.4 | 70    | 54.8  | 92.2 | 71.8  | 78  | 56.6  | 56.7 | 67.8 | 69.1 |

### 4.2.3. Image Translation

Source ì´ë¯¸ì§€ì—ì„œ Target ì´ë¯¸ì§€ ìŠ¤íƒ€ì¼ë¡œ image translationëœ ë°ì´í„°ê°€ ë„ë©”ì¸ gapì„ ì¤„ì´ëŠ”ë° ì–¼ë§ˆë‚˜ ê¸°ì—¬í•˜ëŠ”ì§€ í™•ì¸í•˜ê¸° ìœ„í•œ ì‹¤í—˜ìœ¼ë¡œ, GTA ë°ì´í„°ì™€ Cityscapes ìŠ¤íƒ€ì¼ë¡œ ì¬ìƒì„±ëœ GTA->Cityscapes ë°ì´í„°ë¥¼ ë™ì¼í•œ ëª¨ë¸ë¡œ ê°ê° ì˜ˆì¸¡í–ˆì„ ë•Œ mIoUë¥¼ ë¹„êµí•œë‹¤.

![](/assets/images/21-09-24-paper-style-transfer-prediction.png)

Labeled ë°ì´í„° ì—†ì´ Cityscapes ì´ë¯¸ì§€ì™€ GTA ì´ë¯¸ì§€ë§Œì„ ì´ìš©í•˜ì—¬ CycleGAN ëª¨ë¸ì„ í•™ìŠµí•˜ê³ , CycleGAN ëª¨ë¸ì„ í†µí•´ GTAì´ë¯¸ì§€ë¥¼ Cityscapes ìŠ¤íƒ€ì¼ì˜ ì´ë¯¸ì§€ë¡œ ì¬ìƒì„±í•œë‹¤. Image translationëœ GTA->Cityscapes ìƒ˜í”Œê³¼ ê·¸ì— ëŒ€í•œ segmentation ì˜ˆì¸¡ ê²°ê³¼ëŠ” ê·¸ë¦¼[]ê³¼ ê°™ë‹¤. ì´ ë•Œ segmentation ëª¨ë¸ì€ 4.2.1ì¥ì—ì„œ Cityscapes ë°ì´í„°ë¡œë§Œ í•™ìŠµì‹œí‚¨ DeepLab v3+ ëª¨ë¸ì„ ì‚¬ìš©í–ˆë‹¤.

| Data             | road | walk | building | wall | fence | pole | light | sign | vege. | terrain | sky  | person | rider | car  | truck | bus  | train | mCycle | bicycle | mIoU |
| :--------------- | :--- | :--- | :------- | :--- | :---- | :--- | :---- | :--- | :---- | :------ | :--- | :----- | :---- | :--- | :---- | :--- | :---- | :----- | :------ | :--- |
| GTA              | 70.2 | 19.3 | 60.4     | 23.3 | 13.4  | 25.3 | 31.8  | 17   | 53.5  | 24.3    | 85.6 | 41.2   | 1.7   | 55.4 | 19.1  | 16.8 | 2.4   | 6.4    | 5.4     | 30.1 |
| GTA â†’ Cityscapes | 80.7 | 26.8 | 6.76     | 29.3 | 13.6  | 30.8 | 26.2  | 21.1 | 61.2  | 33.5    | 76.2 | 47.6   | 27    | 63   | 37.1  | 15.8 | 4.9   | 18.2   | 7.4     | 36.2 |
| Cityscapes       | 98   | 84.5 | 92.3     | 56   | 59.1  | 64.6 | 70.2  | 78.9 | 92.4  | 64      | 95   | 81.6   | 63.2  | 94.7 | 69    | 76.9 | 43.3  | 63.4   | 76.5    | 74.9 |

GTAì— ëŒ€í•œ mIoUëŠ” 30.1% ë¡œ Cityscapesì— ì ìš©í–ˆì„ ë•Œì˜ mIoUì¸ 74.9%ì— ë¹„í•´ í˜„ì €íˆ ë‚®ì€ ì„±ëŠ¥ì„ ë³´ì´ê³  ìˆë‹¤. í˜„ì‹¤ê³¼ ë¹„ìŠ·í•œ ì‹œë®¬ë ˆì´ì…˜ í™˜ê²½ì„ êµ¬ì¶•í•˜ê³  ì‚¬ì‹¤ì ì¸ ë Œë”ë§ì„ í•˜ì˜€ìŒì—ë„ ë¶ˆêµ¬í•˜ê³  í° ë„ë©”ì¸ gapì´ ì¡´ì¬í•˜ëŠ” ê²ƒì´ë‹¤.

Image translationí•œ GTA->Cityscapesì˜ mIoUëŠ” 36.2%ë¡œ, GTA ë°ì´í„°ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í–ˆì„ ë•Œ ë³´ë‹¤ mIoUê°€ +6% ì¦ê°€í•˜ì˜€ë‹¤. ì´ë¥¼ í†µí•´ appearance gapì„ ì¤„ì´ëŠ” ë°©ë²•ì„ í†µí•´ target ë„ë©”ì¸ê³¼ì˜ gapì´ ì¼ë¶€ ì¤„ì–´ë“¤ì—ˆë‹¤ê³  íŒë‹¨í•  ìˆ˜ ìˆë‹¤. í•˜ì§€ë§Œ ì—¬ì „íˆ target ë„ë©”ì¸ì— ì ìš©í–ˆì„ ë•Œì˜ ì„±ëŠ¥ë³´ë‹¤ëŠ” ë‚®ê¸° ë•Œë¬¸ì— ì‹œê°ì ìœ¼ë¡œ ë´¤ì„ ë•Œ ìœ ì‚¬í•´ë³´ì´ë”ë¼ë„ ì—¬ì „íˆ ë³´ì´ì§€ ì•ŠëŠ” ë„ë©”ì¸ gapì´ ì”ì¡´í•˜ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.

ì´ë ‡ê²Œ ë³€í™˜ëœ GTA->Cityscapes ë°ì´í„°ëŠ” GTA ë°ì´í„° ëŒ€ì‹  source ë„ë©”ì¸ ë°ì´í„°ë¡œ ì‚¬ìš©í•˜ì—¬ input-level adaptationì„ ì „ì²´ í•™ìŠµ ëª¨ë¸ì— ë…¹ì—¬ë‚¸ë‹¤.

### 4.2.4. Domain CutMix

ì‹œê°ì ì¸ ë¶€ë¶„ì—ì„œ í•œë‹¨ê³„ ë„ë©”ì¸ gapì„ ì¤„ì˜€ì§€ë§Œ ì™¸ê´€ìƒìœ¼ë¡œ ë³´ì´ëŠ” gap ì´ì™¸ì—ë„ ì‹¤ì œë¡œ ëª¨ë¸ì´ ì´ë¯¸ì§€ë¥¼ ì´í•´í•˜ê¸° ìœ„í•´ì„œëŠ” ë³´ì´ì§€ ì•ŠëŠ” êµ¬ì¡°ì ì¸ ë¶€ë¶„ ë˜í•œ í•™ìŠµí•´ì•¼ í•  í•„ìš”ê°€ ìˆë‹¤. ì´ë¥¼ ìœ„í•´ ë„ë©”ì¸ì„ í˜¼í•©í•œ cutmix ë°ì´í„°ë¥¼ ì¶”ê°€í•¨ìœ¼ë¡œì¨ êµ¬ì¡°ì ìœ¼ë¡œ ì´ë¯¸ì§€ë¥¼ í•™ìŠµí•  ìˆ˜ ìˆë„ë¡ êµ¬ì„±í•˜ì˜€ê³ , ì´ëŠ” domain confusionì„ ì•¼ê¸°í•˜ì—¬ ë„ë©”ì¸ ì§€ì‹ê³¼ ê´€ê³„ì—†ì´ taskì— ì§‘ì¤‘í•˜ì—¬ í•™ìŠµí•  ìˆ˜ ìˆë‹¤.

![](/assets/images/21-09-24-paper-combined-augmentation.png)

ê·¸ë¦¼7ì€ Cityscapes ì´ë¯¸ì§€ì™€ GTA->Cityscapes ì´ë¯¸ì§€ë¥¼ ê²°í•©í•˜ì—¬ ë§Œë“  Domain CutMix ë°ì´í„°ì´ë‹¤. ì´ë¯¸ì§€ê°€ ì–´ìƒ‰í•˜ê²Œ ê²°í•©ì´ ë˜ì–´ ìˆìŒì—ë„ ë¶ˆêµ¬í•˜ê³  ëª¨ë¸ì´ ì¼ë°˜í™” ë˜ì–´ í•™ìŠµë˜ì—ˆê¸° ë•Œë¬¸ì— í° ì˜¤ì°¨ ì—†ì´ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤.

### 4.2.5. Pseudo-labeling

ë§Œë“¤ì–´ì§„ DA ëª¨ë¸ì€ Pseudo-labeling í•˜ëŠ” Teacher ëª¨ë¸ì´ ëœë‹¤. ì‹ ë¢°ë„ ë†’ì€ Pseudo-labelingì„ ìœ„í•´ í•˜ë‚˜ì˜ ì´ë¯¸ì§€ì— ëŒ€í•´ Kê°œì˜ augmentation ìƒ˜í”Œ ë°ì´í„°ë¥¼ ìƒì„±í•˜ê³  ì´ë¥¼ ê²°í•©í•˜ì—¬ ì‚¬ìš©í•˜ê²Œ ëœë‹¤. ì‹¤í—˜ì—ì„œëŠ” Kë¥¼ 3ìœ¼ë¡œ ì„¤ì •í•˜ì˜€ê³  Augmentationì€ AutoAugment[20]ë¥¼ ì‚¬ìš©í–ˆë‹¤.  Strong augmentation, weakly augmentationì— ë”°ë¼ prediction í•œ ê²°ê³¼ì˜ confidence mapì€ ë‹¤ë¥´ê²Œ ë‚˜ì˜¤ëŠ”ë°, ì´ ì¤‘ì—ì„œ ê°€ì¥ confidenceê°€ ë†’ì€ ê°’ë§Œì„ ì¶”ì¶œí•˜ì—¬ maximum confidence mapì„ ë§Œë“¤ë©´ nosieê°€ ê°ì†Œë˜ì–´ ì •ì œëœ confidence mapì„ ì–»ì„ ìˆ˜ ìˆë‹¤. ë”ìš± ì‹ ë¢°ë„ ë†’ì€ pseudo-labelingì„ ìœ„í•´ 0.6 ì´ìƒì˜ confidenceë§Œ masking í•˜ì—¬ ì‚¬ìš©í•œë‹¤.

![](/assets/images/21-09-24-paper-pseudo-labeling-result.png)

ëœë¤í•˜ê²Œ augmentation ìƒ˜í”Œë“¤ì„ ìƒì„±í•˜ì—¬ maximum confidence map, confidence-based masking ì‘ì—…ì„ ìˆ˜í–‰í•˜ì˜€ì„ ë•Œ ì‹ ë¢°ë„ ë†’ì€ pseudo-labeling ë°ì´í„°ê°€ ìƒì„±ë˜ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.

![](/assets/images/21-09-24-paper-pseudo-labeling-data.png)

ìƒì„±ëœ pseudo-labeling ë°ì´í„°ë¥¼ í†µí•´ task ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ê³ , ëª¨ë¸ì´ ì–´ëŠì •ë„ í•™ìŠµëì„ ë•Œì—ëŠ” í•´ë‹¹ ëª¨ë¸ì„ ë‹¤ì‹œ teacher ëª¨ë¸ë¡œ ì‚¬ìš©í•˜ì—¬ ìƒˆë¡­ê²Œ pseudo-labeling í•˜ê³  ì ì§„ì ìœ¼ë¡œ í•™ìŠµë˜ì–´ê°€ëŠ” self-training ë£¨í”„ë¥¼ ì´ë£¬ë‹¤.

## 4.3. Quantitative Comparison

### 4.3.1. Performance

DAì™€ Style Transfer, SSL ê¸°ë²•ë“¤ì´ ì°¨ë¡€ëŒ€ë¡œ ì ìš©ë¨ì— ë”°ë¼ Target ë„ë©”ì¸ì¸ Cityscapesì— ëŒ€í•œ ì˜ˆì¸¡ ê²°ê³¼ê°€ í–¥ìƒëœë‹¤.

![](/assets/images/21-10-18-paper-paper-performance.png)

DA ì—†ì´ í•™ìŠµí•œ ëª¨ë¸ì€ Domain shiftì— ì˜í•œ ë§ì€ ì˜¤ì°¨ê°€ ë°œìƒí•œë‹¤. Image Translation ì¦‰, Style Transfer(ST)ë¥¼ ì ìš©í•œ ë°ì´í„°ì—ëŠ” ì–´ëŠ ì •ë„ semantic êµ¬ì¡°ë¥¼ íŒŒì•…í•œ ê²ƒì²˜ëŸ¼ ë³´ì´ì§€ë§Œ ì—¬ì „íˆ ë§ì€ ë…¸ì´ì¦ˆê°€ í¬í•¨ë˜ì–´ ìˆë‹¤. Style Transfer ë°ì´í„°ë¥¼ source ë°ì´í„°ë¡œ í•˜ì—¬ UDAë¥¼ ì ìš©í•œ ëª¨ë¸ì€ ì¼ë°˜ì ì¸ UDA SOTA ìˆ˜ì¤€ì˜ ì„±ëŠ¥ì„ ë³´ì´ê³  ìˆë‹¤. ì—¬ê¸°ì— SSL ê¸°ë²•ìœ¼ë¡œ Domain CutMix, pseudo-labeling, self-trainingë¥¼ ì¶”ê°€í–ˆì„ ë•Œ ê¹”ë”í•˜ê²Œ prediction ê²°ê³¼ê°€ ë‚˜ì˜¤ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤.

| Model         | road | walk | building | wall | fence | pole | light | sign | vege. | terrain | sky  | person | rider | car  | truck | bus  | train | mCycle | bicycle | mIoU |
| :------------ | :--- | :--- | :------- | :--- | :---- | :--- | :---- | :--- | :---- | :------ | :--- | :----- | :---- | :--- | :---- | :--- | :---- | :----- | :------ | :--- |
| No Adaptation | 83.9 | 10.2 | 74.3     | 4.4  | 3.0   | 10.3 | 11.8  | 11.4 | 84.2  | 15.8    | 68.7 | 39.0   | 0.1   | 79.1 | 26.1  | 17.3 | 0.0   | 2.7    | 0.0     | 28.5 |
| UDA           | 83.9 | 10.4 | 77.2     | 22.5 | 2.8   | 29.4 | 4.1   | 2.8  | 84.2  | 39.9    | 78.8 | 31.9   | 0     | 83   | 30.9  | 4.2  | 0     | 0      | 0.0     | 30.8 |
| ST            | 75.7 | 16.7 | 77.2     | 12.5 | 21.0  | 25.4 | 30.0  | 20.1 | 81.3  | 24.6    | 70.3 | 53.7   | 26.4  | 49.9 | 17.1  | 25.8 | 6.4   | 25.2   | 36.0    | 36.6 |
| ST + UDA      | 89.4 | 26.2 | 82.7     | 25.8 | 22.9  | 36.1 | 40.4  | 38.8 | 84.1  | 37.5    | 83.5 | 59.0   | 26.7  | 83.6 | 28.5  | 36.6 | 0.1   | 14.6   | 26.7    | 44.4 |
| ST + DA + SSL | 95.4 | 69.1 | 87.7     | 43.1 | 41.3  | 42.5 | 46.7  | 59.9 | 88.4  | 50.5    | 90.3 | 68.9   | 49.7  | 90.2 | 61.9  | 65.8 | 46.9  | 44.2   | 63.5    | 63.5 |

![](/assets/images/21-10-18-paper-udas-performance.png)

ê° ë‹¨ê³„ì— ëŒ€í•œ í´ë˜ìŠ¤ ë³„ IoUì™€ mIoUëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤. DA ê¸°ë²•ì´ ì „í˜€ ë“¤ì–´ê°€ì§€ ì•Šì•˜ì„ ë•Œ 28.5% mIoU, Style Transferë¥¼ ë„ì…í–ˆì„ ë•ŒëŠ” 36.6% mIoU, STì™€ UDAë¥¼ í•¨ê»˜ ì‚¬ìš©í–ˆì„ ë•Œ 44.4% mIoU, ê·¸ë¦¬ê³  ì—¬ê¸°ì— SSL ë°©ë²•ë¡ ì„ ì¶”ê°€í•˜ì—¬ 63.5%ê¹Œì§€ ì„±ëŠ¥ì„ í–¥ìƒì‹œì¼°ë‹¤.

| Model           | road | walk | building | wall | fence | pole | light | sign | vege. | terrain | sky  | person | rider | car  | truck | bus  | train | mCycle | bicycle | mIoU |
| :-------------- | :--- | :--- | :------- | :--- | :---- | :--- | :---- | :--- | :---- | :------ | :--- | :----- | :---- | :--- | :---- | :--- | :---- | :----- | :------ | :--- |
| Cycada          | 79.1 | 33.1 | 77.9     | 23.4 | 17.3  | 32.1 | 33.3  | 31.8 | 81.5  | 26.7    | 69.0 | 62.8   | 14.7  | 74.5 | 20.9  | 25.6 | 6.9   | 18.8   | 20.4    | 39.5 |
| AdaptSegNet     | 86.5 | 36.0 | 79.9     | 23.4 | 23.3  | 23.9 | 35.2  | 14.8 | 83.4  | 33.3    | 75.6 | 58.5   | 27.6  | 73.7 | 32.5  | 35.4 | 3.9   | 30.1   | 28.1    | 42.4 |
| AdvEnt          | 89.4 | 33.1 | 81.0     | 26.6 | 26.8  | 27.2 | 33.5  | 24.7 | 83.9  | 36.7    | 78.8 | 58.7   | 30.5  | 84.8 | 38.5  | 44.5 | 1.7   | 31.6   | 32.4    | 45.5 |
| Seg-Uncertainty | 90.5 | 35.0 | 84.6     | 34.3 | 24.0  | 36.8 | 44.1  | 42.7 | 84.5  | 33.6    | 82.5 | 63.1   | 34.4  | 85.8 | 32.9  | 38.2 | 2.0   | 27.1   | 41.8    | 48.3 |
| Ours            | 95.4 | 69.1 | 87.7     | 43.1 | 41.3  | 42.5 | 46.7  | 59.9 | 88.4  | 50.5    | 90.3 | 68.9   | 49.7  | 90.2 | 61.9  | 65.8 | 46.9  | 44.2   | 63.5    | 63.5 |

![](/assets/images/21-10-18-paper-uda-sota.png)

í‘œ []ëŠ” UDA SOTAì™€ ë¹„êµí•œ ê²°ê³¼ì´ë‹¤. UDA SOTAì˜ í•™ìŠµ ë°ì´í„° ì‚¬ì´ì¦ˆê°€ ì› ì‚¬ì´ì¦ˆì¸ ê²ƒì„ ê°ì•ˆí•˜ë”ë¼ë„ ì œì•ˆí•œ ëª¨ë¸ì´ ì›”ë“±í•˜ê²Œ ì¢‹ì€ ì„±ëŠ¥ì„ ë‚´ê³ ìˆë‹¤. Labeled ë°ì´í„°ë¥¼ ì¼ë¶€ ì‚¬ìš©í•˜ê¸´ í•˜ì§€ë§Œ Target ë„ë©”ì¸ì— ëŒ€í•œ ì§€ì‹ ì—†ì´ distribution shiftë§Œì„ ì¤„ì´ê¸° ìœ„í•´ ë…¸ë ¥í•˜ëŠ” ë‹¤ë¥¸ ì—°êµ¬ë“¤ ëŒ€ë¹„ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì´ê³  ìˆê¸° ë•Œë¬¸ì— ë” ì‹¤ìš©ì„±ì´ ê°•í™”ëë‹¤ê³  ë³¼ ìˆ˜ ìˆë‹¤. 

### 4.3.2. Data Dependency

Supervised Learning(SL) ë°©ë²•ë¡ ì˜ ê²½ìš° í•™ìŠµì„ ìœ„í•´ì„œëŠ” ë§ì€ ì–‘ì˜ Labeled ë°ì´í„°ê°€ í•„ìš”í•˜ë‹¤. ì ì€ labeled ë°ì´í„°ë¡œ í•™ìŠµì‹œí‚¤ê²Œ ë˜ë©´ í•™ìŠµ ë°ì´í„°ì— overfitting ë˜ëŠ” ê²½í–¥ì´ ìˆë‹¤. Data dependency ì‹¤í—˜ì€ ë°ì´í„°ê°€ ì ì€ ìƒí™©ì—ì„œ ì œì•ˆ ëª¨ë¸ì´ SL ë°©ë²•ì— ë¹„í•´ ì–¼ë§ˆë‚˜ ì•ˆì •ì ì¸ ì„±ëŠ¥ì„ ë³´ì´ëŠ”ì§€ ë¹„êµí•˜ê¸° ìœ„í•œ ì‹¤í—˜ì´ë‹¤.

![](/assets/images/21-10-18-paper-data-dependency.png)

ê·¸ë¦¼[]ëŠ” Labeled target ì´ë¯¸ì§€ ê°¯ìˆ˜ì— ë”°ë¥¸ mIoUë¥¼ ê·¸ë˜í”„ë¡œ ì‹œê°í™”í•œ ê²ƒì´ë‹¤. Cityscapes ë°ì´í„° 2975ì¥ì„ ëª¨ë‘ ì‚¬ìš©í•˜ì—¬ í•™ìŠµí–ˆì„ ë•Œì™€ labeled target í•™ìŠµ ë°ì´í„° ì–‘ì„ 1000, 500, 100, 0ìœ¼ë¡œ ê°ì†Œì‹œí‚¤ë©° mIoUë¥¼ ë¹„êµí•˜ì˜€ë‹¤. SLì€ labeled ë°ì´í„°ì–‘ì´ ì ì–´ì§ì— ë”°ë¼ ê¸‰ê²©í•˜ê²Œ ì„±ëŠ¥ì´ í•˜ë½í•˜ì§€ë§Œ UDASëŠ” labeled ë°ì´í„°ì–‘ì´ ì ì–´ì§€ë”ë¼ë„ ì•ˆì •ì ì¸ ì„±ëŠ¥ì„ ë³´ì¸ë‹¤. ì´ë¥¼ í†µí•´ ì œì•ˆí•œ UDAS í”„ë ˆì„ì›Œí¬ë¥¼ ì‚¬ìš©í•˜ë©´ ë„ë©”ì¸ ë°ì´í„°ì— ëŒ€í•œ ì˜ì¡´ì„±ì´ ë‚®ì•„ì¡Œë‹¤ê³  í•  ìˆ˜ ìˆë‹¤.

| Methods    | 2975 | 1000         | 500           | 100           | 0             |
| :--------- | :--- | :----------- | :------------ | :------------ | :------------ |
| Supervised | 69.1 | 61.2 (-7.9%) | 55.4 (-13.7%) | 41.6 (-27.5%) | -             |
| UDAS       | 63.5 | 60.6 (-2.9%) | 61.3 (-2.2%)  | 55.6 (-7.9%)  | 43.9 (-12.5%) |

ìˆ˜ì¹˜ì ìœ¼ë¡œ ë´¤ì„ ë•Œ Labeled target ì´ë¯¸ì§€ë¥¼ ëª¨ë‘ ì‚¬ìš©í•œ ê²½ìš° UDAS í”„ë ˆì„ì›Œí¬ëŠ” SLì— ë¹„í•´ -5.6% ì˜ ì„±ëŠ¥ì„ ë³´ì´ê³  ìˆê³ , 1000ì¥ì¸ ê²½ìš° -0.6%, 500ì¥ì¸ ê²½ìš° +5.9%, ê·¸ë¦¬ê³  100ì¥ì¸ ê²½ìš° +14.0% mIoUë¡œ, labeled target ë°ì´í„°ê°€ ì ì–´ì§ˆ ìˆ˜ë¡ UDASê°€ SL ë³´ë‹¤ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì´ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.

UDAS í”„ë ˆì„ì›Œí¬ëŠ” íƒ€ê²Ÿ ë„ë©”ì¸ì˜ í•™ìŠµ ë°ì´í„° ì˜ì¡´ì„±ì´ ì¤„ì–´ë“¤ì–´ synthetic ë°ì´í„°ë¥¼ ì´ìš©í•œ ë‹¤ì–‘í•œ ìƒí™©ë“¤ì„ í•™ìŠµí•  ìˆ˜ ìˆê²Œ ë˜ì—ˆê³ , íŠ¹íˆë‚˜ í•™ìŠµ ë°ì´í„°ê°€ ì ê±°ë‚˜ ì—†ëŠ” ìƒí™©ì—ì„œëŠ” SL ë°©ë²•ë³´ë‹¤ ë” ì•ˆì •ì ì´ê³  ë†’ì€ ì„±ëŠ¥ì„ ë³´ì¸ë‹¤.

# 5. ê²°ë¡ 

AI í•™ìŠµì— ìˆì–´ì„œ í•­ìƒ í° ì¥ì• ë¬¼ì´ ë˜ëŠ” ë°ì´í„° ë¶€ì¡± ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ì—°êµ¬ë¥¼ ì§„í–‰í•˜ì˜€ë‹¤. ì»´í“¨í„° ê·¸ë˜í”½ì´ ë°œì „í•¨ì— ë”°ë¼ ì ì°¨ì ìœ¼ë¡œ í˜„ì‹¤ì„¸ê³„ì™€ ê°€ìƒì„¸ê³„ì˜ ì‹œê°ì  ê²½ê³„ê°€ ëª¨í˜¸í•´ì§€ê³  ìˆê¸° ë•Œë¬¸ì—, ë¬´í•œí•œ ë°ì´í„°ë¥¼ ì°½ì¶œí•´ë‚¼ ìˆ˜ ìˆëŠ” synthetic ë°ì´í„°ì— ëŒ€í•œ í™œìš©ë„ê°€ ë†’ì•„ì§ˆ ê²ƒì´ë‹¤. ë˜í•œ AI ëª¨ë¸ì´ ì»¤ì§€ë©´ì„œ ì ì  ë” ë§ì€ ë°ì´í„°ê°€ ìš”êµ¬ë˜ëŠ”ë° í•­ìƒ ë°ì´í„°ì— ì˜ì¡´ì ì¸ AI ëª¨ë¸ì—ì„œ ë²—ì–´ë‚˜ ìŠ¤ìŠ¤ë¡œ ê·œì¹™ì„ ì°¾ì•„ë‚´ëŠ” ëª¨ë¸ì„ ë§Œë“¤ê³ ì í–ˆë‹¤. 

ë³¸ ë…¼ë¬¸ì—ì„œëŠ” Synthetic ë°ì´í„°ë¥¼ ì´ìš©í•˜ì—¬ DAì™€ SSLê¸°ë²•ì„ ê²°í•©í•œ UDAS í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•˜ì˜€ê³  íƒ€ê²Ÿ í•™ìŠµ ë°ì´í„°ê°€ ë¶€ì¡±í•œ ìƒí™©ì—ì„œë„ ì•ˆì •ì ê³  ë†’ì€ ì„±ëŠ¥ì„ ë³´ì´ëŠ” ê²ƒì„ í™•ì¸í–ˆë‹¤.

DA ë°©ë²•ë¡ ê³¼ synthetic ë°ì´í„°ì˜ ì§ˆì´ í–¥ìƒí•¨ì— ë”°ë¼ ì œì•ˆí•œ UDAS êµ¬ì¡°ì˜ ì„±ëŠ¥ì€ ê·¸ì™€ ë¹„ë¡€í•˜ì—¬ í–¥ìƒë  ìˆ˜ ìˆì„ ê²ƒìœ¼ë¡œ ì˜ˆìƒí•œë‹¤.

# 6. ì°¸ê³  ë¬¸í—Œ

[1]	Long, Jonathan, Evan Shelhamer, and Trevor Darrell. "Fully convolutional networks for semantic segmentation." Proceedings of the IEEE conference on computer vision and pattern recognition. 2015.
[2]	Zhao, Hengshuang, et al. "Pyramid scene parsing network." Proceedings of the IEEE conference on computer vision and pattern recognition. 2017.
[3]	Yu, Fisher, Vladlen Koltun, and Thomas Funkhouser. "Dilated residual networks." Proceedings of the IEEE conference on computer vision and pattern recognition. 2017.
[4]	Chen, Liang-Chieh, et al. "Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs." IEEE transactions on pattern analysis and machine intelligence 40.4 (2017): 834-848.
[5]	Tzeng, Eric, et al. "Adversarial discriminative domain adaptation." Proceedings of the IEEE conference on computer vision and pattern recognition. 2017.
[6]	Hoffman, Judy, et al. "Cycada: Cycle-consistent adversarial domain adaptation." International conference on machine learning. PMLR, 2018.
[7]	Tsai, Yi-Hsuan, et al. "Learning to adapt structured output space for semantic segmentation." Proceedings of the IEEE conference on computer vision and pattern recognition. 2018.
[8]	Xie, Qizhe, et al. "Self-training with noisy student improves imagenet classification." Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020.
[9]	Pham, Hieu, et al. "Meta pseudo labels." Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021.
[10] Miyato, Takeru, et al. "Virtual adversarial training: a regularization method for supervised and semi-supervised learning." IEEE transactions on pattern analysis and machine intelligence 41.8 (2018): 1979-1993.
[11] Xie, Qizhe, et al. "Unsupervised data augmentation for consistency training." arXiv preprint arXiv:1904.12848 (2019).
[12] Berthelot, David, et al. "Mixmatch: A holistic approach to semi-supervised learning." arXiv preprint arXiv:1905.02249 (2019).
[13] Sohn, Kihyuk, et al. "Fixmatch: Simplifying semi-supervised learning with consistency and confidence." arXiv preprint arXiv:2001.07685 (2020).
[14] Zheng, Zhedong, and Yi Yang. "Unsupervised scene adaptation with memory regularization in vivo." arXiv preprint arXiv:1912.11164 (2019).
[15] Yun, Sangdoo, et al. "Cutmix: Regularization strategy to train strong classifiers with localizable features." Proceedings of the IEEE/CVF International Conference on Computer Vision. 2019.
[16] Tremblay, Jonathan, et al. "Training deep networks with synthetic data: Bridging the reality gap by domain randomization." Proceedings of the IEEE conference on computer vision and pattern recognition workshops. 2018.
[17] Kar, Amlan, et al. "Meta-sim: Learning to generate synthetic datasets." Proceedings of the IEEE/CVF International Conference on Computer Vision. 2019.
[18] Andrychowicz, OpenAI: Marcin, et al. "Learning dexterous in-hand manipulation." The International Journal of Robotics Research 39.1 (2020): 3-20.
[19] Cordts, Marius, et al. "The cityscapes dataset for semantic urban scene understanding." Proceedings of the IEEE conference on computer vision and pattern recognition. 2016.
[20] Richter, Stephan R., et al. "Playing for data: Ground truth from computer games." European conference on computer vision. Springer, Cham, 2016.
[21] Cubuk, Ekin D., et al. "Autoaugment: Learning augmentation strategies from data." Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019.
[22] Hoffman, Judy, et al. "Cycada: Cycle-consistent adversarial domain adaptation." International conference on machine learning. PMLR, 2018.
[23] Tsai, Yi-Hsuan, et al. "Learning to adapt structured output space for semantic segmentation." Proceedings of the IEEE conference on computer vision and pattern recognition. 2018.
[24] Vu, Tuan-Hung, et al. "Advent: Adversarial entropy minimization for domain adaptation in semantic segmentation." Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019.

---

í•˜ì§€ë§Œ ëŒ€ëŸ‰ì˜ labeled ë°ì´í„°ì— ì˜ì¡´ì ì´ë¼ ë°ì´í„°ì…‹ì„ í™•ë³´í•˜ëŠ” ë°ì— ë§ì€ ì‹œê°„ì„ ë“¤ì—¬ì•¼ í•œë‹¤. í”½ì…€ ë³„ë¡œ annotation í•˜ëŠ” ê²ƒì€ ë‹¤ë¥¸ visual task ì— ë¹„í•´ ë¹„ìš©ê³¼ ì‹œê°„ì´ ë” ë§ì´ ì†Œìš”ë˜ê¸° ë•Œë¬¸ì— ì›í•˜ëŠ” ë„ë©”ì¸ì˜ ë°ì´í„°ë¥¼ ì–»ê¸°ê°€ ì‰½ì§€ ì•Šë‹¤. ì´ì— ë”°ë¼ synthetic ë°ì´í„°ì— ëŒ€í•œ í™œìš©ì²˜ê°€ ë†’ì€ taskë¡œ íŒë‹¨í•˜ê³ , semantic segmentationì„ downstream taskë¡œ ì„¤ì •í•˜ì—¬ ì—°êµ¬ë¥¼ ì§„í–‰í•œë‹¤. ê¸°ì¡´ì˜ ì˜ ì•Œë ¤ì§„ task ëª¨ë¸ì„ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ìš©í•˜ë‚˜ synthetic ë°ì´í„°ë¥¼ í•¨ê»˜ í™œìš©í•˜ì—¬ í•™ìŠµí•˜ê¸° ë•Œë¬¸ì— ë„ë©”ì¸ í•™ìŠµ ë°ì´í„°ì˜ ì–‘ì´ ì ì„ ë•Œì—ë„ robustí•˜ê³  ì•ˆì •ì ì¸ ì„±ëŠ¥ì˜ ëª¨ë¸ì„ ë§Œë“¤ ìˆ˜ ìˆë‹¤.

UDAS í”„ë ˆì„ì›Œí¬ëŠ” Kê°œì˜ augmentation ìƒ˜í”Œ ë°ì´í„°ë¥¼ ë§Œë“¤ê³  ë°ì´í„° ê°„ consistency regularizationê³¼ maximum confidence map, confidence-basedë¥¼ ì ìš©í•˜ì—¬ unlabeled target ë°ì´í„°ì— pseudo-labelnigì„ í•œë‹¤. ì´ ë•Œ pseudo-labeling í•˜ëŠ” Teacher ëª¨ë¸ì€ synthetic ë°ì´í„°ë¡œ í•™ìŠµëœ ëª¨ë¸ì„ ì‚¬ìš©í•˜ê³ , pseudo-labeling í•œ ë°ì´í„°ì™€ í•¨ê»˜ Studnet ëª¨ë¸ì„ í•™ìŠµí•œ í›„ ì¼ì • ì´ìƒì˜ í•™ìŠµì´ ë˜ë©´ student ëª¨ë¸ì„ teacher ëª¨ë¸ë¡œ ì‚¬ìš©í•˜ì—¬ pseudo-labeling í•˜ëŠ” self-training ë£¨í”„ë¥¼ êµ¬ì„±í•œë‹¤.

ë³¸ ë…¼ë¬¸ì—ì„œëŠ” Unlabeled target ë°ì´í„°ì— Kê°œì˜ augmentation ë°ì´í„°ë¥¼ ìƒì„±í•˜ê³ , ê° ë°ì´í„°ì˜ prediction confidence mapì„ ë¹„êµí•˜ì—¬ ê°€ì¥ í° confidenceë¥¼ ê°–ëŠ” prediction ê°’ë§Œ ì·¨í•˜ëŠ” maximum confidence mapì„ ë§Œë“ ë‹¤. ê·¸ë ‡ê²Œ ë§Œë“¤ì–´ì§„ pseudo-label ì¤‘ì—ì„œ íŠ¹ì • confidence ê°’ ì´ìƒì˜ ì˜ˆì¸¡ ë°ì´í„°ë§Œ í•™ìŠµ ë°ì´í„°ë¡œ ì‚¬ìš©í•˜ëŠ” confidence-based maskingì„ ì‚¬ìš©í•¨ìœ¼ë¡œì¨ ì˜¤ë¥˜ ì „íŒŒìœ¨ì„ ë‚®ì¶˜ë‹¤.

UDA ì—°êµ¬ëŠ” ë„ë©”ì¸ ë¶„í¬ì˜ ì°¨ë³„ì„±ì— ë”°ë¼ ì ìš©í•  ìˆ˜ ìˆëŠ” ë„ë©”ì¸ì´ í•œì •ì ì¼ ìˆ˜ ìˆë‹¤. íƒ€ê²Ÿ ë„ë©”ì¸ ë°ì´í„°ì— ëŒ€í•œ ì •ë³´ê°€ ì „í˜€ ì—†ê¸° ë•Œë¬¸ì— ë„ë©”ì¸ ë¶„í¬ë¥¼ íŒŒì•…í•˜ëŠ” ê²ƒì´ ë³´ë‹¤ ì–´ë ¤ìš´ ì‘ì—…ì´ ë  ìˆ˜ ìˆê³  íŠ¹ì • ë„ë©”ì¸ í˜ì–´ì— í•œí•´ì„œë§Œ ì˜ ë™ì‘í•˜ëŠ” ëª¨ë¸ì´ ë  ìˆ˜ë„ ìˆë‹¤.

ë³¸ ë…¼ë¬¸ì—ì„œëŠ” Unsupervised Domain Adaptationì— Semi-supervised learningê¸°ë²•ì„ ê²°í•©í•˜ì—¬ ì ì€ Labeled target ë°ì´í„°ë¥¼ ê°–ê³  ìˆëŠ” ê²½ìš°ì—ëŠ” ë”ìš± ì‰½ê²Œ í•™ìŠµë˜ê³  ë†’ì€ ì„±ëŠ¥ì„ ë³´ì´ëŠ” ëª¨ë¸ì„ ë§Œë“œëŠ”  ë°©ë²•ì„ ì œì•ˆí•œë‹¤.

---
title: 💡 Paper
tags: paper 💡 🔥
mathjax_autoNumber: false
---

- **Self-training with combined unsupervised domain adaptation and semi-supervised learning**   
    (비지도 도메인 적응과 준지도 학습을 결합한 자가 학습)
- **Unsupervised domain adaptation with semi-supervised learning for semantic segmentation**   
    (의미론적 분할을 위해 반지도 학습 방법을 적용한 비지도 도메인 적응)
- **Semi-Supervised Domain Adaptation for Semantic Segmentation**   
    (의미론적 분할을 위해 반지도 학습 방법을 적용한 비지도 도메인 적응)

<!--more-->

- Self-supervised learning with image based synthetic teacher model
- Synthetic 이미지 기반의 교사 모델을 이용한 Self-training
- Semi-supervised domain adaptation for semantic segmentation
- A Unified Approch to Semi-supervised learning and domain adaptation
- Self-training for Unsupervised domain adaptation on semantic segmentation
- Self-training with Semi-supervised domain adaptation for semantic segmentation
- Self-training that combines unsupervised domain adaptation and semi-supervised learning for semantic segmentation
- Unsupervised domain adaptation with semi-supervised methods for semantic segmentation
- 기존 UDA에 SSL 방법론 적용

# 국문 요약 / 영문 요약

Data-driven 기반의 Supervised learning이 여러 visual task 분야에서 좋은 성과를 거둬오고 있다. 학습 모델이 더욱 커지고 깊어짐에 따라 양질의 학습 데이터가 더욱 중요해지고, 그에 따라 모델의 성능이 좌우되고 있다. 하지만 문제와 도메인에 맞는 annotation 데이터를 얻기 위한 비용은 human resource가 많이 들어가고 여전히 매우 비싸다.

이러한 데이터 부족 문제를 해결하기 위해 적은 데이터를 활용한 학습 방법인 Semi-supervised learning, Unsupervised learning나 비교적 얻기 쉬운 다른 도메인의 데이터를 이용한 Transfer learning 기반 방식이 활발하게 연구되고 있다.
이 때 현실과 유사하게 digital twin 세계를 구축하면 가상의 공간 내에서 원하는 Ground Truth 데이터를 무한하게 얻을 수 있고 더 나아가 현실 세계에서 얻기 어려운 데이터들도 비교적 쉽게 획득할 수 있다. 

본 논문에서는 시뮬레이션을 통해 얻을 수 있는 synthetic 데이터를 이용해 데이터 획득 비용을 줄이고, 유사한 도메인 데이터를 사용하여 학습하더라도 안정적인 성능을 보일 수 있도록 도메인 학습 데이터에 대한 의존성을 낮추는 모델을 제안한다.


Synthetic 데이터로 학습한 AI 모델이 현실 세계에서 잘 동작하기 위해서는 실제 데이터와 synthetic 데이터 간 distribution gap을 최소화 해야하는데, 이를 위해 Generative model, Adversarial learning, Pseudo-labeling 등의 기법들을 이용한 Unsupervised Domain Adaptation이 많이 연구 되고 있다. 하지만 대부분의 연구가 Domain shift를 줄이는 데에만 목적을 두고 있고 unlabeled 데이터 활용 방안에는 집중하고 있지 않다.

본 논문에서는 Synthetic 데이터를 이용한 Unsupervised Domain Adaptation (UDA)에 Semi-supervised learning (SSL)을 결합하여 domain 학습 데이터에 대한 의존성을 감소시키고, 갖고 있는 unlabeled 데이터까지 효과적으로 활용하는 UDAS (UDA + SSL) 모델을 제안한다. 


- 핵심어: UDomain Adaptation, Semi-supervised learning, Pseudo-labeling, Self-Training, Synthetic data, Semantic Segmentation

# 1. 서론

ImageNet의 데이터 공개를 시작으로 Data-driven 기반의 Supervised learning이 여러 visual task 분야에서 좋은 성과를 거두고 있다. 검증된 좋은 모델들이 이미 존재하기 때문에, 특정 문제에 맞는 AI모델을 만들기 위해서는 그에 맞는 양질의 데이터를 얼마나 많이 확보하느냐가 결국 모델의 성능을 좌우하게 되었다.

하지만 특정 도메인의 task에 맞는 데이터를 확보하고, 그 데이터에 labeling 하는 것은 많은 시간과 비용을 필요로 한다. 이러한 데이터 의존적인 문제들을 해결하기 위해 데이터가 부족하더라도 효과적인 학습을 할 수 있는 다양한 연구들이 진행되고 있다. 유사한 task에서 학습된 pretrain 모델을 가져와 target task에 적용하는 Transfer Learning, Domain Adaptation, 그리고 labeled data가 적거나 없을 때 사용하는 Semi-Supervised Learning, Self-Supervised Learning 등의 연구들이 있다.

Labeling 작업의 비용이 큰 문제도 있지만 특정 상황에 대한 데이터 자체를 얻기 어려운 경우도 있다. 특히나 자율주행차량, 로보틱스 분야에서는 현실 세계에서 얻기 어려운 데이터들이 존재한다. 예를 들어 교차로 한가운데에 사람이 있는 데이터나, 로봇의 카메라 시점에서의 데이터 등은 쉽게 획득할 수가 없을 것이다.

이 때 현실 세계와 비슷하게 구현된 시뮬레이션을 사용하면 가상의 공간에서 원하는 Ground Truth 데이터들을 비교적 쉽게 얻을 수 있다. 컴퓨터 그래픽과 계산식에 의하여 이미지, 센서, Bounding box, class 정보 등을 자동으로 생성하여 무한히 많은 synthetic 데이터들을 추출하고, 이 데이터들을 AI 학습에 이용할 수 있다면 human resource가 발생하는 annotation 작업 양이 현저히 줄어들 수 있을 것이다.



이 경우 실제 세계를 digital twin 하여 시뮬레이션을 구축하는 방안이 있다. 시뮬레이션을 사용하게 되면 현실 세계에서 재현할 수 없는 상황들을 연출하고 환경을 자유자재로 변경하여 데이터들을 추출할 수가 있다. 컴퓨터 그래픽에 의해 RGB 이미지, depth 데이터, 객체의 bounding box, pixel 별 class, optical flow 등의 정확한 Ground Truth 데이터들이 자동으로 생성되도록 할 수 있기 때문에, 제작 비용이 저렴한 synthetic 데이터를 이용해 AI 학습에 사용할 수 있다.

Synthetic 데이터를 사용해 학습한 모델이 현실 세계에서 잘 동작하기 위해서는 실제 세계와 시뮬레이션 간의 격차가 작아야만 한다. 하지만 시뮬레이션을 최대한 사실적으로 만들기 위해서는 많은 노력이 들어가고, 실제 세계의 모든 데이터들을 모델링하기에는 쉽지 않은 일이다.

이러한 synthetic 데이터의 장단점을 이해하고 synthetic 데이터와 현실 데이터와의 domain distribution gap을 줄일 수 있는 Unsupervised Domain Adaptation 방법론들이 연구되고 있다. Generative 모델을 사용하거나 adversarial learning, self-training 등을 결합한 다양한 방법들이 제시되고 있는데, 대부분의 연구들이 Domain shift 자체를 감소시키는 연구에 집중하고 있다. Target 데이터가 없다는 가정하에 문제가 전개되기 때문에 좋은 성능이 나오기 어려워진다.

본 논문에서는 synthetic 데이터를 활용한 Unsupervised Domain Adaptation (UDA)을 수행하면서 소량의 labeled target 데이터와 다량의 unlabeled target데이터가 존재하는 경우 이를 효율적으로 사용할 수 있도록 Semi-supervised learning (SSL)을 결합한 UDAS (UDA + SSL) 프레임워크를 제안한다. 


# 2. 관련 연구

## 2.1. Semantic Segmentation

Semantic segmentation는 이미지의 각 픽셀이 어느 클래스에 속하는 지 예측하는 것으로, 이미지의 전반적인 의미와 구조를 파악하여 더 깊이 있게 이해하는 작업이다. 픽셀 수준의 labeling이 수행되기 때문에 dense labeling task 라고 불리고, 이는 classification 이나 localization에 비해 어려운 문제로 분류된다.

Semantic segmentation 모델은 Supervised-learning 기반의 딥러닝 아키텍처가 나오면서 성능이 상당히 개선되었는데 최근에는 입력 공간 차원을 유지하며 전역의 semantic 을 추출하기 위해 encoder, decoder로 구성된 auto-encoder 구조가 많이 사용되고 있다. 대표적으로 FCN[1], PSPNet[2], DRN[3], DeepLab[4] 등과 같은 아키텍처들이 제안되었으며 좋은 성능을 보이고 있다.

하지만 대량의 labeled 데이터에 의존적이라 데이터셋을 확보하는 데에 많은 시간을 들여야 한다. 픽셀 별로 annotation 하는 것은 다른 visual task 에 비해 비용과 시간이 많이 소요되기 때문에 원하는 도메인의 데이터를 얻기가 쉽지 않다.

이에 따라 synthetic 데이터에 대한 활용처가 높은 task로 판단하고, semantic segmentation을 downstream task로 설정하여 연구를 진행한다. 기존의 잘 알려진 task 모델을 기반으로 사용하나 synthetic 데이터를 함께 활용하여 학습하기 때문에 학습 데이터의 양이 적을 때에도 robust하고 안정적인 성능의 모델을 만들 수 있다.


## 2.3. Unsupervised Domain Adaptation

일반적으로 labeled 데이터가 부족하면 pre-trained된 큰 모델을 이용하여 transfer learning을 수행하는 방법이 있다. Transfer learning을 하기 위해서도 해당 도메인에 맞는 일정 양의labeled 데이터가 필수적으로 필요하나, Domain Adaptation(DA)은 labeled 데이터가 없더라도 이전 task 에서 배운 지식을 활용하여 새로운 상황에서도 잘 예측하도록 학습한다. 즉, 다른 도메인(Source)에서 배운 지식을 활용해 실제  도메인(Target) 모델에 적용하기 위한 방법으로, Zero-shot learning, few-shot learning, self-supervised learning과 함께 sample-efficient learning의 한 유형이다.

![](/assets/images/21-09-24-paper-domain-adaptation.png)

Source 도메인과 target 도메인 간의 데이터 분포가 다르기 때문에 source 도메인에서 학습된 모델을 target 도메인에 사용하게 되면 distribution shift(또는 domain gap)와 dataset bias에 의해 오차가 발생할 가능성이 크다.

그렇기 때문에 도메인이 다른 학습데이터를 활용하기 위해서는 두 도메인의 distribution 불일치성을 줄이면서 동시에 task loss도 최소화 하는 모델을 만들어야 한다. DA는 source 와 target 도메인 간 격차를 줄이며 데이터 분포를 유사하게 학습한다. 


![](/assets/images/21-09-24-paper-adaptation-level.png)

DA 문제를 풀기 위한 주요 전략은 source 도메인과 target 도메인 사이의 격차를 줄임으로써 예측 모델의 성능 저하를 막는 방법이다. Input-level, feature-level, output-level에서 각각 adaptation 모듈을 추가하여 도메인 간 불일치성을 줄일 수 있다.

그 중 Unsupervised Domain Adaptation(UDA)는 Target 도메인의 unlabeled 데이터를 사용하기 위해 다른 도메인의 labeled 데이터를 사용하는 일종의 Semi-supervised learning의 변형이라고 볼 수 있다. Target 도메인의 labeled 데이터는 필요하지 않지만 충분한 양의 Unlabeled target 데이터가 필요하다.

Tzeng, Eric, et al.[5] 는 UDA에 GAN을 도입하여 adversarial adaptation을 처음 제안한 논문으로, feature level에 domain discriminator를 추가하고 adversarial loss를 사용하여 domain discrepancy를 최소화 하는 방법을 제안했다. 

Hoffman, Judy, et al.[6] 에서는 feature level에 adversarial learning을 그대로 사용하고, input-level에서 source 도메인 이미지를 target 이미지와 유사한 스타일로 생성하도록 generative 모델을 도입한 방법을 제안했다. Generative 모델을 도입함으로써 input 데이터의 숨겨진 확률분포를 찾아내고 유사한 확률분포의 데이터를 생성해 사용하는 방식이다. 

Tsai, Yi-Hsuan, et al.[7] 에서는 두 도메인의 이미지 스타일이 다르더라도 segmentation 결과에 포함되어 있는 공간 레이아웃, local context 등의 구조화된 특성은 유사하기 때문에 output-level에 adversarial learning을 도입하여 segmentation 예측 결과의 분포를 유사하게 학습하는 방법을 제안했다.

![](/assets/images/21-09-24-paper-da-methods.png)

정리하면 기존의 UDA 연구는 (a) source 도메인과 target 도메인의 불일치성을 명시적으로 측정하여 loss로 사용하는 방법 (b) domain confusion을 위해 discriminator와 learning loss를 사용하는 방법 (c) source 도메인 이미지를 target 이미지와 유사한 스타일로 생성하기 위해 input-level에 generative 모델을 추가하여 appearance gap을 줄이는 방법 (d) task 모델과 self-supervised 모델을 함께 학습시키는 방법으로 분류될 수 있다. 그 외에도 Classifier discrepancy 분석, Entropy minimization, Curriculum learning, Multi-tasking learning 방법들이 있고 이러한 방법론들을 변형하거나 적절이 섞어가며 다양한 방법들이 제안되고 있다. 

UDA 연구는 도메인 분포의 차별성에 따라 적용할 수 있는 도메인이 한정적일 수 있다. 타겟 도메인 데이터에 대한 정보가 전혀 없기 때문에 도메인 분포를 파악하는 것이 보다 어려운 작업이 될 수 있고 특정 도메인 페어에 한해서만 잘 동작하는 모델이 될 수도 있다.

본 논문에서는 Unsupervised Domain Adaptation에 Semi-supervised learning을 결합하여 적은 Labeled target 데이터를 갖고 있는 경우에 쉽게 학습되고 실제 학습 데이터로 사용될 수 있도록 실용성이 강화된 방법을 제안한다.


## 2.3. Semi-supervised learning

Semi-supervised learning(SSL)은 소량의 labeled 데이터를 이용한 학습 방법이다. 소량의 labeled 데이터로만 학습하게 되면 overfitting과 같은 문제가 발생할 가능성이 크기 때문에 소량의 labeled 데이터와 대량의 unlabeled 데이터를 함께 사용하여 성능을 향상시키기 위해 semi-supervised learning이 사용된다.

![](/assets/images/21-09-24-paper-self-training.png)

Self-training은 가장 간단한 SSL방법으로, 소량의 labeled 데이터로 충분히 학습된 모델을 사용하여 unlabeled 데이터에 pseudo-labeling 을 하고, pseudo labeled 데이터와 labeled 데이터를 함께 학습에 사용하는 방식이다. Iteration을 반복할 수록 labeled 데이터가 늘어나고 확장된 데이터셋을 이용해 모델의 성능도 함께 향상될 수 있다. 하지만 처음 pseudo-labeling을 하는 모델의 성능이 좋지 않으면 error rate이 계속해서 증가하기 때문에 잘못된 방향으로 학습이 될 수 있다는 문제가 있다.

![](/assets/images/21-09-24-paper-entropy-minimization.png)

그래서 Unlabeled data를 학습시킬 때 pseudo-labeling을 그대로 사용하지 않고, pseudo-label의 maximum softmax probability를 기준으로 예측 확률의 confidence를 높이기 위해 entropy minimization을 사용한다. 주로 softmax temperature를 적용하여 unlabeled 데이터의 예측값을 계산할 때 decision boundary에서 멀어지도록 더 sharp하게 예측 확률값을 정한다. 단일 기법으로는 성능이 좋지 않지만 최신 연구에 많이 적용해 사용하고 있다. 

최근 많이 사용되는 SSL 접근법은 Consistency regularization을 사용하는 것으로, unlabeled 데이터에 노이즈를 추가하더라도 예측 분포는 유지된다는 아이디어에 기안한 방법이다. 노이즈가 없는 원본 데이터와 노이즈가 주입된 데이터를 동일한 예측 분포로 학습하는 것이 consistency regularization의 목표이다.

Xie, Qizhe, et al. [8]는 Teacher-student 기반의 Self-training으로, labeled 데이터로 학습된 teacher를 이용해 unlabeled 데이터에 pseudo-labeling 하고 이 데이터에 노이즈를 추가하여 student 모델을 학습하는 방법이다. Student 모델이 어느정도 학습이 되면 teacher 모델로 설정하고 다시 pseudo labeling을 수행하게 만들어 반복적인 학습 파이프라인을 구성했고 이를 통해 SOTA를 기록할 정도로 좋은 성능을 보인 연구이다.

Pham, Hieu, et al. [9]은 Teacher 모델과 Student 모델을 동시에 학습하는 방법으로, Teacher 모델이 잘 학습되어 있지 않더라도 student 모델과 유기적으로 학습할 수 있도록 Co-training 방식을 제안하였다.

Miyato, Takeru, et al. [10]은 입력 데이터의 adversarial transformation을 생성하고 예측 결과 간의 KL-divergence를 측정하여 학습시키는 방법을 제안했다. Xie, Qizhe, et al. [11]은 노이즈 대신 최신 augmentation 기법인 AutoAugment를 사용하여 예측 결과 간 KL-divergence를 loss로 사용한 방법으로, 적은 labeled 데이터만으로 supervised learning을 뛰어넘는 성능을 보였다.

그리고 Berthelot, David, et al. [12], Sohn, Kihyuk, et al. [13] 등 Pseudo-labeling과 함께 augmentation, entropy minimization, consistency regularization을 포함한 다양한 기법을 함께 사용하여 다양한 SSL연구들이 나오고 있다.

본 논문에서는 Unlabeled target 데이터가 있는 경우 
..

본 논문에서는 Unlabeled target 데이터에 K개의 augmentation 데이터를 생성하고, 각 데이터의 prediction confidence map을 비교하여 가장 큰 confidence를 갖는 prediction 값만 취하는 maximum confidence map을 만든다. 그렇게 만들어진 pseudo-label 중에서 특정 confidence 값 이상의 예측 데이터만 학습 데이터로 사용하는 confidence-based masking을 사용함으로써 오류 전파율을 낮춘다.

# 3. 제안 방법

## 3.1. 개요

UDAS의 전체적인 프레임워크는 그림5과 같다. 크게 Domain Adaptation 을 위한 Teacher모델과 Teacher 모델로부터 Knowledge Distillation 을 받아 실제 task를 수행하는 Student 모델 두 부분으로 나눌 수 있다.

기본적인 구조는 UDA를 기반으로 하고 있어서 target labeled 데이터가 없더라도 target task를 수행할 수 있고, target labeled 데이터가 늘어남에 따라 Semi-Domain Adaptation 구조로 동작하면서 target task의 성능이 향상된다. 이 때 target labeled 데이터를 간접적으로 사용함으로써 target 데이터에 대한 의존성이 높아지지 않도록 한다. 

DA를 통해 학습된 모델을 이용하여 unlabeled target 데이터에 pseudo-labeling을 해주고 실제 target task를 수행하는 모델은 pseudo-labeling 된 데이터를 이용하여 Supervised learning 구조로 학습하게 된다. 

본 논문의 contribution은 다음과 같다.
-	Unsupervised Domain Adaptation와 Semi-supervised learning 을 결합한 모델 구조 제안
-	Labeled target 데이터 양을 줄였을 때 Supervised learning보다 안정적인 성능
-	기존 UDA SOTA 대비 높은 mIoU 달성

## 3.2. Style Transfer

시각적으로 봤을 때 큰 차이가 있는 데이터는 그만큼 큰 도메인 gap 이 있을 것이라고 가정할 수 있다. Input-level에서의 도메인 gap을 줄이기 위해 source 도메인의 image appearance가 target domain과 유사하도록 생성해낸다. 대표적인 image-to-image translation 방법인 CycleGAN을 이용하여 target domain 스타일의 이미지로 변형시켜 domain adaptation 모델의 source 데이터로 사용한다.

X가 source domain 샘플, Y가 target domain 샘플이라고 했을 때 X->Y 의 adversarial loss와 Y->X adversarial loss에 각각 원본으로 복구하는 cycle consistency loss를 더해준 값이 총 Loss가 되고, 이 Loss를 최소화 하는 방향으로 G: X->Y 와 F: Y->X 를 학습시킨다.

$$ L(G, F, D_x, D_Y) = L_{GAN}(G, D_Y, X, Y) + L_{GAN}(F, D_x, Y, X) + \lambda L_{cyc}(G, F) $$

$$ G^*, F^* = arg min_{G, F} max_{D_x, D_y} L(G, F, D_x, D_y) $$

Source 도메인과 target 도메인의 분포를 구분할 수 없도록 학습시키는 방법이기 때문에 도메인 간 gap이 줄었을 것이라고 기대할 수 있고, G: X->Y 모델을 통해 생성된 데이터는 target 도메인 분포에 가까운 형태가 될 수 있다. 결과적으로 Source 도메인 데이터를 그대로 사용하는 것 보다 Target 도메인의 스타일에 맞게 이미지를 재생성해서 target task 모델에 적용했을 때 성능이 향상되었다.

## 3.3. Unsupervised Domain Adaptation

UDA 모듈은 Adversarial Generative Model 구조로, 입력으로 image translation 된 source 이미지와 label, 그리고 label 이 없는 target 이미지를 사용하고 output-level에 discriminator를 추가하여 input-level adaptation, output-level adaptation을 수행한다.

먼저 image translation 된 source 이미지와 labels을 이용하여 source 도메인 기반으로 segmentation 모델을 학습한다. 사용된 모델은 multi-level segmentation 모델로, 기존 segmentation 모델에서 마지막 레이어로 prediction하는 primary classifier와 그 직전 레이어로 prediction 하는 auxiliary classifier로 구성된다. 

Primary classifier 와 auxiliary classifier를 모두 학습에 이용하기 위해 Primary classifier loss $L_{seg}^p$와 auxiliary classifier loss $L_{seg}^a$를 더하여 전체 segmentation loss로 사용한다. 여기서 $\lambda_{seg}$은 auxiliary classifier loss에 대한 가중치를 위한 하이퍼파라미터를 뜻한다. Segmentation loss function은 pixel-wise cross-entropy loss를 사용하고 전체 loss를 표현한 수식은 아래와 같다.

$$ L = -\sum_{h=1}^{H}\sum_{w=1}^{W}\sum_{c=1}^{C} y^i log(F(x^i))  $$

$$ L_{seg}^p = -\sum_{h=1}^{H}\sum_{w=1}^{W}\sum_{c=1}^{C} y_s^i log(F_p(x_s^i))  $$

$$ L_{seg}^a = -\sum_{h=1}^{H}\sum_{w=1}^{W}\sum_{c=1}^{C} y_s^i log(F_a(x_s^i)) $$

$$ L_{seg} = L_{seg}^p + \lambda_{seg} L_{seg}^a  $$

$$ L_{seg} = -\sum_{h=1}^{H}\sum_{w=1}^{W}\sum_{c=1}^{C} y_s^i log(F_p(x_s^i)) - \lambda_{seg} \sum_{h=1}^{H}\sum_{w=1}^{W}\sum_{c=1}^{C} y_s^i log(F_a(x_s^i)) $$

추가적으로 모델 학습 시에는 softmax temperature와 confidence-based masking을 추가하여 보다 정확한 예측을 하도록 유도한다.

이렇게 학습된 Segmentation 모델은 source 도메인 데이터만을 이용하여 학습되기 때문에 데이터 distribution이 다른 target 도메인에서는 오차가 발생할 확률이 높다.
Source 도메인과 target 도메인 간 불일치성을 줄이기 위한 방법으로 source domain classifier와 target domain classifier에 discriminator와 adversarial loss를 추가하여 prediction 결과에 대한 domain confusion을 야기시킨다. Primary classifier와 auxiliary classifier에 모두 적용되기 때문에 multi-level로 adaptation이 일어나고 이를 수식으로 표현한 전체 adversarial loss는 아래와 같다. 

$$ L_{adv}^p = \mathbb{E}[ log(D_p(F_a(x_s^i))) + log (1 - D_p(F_p(x_t^j))) ] $$

$$ L_{adv}^a = \mathbb{E}[ log(D_a(F_a(x_s^i))) + log (1 - D_a(F_a(x_t^j))) ] $$

$$ L_{adv} = \mathbb{E}[ log(D_p(F_a(x_s^i))) + log (1 - D_p(F_p(x_t^j))) ]  + \lambda_{adv} \mathbb{E}[ log(D_a(F_a(x_s^i))) + log (1 - D_a(F_a(x_t^j))) ]$$

Adversarial learning과 multi-level adaptation을 통해 source 도메인에서 학습한 feature들이 target 도메인 영역으로 전달되고 inter-domain knowledge, intra-domain knowledge를 동시에 학습할 수 있다. 

UDA의 전체 loss는 segmentation loss와 adversarial loss를 더한 값이다. Adversarial loss의 가중치를 조정하기 adversarial loss term에는 하이퍼파라미터 $\lambda_{adv}$ 를 곱해준다.

$$ L_{total} = L_{seg} + \lambda_{adv} L_{adv}$$

$$ L_{kl} = -\sum_{h=1}^{H}\sum_{w=1}^{W}\sum_{c=1}^{C} F_a(x_t^i) log(F_p(x_t^i)) - \sum_{h=1}^{H}\sum_{w=1}^{W}\sum_{c=1}^{C} F_p(x_t^i) log(F_a(x_t^i)) $$

## 3.4. Domain CutMix

Labeled target 데이터가 없는 경우에는 3.3.의 서술 내용처럼 UDA 구조로 동작할 수 있지만 소량의 labeled 데이터가 있는 경우 target 도메인에 대한 정보를 토대로 모델의 성능을 더욱 향상시킬 수 있다.

Labeled target 데이터를 supervised learning 형태로 그대로 이용하게 되면 당장의 성능은 좋아질 수 있겠지만 target 학습 데이터에 대한 의존성이 강해져 학습하지 못한 데이터에 대한 추론 능력이 떨어지고, source 도메인 데이터가 추가 됐을 때 모델 학습에 혼란을 야기할 수 있다. 이러한 문제를 방지하기 위해 본 연구에서는 도메인 독립적으로 모델을 학습시키기 위해 source 도메인 샘플과 target 도메인 샘플을 결합하여 학습 데이터로 사용한다. 

Semantic segmentation은 각 픽셀의 예측값과 더불어 주변 pixel에 대한 지역적인 예측 정보도 필요한데, 한 이미지에 source 도메인 이미지와 target 도메인 이미지가 결합되어 있으면 도메인과 관계 없이 이미지 자체의 representation을 학습할 수 있다. 이를 위해 CutMix[14]의 형태로 두 도메인의 이미지를 임의로 결합시켜 학습 데이터로 사용한다.

CutMix는 두 이미지를 섞어 하나의 이미지에 표현하는 Data augmentation 방법으로, 이미지의 전체적인 영역을 보고 학습할 수 있도록 유도하기 때문에 Regularization과 Localization 성능을 높일 수 있다. CutMix의 아이디어에 기반한 Domain CutMix는 도메인이 다른 두 이미지를 섞어 하나의 이미지로 생성한다.

우선 데이터 생성을 위해 매 배치마다 랜덤으로 bounding box $B$ 사이즈를 결정하고, $B$ 사이즈에 맞게 binary 마스크 $M$을 생성한다. $\lambda$는 영역의 비율을 뜻하는 파라미터이다. 

$$ r_x \sim Unif \{ 0, W \}, r_w = W \sqrt {1- \lambda}
\\
r_y \sim Unif \{ 0, H \}, r_h = H \sqrt {1- \lambda}
\\
B = \{ r_x , r_y, r_w, r_h \}
\\
M \in \left\{0, 1\right\}^{W \times H}
$$

그 후, Source 도메인의 샘플 $x_s$ 에서 $M$ 영역만큼 지우고, 그 부분에 target 도메인 샘플 $x_t$를 채운다. 마찬가지로 source 도메인 샘플에 대한 label 데이터인 $y_s$에서도 $\tilde{x} $와 페어되는 labeled cutmix 데이터를 만들어낸다.

$$
\tilde {x} = M \odot x_s + (1-M) \odot x_t
\\
\tilde {y} = \lambda_{y_s} + (1- \lambda)y_t
$$

이렇게 생성된 cutmix 데이터에 augmentation 기법인 AutoAugment를 취해서 새로운 데이터를 생성하게 된다. 이 데이터는 기존에 학습에 사용되던 source 이미지 대신 입력 데이터로 사용하여 segmentation 모델을 학습시킨다. 이미지 데이터와 label이 모두 존재하기 때문에 segmentation 모델을 supervised learning 방식으로 학습시킬 수 있다. 

Domain CutMix 데이터는 랜덤하게 결합되고 augmentation 까지 취해졌기 때문에 현실과 거리가 먼 이질적인 데이터처럼 보인다. 그럼에도 불구하고 도메인의 차이보다는 지역적인 정보들에 집중하여 학습하기 때문에 모델의 일반화 능력이 향상되고, 무엇보다 값싸게 데이터를 생성할 수 있다는 장점이 있다. 

Sim-to-real을 위한 Domain Randomization논문들[15][16][17]에 의거하면 랜덤하게 생성된 synthetic 데이터를 사용하면 모델이 더욱 robust 하게 학습되기 때문에 실제 세계에 적용할 때 더 좋은 성능을 보인다는 것을 실험적으로 알 수 있다.

## 3.5. Self-Training

DA를 통해 학습된 segmentation 모델은 바로 task 모델이 되지 않고, unlabeled target 데이터에 pseudo-labeling 하는데 사용된다. 


Domain Adaptation 모듈을 통해 얻어진 Task model 이 있지만 Target 도메인에서의 Fine-tuning을 위해 Unlabeled Target 데이터에 Pseudo-labeling을 하여 학습 데이터로 사용한다. 앞서 학습한 segmentation모델은 이제 Teacher 모델이 되어 target 도메인에 적용할 모델인 Student 모델에게 Knowledge Distillation 하게 된다.

Teacher 모델이 예측한 결과를 그대로 Pseudo-labels로 사용하게 되면 teacher 모델이 갖고 있는 오차 정보가 그대로 전파되기 때문에 Sharpening predictions 해주는 작업이 필요하다. 

![](/assets/images/21-09-24-paper-pseudo-labeling.png)

Pseudo-labeling 방법은 다음과 같다. Unlabeled target 데이터에 K개의 augmentation 데이터를 생성하고 각각 teacher 모델을 통해 prediction probability를 얻는다. Confidence map을 비교했을 때 가장 높은 confidence의 예측값을 사용하기 위해 maximum confidence map을 구성하고 이를 통해 만들어진 pseudo-label 중에서 특정 confidence 값 이상의 예측 데이터만 학습 데이터로 사용한다.

$$ \sum_{b=1}^{\mu B}  \lVert p_m (y | \alpha (u_b)) - p_m(y | \alpha(u_b)) \rVert _2 ^ 2 $$

> Consistency regularization

Unlabeled 데이터에 pseudo-labeling이 된 이후에는 source 도메인 데이터는 사용하지 않는다. Pseudo-labeling된 데이터들을 이용해 Student 모델을 Supervised learning 방식으로 학습하고, Student 모델이 target 도메인에 대하여 fine-tuning 되어 더 나은 성능에 도달하게 되면 기존의 pseudo-labeling 데이터를 업데이트 해준다. 그리고 재생성된 pseudo-labeled 데이터를 이용하여 Student 모델이 견고하고 신뢰도 높은 방향으로 재학습되도록 구성하여 반복적인 학습이 이루어지도록 한다.

$ q_b = p_m(y | a(u_b)) $ : weakly-augmentation's predicted class distribution

$ \hat{q}_b = argmax(q_b) $ : pseudo-label

$$ L =  {1 \over \mu B } \sum_{b=1}^{\mu B} 1 (max(q_b) \ge \tau ) H(\hat{q}_b, p_m(y | A (u_b))) $$

> Confidence-based Pseudo-labeling

# 4. 실험 및 결과

## 4.1. 실험 설계

UDAS 프레임워크 코드는 Ubuntu 20.04 환경에서 Python 3.8과 Pytorch 1.9.0 버전을 기준으로 구현되었고 모든 실험은 메모리 11GB의 GeForce GTX 1080 Ti 그래픽카드가 2개 장착된 PC에서 진행되었다.

### 4.1.1. 데이터셋

![](/assets/images/21-09-24-paper-dataset.png)

Cityscapes[18]는 유럽 50개 도시의 거리에서 차량 시점으로 캡쳐한 2048x1024 픽셀의 이미지 데이터셋이다. 2,975개의 training set, 500개의 validation set, 1,595개의 test set으로 총 5,000개의 이미지 데이터와 pixel-wise semantic label로 구성되어 있다. 추가로 label 되어 있지 않은 19,998 개의 raw 이미지 데이터도 제공된다.

GTA5[19]는 도시 운전을 위한 대규모 synthetic 데이터셋 중 하나이다. 사실적인 그래픽을 보여주는 GTA V 게임을 통해 렌더링 되었고 미국 스타일 도시에서의 자동차 관점에서 촬영되었다. 24,966개의 1914x1052 픽셀의 이미지와 pixel-wise semantic label로 구성되어 있다. Label은 Cityspcaes의 클래스에 맞게 19개의 클래스로 재매핑 할 수 있다.

Cityscapes와 GTA5는 시각적인 appearance 차이가 존재하지만 비슷한 차량의 높이에서 도로 상황을 캡쳐한 데이터라는 점에서 구조가 유사하다고 볼 수 있다. GTA5 데이터가 Cityscapes 데이터 양에 비해 약 5배 가량 많기 때문에 GTA5의 데이터를 최대한 활용하여 이미지의 전반적인 의미와 구조를 충분히 학습하고, Cityscapes 환경에서의 fine-tuning을 위하여 Cityscapes의 unlabeled 데이터를 함께 사용한다.

|  Dataset   |   Type    | # of Labeled Data | # of Raw Data | # of Classes |
| :--------: | :-------: | :---------------: | :-----------: | :----------: |
| Cityscapes |   Real    |       5,000       |    19,998     |      19      |
|    GTA     | Synthetic |      24,966       |       -       |      19      |

### 4.1.2. 평가 Metric

평가 metric으로는 Semantic segmentation 평가 시에 주로 사용되는 클래스 별 IoU(Intersection over Union) 와 mIoU(Mean Intersection over Union)를 사용한다. $TP_i$, $FP_I$, $FN_i$ 는 특정 클래스 $i$의 True Positive, False Positive, False Negative 이고 $N$은 클래스의 갯수이다.

$$ {IoU}_i = \left( \frac{TP_i}{FP_i + FN_i + TP_i} \right) $$

$$ mIoU = \sum_{i=1}^N \left( \frac{IoU_i}{N} \right ) $$

## 4.2. Implementation Details

대부분의 UDA SOTA 모델은 source 도메인 이미지 사이즈는 1280 x 720, target 도메인 이미지 사이즈는 1024 x 512로 원본 사이즈를 그대로 사용하여 학습한다. 아래 실험에서는 GPU 메모리 부족으로 모델은 동일하게 사용하나 source 도메인과 target 이미지 사이즈는 640 x 360으로 resize하여 실험한다. 

표[]는 학습 이미지 사이즈에 따른 성능 저하를 확인하기 위해 실험 환경과 동일하게 이미지 샘플 사이즈를 640 x 360으로 설정한 뒤 UDA SOTA 모델을 학습시킨 결과이다. 640 x 360 사이즈의 이미지를 사용했을 때 원 이미지 사이즈를 사용했을 때의 성능 대비 약 30% 감소한 것을 볼 수 있다. 이에 따라 이후에 기입될 실험 결과는 원 사이즈로 학습했을 때 성능이 더욱 향상될 것으로 예상할 수 있다.

| Model       | Road | Sidewalk | building | wall | fence | pole | Traffic light | Traffic sign | vegetation | terrain | sky  | person | rider | car  | truck | Bus  | train | motorcycle | Bicycle | mIoU  |
| :---------- | :--- | :------- | :------- | :--- | :---- | :--- | :------------ | :----------- | :--------- | :------ | :--- | :----- | :---- | :--- | :---- | :--- | :---- | :--------- | :------ | :---- |
| 1280 x 720 | 89.1 | 23.7     | 82.4     | 19.6 | 20.2  | 33.0 | 42.4          | 39.7         | 85.3       | 33.1    | 76.9 | 60.5   | 33.0  | 85.7 | 36.2  | 43.6 | 5.0   | 22.3       | 30.0    | 42.35 |
| 640 x 360  | 83.9 | 10.2     | 74.3     | 4.4  | 3.0   | 10.3 | 11.8          | 11.4         | 84.2       | 15.8    | 68.7 | 39.0   | 0.1   | 79.1 | 26.1  | 17.3 | 0     | 2.7        | 0       | 28.59 |

### 4.2.1. Semantic Segmentation

우선 일반적인 Supervised-learning 방식의 Semantic segmentation 모델과의 성능 비교를 위해 SOTA 모델인 FCN과 DeepLab v3+ 성능 비교를 수행하였다. Backbone은 Resnet-50으로 설정하였고 Cityscapes의 Labeled 이미지들로 Supervised-learning 방식으로 학습시켰다.

![](/assets/images/21-09-24-paper-sl-learning-result.png)

|    model    | road | sidewalk | building | wall | fence | pole | traffic light | traffic sign | vegetation | terrain | sky  | person | rider | car  | truck | bus  | train | motorcycle | bicycle | Pixel Accuracy | mIoU  |
| :---------: | :--: | :------: | :------: | :--: | :---: | :--: | :-----------: | :----------: | :--------: | :-----: | :--: | :----: | :---: | :--: | :---: | :--: | :---: | :--------: | :-----: | :------------: | :---: |
|     FCN     | 0.97 |   0.81   |   0.9    | 0.26 | 0.45  | 0.54 |     0.67      |     0.75     |    0.91    |  0.53   | 0.93 |  0.78  | 0.57  | 0.93 | 0.48  | 0.7  | 0.36  |    0.56    |  0.75   |     94.693     | 68.03 |
| DeepLab v3+ | 0.98 |   0.84   |   0.92   | 0.56 | 0.59  | 0.64 |      0.7      |     0.78     |    0.92    |  0.64   | 0.95 |  0.81  | 0.63  | 0.94 | 0.69  | 0.76 | 0.43  |    0.63    |  0.76   |     95.826     | 74.97 |

사용하는 모델과 Backbone에 따라 mIoU 값 차이가 있으나, Supervised learning으로 모델을 학습시켰을 때 mIoU가 약 70 정도 나오는 것으로 확인했다. 

많은 domain adaptation 논문들이 Resnet101을 backbone으로 한 DeepLab v2를 Semantic segmentation 모델로 사용하고 있기 때문에, 동등한 비교를 위해 이후 실험에서는 Atrous Spatial Pyramid Pooling(ASPP) 모듈을 포함한 DeepLab v2 모델을 task 모델로 설정하여 진행한다. Cityscapes 데이터셋을 이용해 학습한 Resnet101 + DeepLab v2 모델의 기본 성능은 아래와 같다.

| Model                  | road | walk | build. | wall | fence | pole | light | sign | vege. | terr. | sky  | persn | rider | car  | truck | bus | train | mcyc | Bike | mIoU |
| :--------------------- | :--- | :--- | :----- | :--- | :---- | :--- | :---- | :--- | :---- | :---- | :--- | :---- | :---- | :--- | :---- | :-- | :---- | :--- | :--- | :--- |
| DeepLab v2 (Resnet101) | 96.7 | 76.2 | 88.6   | 47.2 | 47.6  | 45.1 | 57.9  | 67.6 | 89.1  | 58.2  | 90.4 | 70    | 54.8  | 92.2 | 71.8  | 78  | 56.6  | 56.7 | 67.8 | 69.1 |

### 4.2.2. Image Translation

Source 이미지에서 Target 이미지 스타일로 image translation된 데이터가 도메인 gap을 줄이는데 얼마나 기여하는지 확인하기 위한 실험으로, 동일한 모델을 GTA 데이터와 Cityscapes 스타일로 재생성된 GTA->Cityscapes 데이터에 적용했을 때의 mIoU를 비교한다.

Labeled 데이터 없이 Cityscapes 이미지와 GTA 이미지만을 이용하여 CycleGAN 모델을 학습하고, CycleGAN 모델을 통해 GTA이미지를 Cityscapes 스타일의 이미지로 재생성한다. Image translation된 GTA->Cityscapes 샘플과 그에 대한 segmentation 예측 결과는 그림[]과 같다. 이 때 segmentation 모델은 4.2.1장에서 Cityscapes 데이터로만 학습시킨 DeepLab v3+ 모델을 사용했다.

![](/assets/images/21-09-24-paper-style-transfer-prediction.png)

| Data             | road | walk | building | wall | fence | pole | light | sign | vege. | terrain | sky  | person | rider | car  | truck | bus  | train | mCycle | bicycle | mIoU |
| :--------------- | :--- | :--- | :------- | :--- | :---- | :--- | :---- | :--- | :---- | :------ | :--- | :----- | :---- | :--- | :---- | :--- | :---- | :----- | :------ | :--- |
| GTA              | 70.2 | 19.3 | 60.4     | 23.3 | 13.4  | 25.3 | 31.8  | 17   | 53.5  | 24.3    | 85.6 | 41.2   | 1.7   | 55.4 | 19.1  | 16.8 | 2.4   | 6.4    | 5.4     | 30.1 |
| GTA → Cityscapes | 80.7 | 26.8 | 6.76     | 29.3 | 13.6  | 30.8 | 26.2  | 21.1 | 61.2  | 33.5    | 76.2 | 47.6   | 27    | 63   | 37.1  | 15.8 | 4.9   | 18.2   | 7.4     | 36.2 |
| Cityscapes       | 98   | 84.5 | 92.3     | 56   | 59.1  | 64.6 | 70.2  | 78.9 | 92.4  | 64      | 95   | 81.6   | 63.2  | 94.7 | 69    | 76.9 | 43.3  | 63.4   | 76.5    | 74.9 |

GTA에 대한 예측 결과는 30.1로 Cityscapes에 적용했을 때의 예측 결과인 74.9에 비해 현저히 낮은 성능을 보이고 있다. 현실과 비슷한 시뮬레이션 환경을 구축하고 사실적인 렌더링을 하였음에도 불구하고 큰 도메인 gap이 존재하는 것이다.

Image translation한 GTA->Cityscapes의 예측 결과는 36.2로, GTA 데이터를 그대로 사용했을 때 보다 mIoU가 6% 증가하였다. 이에 따라 appearance gap을 줄이는 방법을 통해 target 도메인과의 gap이 일부 줄어들었다고 판단할 수 있다. 하지만 여전히 target 도메인에 적용했을 때의 성능보다는 낮기 때문에 시각적으로 봤을 때 유사해보이더라도 여전히 보이지 않는 도메인 gap이 잔존하는 것을 알 수 있다.

이렇게 변환된 GTA->Cityscapes 데이터는 GTA 데이터 대신 source 도메인 데이터로 사용하여 input-level adaptation을 전체 학습 모델에 녹여낸다.

### 4.2.3. Domain CutMix

![](/assets/images/21-09-24-paper-combined-augmentation.png)

그림7은 Cityscapes 이미지와 GTA->Cityscapes 이미지를 결합하여 만든 Domain CutMix 데이터이다. 이미지가 어색하게 결합이 되어 있음에도 불구하고 모델이 일반화 되어 학습되었기 때문에 큰 오차 없이 예측하는 것을 볼 수 있다.

시각적인 부분에서 한단계 도메인 gap을 줄였지만 외관상으로 보이는 gap 이외에도 실제로 모델이 이미지를 이해하기 위해서는 보이지 않는 구조적인 부분 또한 학습해야 할 필요가 있다. 이를 위해 도메인을 혼합한 cutmix 데이터를 추가함으로써 구조적으로 이미지를 학습할 수 있도록 구성하였고, 이는 domain confusion을 야기하여 도메인 지식과 관계없이 task에 집중하여 학습할 수 있다.

### 4.2.4. Pseudo Labeling

![](/assets/images/21-09-24-paper-pseudo-labeling-result.png)

K=3으로 설정하고 Unlabeled target 데이터에 대해augmentation 데이터와 예측값의 confidence map을 얻어내고, maximum confidence map과 confidence-based masking을 통해 만들어진 pseudo-labeling 과정이다. Augmentation은 AutoAugment [20]를 사용했다. 

![](/assets/images/21-09-24-paper-pseudo-labeling-data.png)

이렇게 생성된 pseudo-labeling 데이터들을 이용해 student segmentation 모델을 학습시킨다.

## 4.3. Quantitative Comparison

### 4.3.1. Performance

![](/assets/images/21-09-24-paper-performance.png)

DA와 SSL 기법들이 적용됨에 따라 Target 도메인인 Cityscapes 데이터 대상으로 semantic segmentation예측을 했을 때 결과가 좋아짐을 볼 수 있다. 
Domain shift에 대한 고려없이 직접적으로 예측했을 때, 모델의 성능과 관계없이 많은 오차가 발생한다. 
Style Transfer를 적용한 데이터에는 어느 정도 semantic 구조를 파악한 것처럼 보이지만 여전히 많은 노이즈가 있다. 
Style Transfer 데이터를 source 데이터로 하여 UDA를 적용한 모델은 일반적인 UDA SOTA 수준의 성능을 보이고 있다. 
약간의 Labeled target 데이터가 있다는 가정하에 SSL 모듈을 도입하게 되면 target 도메인에 대한 정보들을 토대로 성능이 훨씬 향상된 것을 볼 수 있다. 

| Model               | road | walk | building | wall | fence | pole | light | sign | vege. | terrain | sky  | person | rider | car  | truck | bus  | train | mCycle | bicycle | mIoU |
| :------------------ | :--- | :--- | :------- | :--- | :---- | :--- | :---- | :--- | :---- | :------ | :--- | :----- | :---- | :--- | :---- | :--- | :---- | :----- | :------ | :--- |
| No Adaptation       | 83.9 | 10.2 | 74.3     | 4.4  | 3     | 10.3 | 11.8  | 11.4 | 84.2  | 15.8    | 68.7 | 39     | 0.1   | 79.1 | 26.1  | 17.3 | 0     | 2.7    | 0       | 28.5 |
| ST (Style Transfer) | 75.7 | 16.7 | 77.2     | 12.5 | 21    | 25.4 | 30    | 20.1 | 81.3  | 24.6    | 70.3 | 53.7   | 26.4  | 49.9 | 17.1  | 25.8 | 6.4   | 25.2   | 36      | 36.6 |
| ST + UDA            | 89.4 | 26.2 | 82.7     | 25.8 | 22.9  | 36.1 | 40.4  | 38.8 | 84.1  | 37.5    | 83.5 | 59     | 26.7  | 83.6 | 28.5  | 36.6 | 0.1   | 14.6   | 26.7    | 44.4 |
| ST + UDA + SSL      | 94.7 | 67.6 | 83.6     | 38.2 | 27.9  | 41.6 | 44.7  | 50   | 84.4  | 41.9    | 86.2 | 66.2   | 43.8  | 89.5 | 68.6  | 52.8 | 36.3  | 39.2   | 53.1    | 58.4 |

각 단계에 대한 클래스 별 IoU와 mIoU는 다음과 같다. SSL을 도입한 후 급격하게 성능이 향상되었고, Knowledge Distillation를 추가하여 불확실한 정보로 Supervised learning을 수행하더라도 성능이 상승한 것을 확인할 수 있다.

| Model           | road | walk | building | wall | fence | pole | light | sign | vege. | terrain | sky  | person | rider | car  | truck | bus  | train | mCycle | bicycle | mIoU |
| :-------------- | :--- | :--- | :------- | :--- | :---- | :--- | :---- | :--- | :---- | :------ | :--- | :----- | :---- | :--- | :---- | :--- | :---- | :----- | :------ | :--- |
| Cycada          | 79.1 | 33.1 | 77.9     | 23.4 | 17.3  | 32.1 | 33.3  | 31.8 | 81.5  | 26.7    | 69   | 62.8   | 14.7  | 74.5 | 20.9  | 25.6 | 6.9   | 18.8   | 20.4    | 39.5 |
| AdaptSegNet     | 86.5 | 36   | 79.9     | 23.4 | 23.3  | 23.9 | 35.2  | 14.8 | 83.4  | 33.3    | 75.6 | 58.5   | 27.6  | 73.7 | 32.5  | 35.4 | 3.9   | 30.1   | 28.1    | 42.4 |
| AdvEnt          | 89.4 | 33.1 | 81       | 26.6 | 26.8  | 27.2 | 33.5  | 24.7 | 83.9  | 36.7    | 78.8 | 58.7   | 30.5  | 84.8 | 38.5  | 44.5 | 1.7   | 31.6   | 32.4    | 45.5 |
| Seg-Uncertainty | 90.5 | 35   | 84.6     | 34.3 | 24    | 36.8 | 44.1  | 42.7 | 84.5  | 33.6    | 82.5 | 63.1   | 34.4  | 85.8 | 32.9  | 38.2 | 2     | 27.1   | 41.8    | 48.3 |
| Ours            | 94.7 | 67.6 | 83.6     | 38.2 | 27.9  | 41.6 | 44.7  | 50   | 84.4  | 41.9    | 86.2 | 66.2   | 43.8  | 89.5 | 68.6  | 52.8 | 36.3  | 39.2   | 53.1    | 58.4 |

UDA SOTA 모델들과 클래스 별 IoU와 mIoU를 비교한 결과이다. UDAS 모델이 월등하게 좋은 성능을 내고있다. 

### 4.3.2. Data Dependency

Supervised Learning(SL) 방법론의 경우 학습을 위해서는 많은 양의 Labeled 데이터가 필요하다. 적은 labeled 데이터로 학습시키게 되면 학습 데이터에 overfitting 되는 경향이 있다. 데이터가 적은 상황에서 제안 모델이 SL 방법에 비해 얼마나 안정적인 성능을 보이는지 비교하기 위한 Data Dependency실험이다.

Labeled target 이미지는 Cityscapes 데이터이고, 총 labeled 데이터 2,979장을 모두 사용하여 학습했을 때와 labeled 학습 데이터 양을 1000, 500, 100, 0으로 감소시키며 mIoU를 비교한 결과이다. 

| Model | 2975 | 1000         | 500           | 100           | 0             |
| :---- | :--- | :----------- | :------------ | :------------ | :------------ |
| SL    | 69.1 | 61.2 (-7.9%) | 55.4 (-13.7%) | 41.6 (-27.5%) | -             |
| UDAS  | 56.4 | 51.9 (-4.5%) | 52.8 (-3.6%)  | 49.2 (-7.2%)  | 43.9 (-12.5%) |

Labeled target 이미지가 2975장인 경우 UDAS모델이 -12.7% 의 성능을 보이고 있고, 1000장인 경우 -9.3%, 500장인 경우 -2.6%, 그리고 100장인 경우 +7.6%로, labeled target 데이터가 적어질 수록 UDAS가 SL 보다 좋은 성능을 보이는 것을 알 수 있다.

![](/assets/images/21-09-24-paper-udas-sl.png)

Labeled target 이미지에 따른 mIoU를 그래프로 시각화한 것이다. SL 모델은 labeled 데이터양이 적어짐에 따라 급격하게 성능이 하락하지만 UDAS 모델은 labeled 데이터양이 적어지더라도 안정적인 성능을 보이는 것을 알 수 있다. 이를 통해 제안한 UDAS 모델이 도메인 데이터에 대한 의존성이 낮아졌다고 미루어 짐작해볼 수 있다.

무엇보다 데이터가 전혀 존재하는 않는 경우, 제안 모델이 Source 도메인 데이터만으로 adaptation 하는 UDA 구조로 동작하기 때문에 labeled 데이터 없이는 무용지물이 되는 SL 모델에 비해 경쟁력 있다고 할 수 있다.

# 5. 결론

AI 학습에 있어서 항상 큰 장애물이 되는 데이터 부족 문제를 해결하기 위해 연구를 진행하였다. 컴퓨터 그래픽이 발전함에 따라 점차적으로 현실세계와 가상세계의 시각적 경계가 모호해지고 있기 때문에, 무한한 데이터를 창출해낼 수 있는 synthetic 데이터에 대한 활용도가 높아질 것이다. 또한 AI모델이 커지면서 점점 더 많은 데이터가 요구되는데 항상 데이터에 의존적인 AI 모델에서 벗어나 스스로 규칙을 찾아내는 모델을 만들고자 했다. 

본 논문에서는 Synthetic데이터를 이용하여 UDA와 SSL을 결합한 구조를 제안하였고 semantic segmentation 문제에 맞는 augmentation, pseudo-labeling 기법을 추가하여 데이터가 부족한 상황에서도 안정적인 성능을 보였다. 

UDA 방법론과 synthetic 데이터의 질이 향상함에 따라 제안한 UDAS 구조의 성능은 그와 비례하여 향상될 수 있을 것으로 예상한다.

# 6. 참고 문헌

[1] Long, Jonathan, Evan Shelhamer, and Trevor Darrell. "Fully convolutional networks for semantic segmentation." Proceedings of the IEEE conference on computer vision and pattern recognition. 2015.
[2] Zhao, Hengshuang, et al. "Pyramid scene parsing network." Proceedings of the IEEE conference on computer vision and pattern recognition. 2017.
[3] Yu, Fisher, Vladlen Koltun, and Thomas Funkhouser. "Dilated residual networks." Proceedings of the IEEE conference on computer vision and pattern recognition. 2017.
[4] Chen, Liang-Chieh, et al. "Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs." IEEE transactions on pattern analysis and machine intelligence 40.4 (2017): 834-848.
[5] Tzeng, Eric, et al. "Adversarial discriminative domain adaptation." Proceedings of the IEEE conference on computer vision and pattern recognition. 2017.
[6] Hoffman, Judy, et al. "Cycada: Cycle-consistent adversarial domain adaptation." International conference on machine learning. PMLR, 2018.
[7] Tsai, Yi-Hsuan, et al. "Learning to adapt structured output space for semantic segmentation." Proceedings of the IEEE conference on computer vision and pattern recognition. 2018.
[8] Xie, Qizhe, et al. "Self-training with noisy student improves imagenet classification." Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020.
[9] Pham, Hieu, et al. "Meta pseudo labels." Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021.
[10] Miyato, Takeru, et al. "Virtual adversarial training: a regularization method for supervised and semi-supervised learning." IEEE transactions on pattern analysis and machine intelligence 41.8 (2018): 1979-1993.
[11] Xie, Qizhe, et al. "Unsupervised data augmentation for consistency training." arXiv preprint arXiv:1904.12848 (2019).
[12] Berthelot, David, et al. "Mixmatch: A holistic approach to semi-supervised learning." arXiv preprint arXiv:1905.02249 (2019).
[13] Sohn, Kihyuk, et al. "Fixmatch: Simplifying semi-supervised learning with consistency and confidence." arXiv preprint arXiv:2001.07685 (2020).
[14] Yun, Sangdoo, et al. "Cutmix: Regularization strategy to train strong classifiers with localizable features." Proceedings of the IEEE/CVF International Conference on Computer Vision. 2019.
[15] Tremblay, Jonathan, et al. "Training deep networks with synthetic data: Bridging the reality gap by domain randomization." Proceedings of the IEEE conference on computer vision and pattern recognition workshops. 2018.
[16] Kar, Amlan, et al. "Meta-sim: Learning to generate synthetic datasets." Proceedings of the IEEE/CVF International Conference on Computer Vision. 2019.
[17] Andrychowicz, OpenAI: Marcin, et al. "Learning dexterous in-hand manipulation." The International Journal of Robotics Research 39.1 (2020): 3-20.
[18] Cordts, Marius, et al. "The cityscapes dataset for semantic urban scene understanding." Proceedings of the IEEE conference on computer vision and pattern recognition. 2016.
[19] Richter, Stephan R., et al. "Playing for data: Ground truth from computer games." European conference on computer vision. Springer, Cham, 2016.
[20] Cubuk, Ekin D., et al. "Autoaugment: Learning augmentation strategies from data." Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019.

---
title: üìù A Review of Single-Source Deep Unsupervised Visual Domain Adaptation
category: AI
tags: ai paper üìù üî•
---

A Review of Single-Source Deep Unsupervised Visual Domain Adaptation

<!--more-->

# Paper

- 2020
- [Paper](https://arxiv.org/pdf/2009.00155.pdf)

---

<details>
<summary>ENG</summary>
<div markdown="1">


Abstract
Large-scale labeled training datasets have enabled deep neural networks to excel across a wide range of benchmark vision tasks. However, in many applications, it is prohibitively expensive and time-consuming to obtain large quantities of labeled data. To cope with limited labeled training data, many have attempted to directly apply models trained on a large-scale labeled source domain to another sparsely labeled or unlabeled target domain. Unfortunately, direct transfer across domains often performs poorly due to the presence of domain shift or dataset bias. Domain adaptation is a machine learning paradigm that aims to learn a model from a source domain that can perform well on a different (but related) target domain. In this paper, we review the latest single-source deep unsupervised domain adaptation methods focused on visual tasks and discuss new perspectives for future research. We begin with the definitions of different domain adaptation strategies and the descriptions of existing benchmark datasets. We then summarize and compare different categories of single-source unsupervised domain adaptation methods, including discrepancy-based methods, adversarial discriminative methods, adversarial generative methods, and selfsupervision-based methods. Finally, we discuss future research directions with challenges and possible solutions.

 I. INTRODUCTION
 
 In the last decade, deep neural networks (DNNs) have achieved significant progress in various computer vision tasks where large-scale labeled training data are available. For example, the classification error of the ‚ÄúClassification + localization with provided training data‚Äù task in the Large Scale Visual Recognition Challenge was reduced from 0.28 in 2010 to 0.022 in 20171 , even outperforming humans. However, in many applications, it is difficult to obtain a large amount of labels, as manual annotation is expensive and time-consuming.

Source: GTA Target: Cityscapes (a) Object classification (b) Semantic segmentation Train on source Train on target mIoU on target Train on source 21.7 Train on target 62.6 Source: Art Target: Clipart Accuracy on target Train on source 34.9 Train on target 96.0 Ground truth car road bicycle sidewalk building sky person rider traffic sign vegetation Fig. 1. An example of domain shift. For both image-level object classification and pixel-wise semantic segmentation tasks, direct transfer of the models trained on the labeled source domain to the unlabeled target domain results in a dramatic performance drop.
An alternative solution is to train a model on another related large-scale source domain with labels (e.g. a simulation domain) and apply it to the unlabeled target domain (e.g. a real-world domain). However, due to the presence of domain shift or dataset bias [1], such a direct transfer might not perform well, as shown in Figure 1.
One may argue that the pre-trained source models can be fine-tuned in the target domain. However, fine-tuning still requires considerable quantities of labeled training data, which may be not available for many applications. For example, in fine-grained recognition, only experts are able to provide reliable labeled data [2]; in semantic segmentation, it takes about 90 minutes to label each image in the Cityscapes dataset [3]; in autonomous driving, the substantial traffic data obtained with different sensors, such as 3D LiDAR point clouds, are difficult to label [4, 5]; in affective image content analysis, the perceived emotions are subjective and personalized across viewers [6, 7].
a) Domain Adaptation in context of other sampleefficient learning methods: Domain adaptation techniques were introduced to addresses the domain shift between source and target domains [8] and for this reason, they have recently attracted significant interest in both academia and industry.
Domain adaptation (DA), also known as domain transfer, is a specialized form of transfer learning [9] that aims to learn a arXiv:2009.00155v3 [cs.CV] 19 Sep 2020 UNDER REVIEW 2 Task loss Discrepancy loss Source data Target data Feature extractor Classifier Feature extractor Weight sharing (a) Discrepancy-based methods Task loss Adversarial loss Source data Target data Feature extractor Classifier Feature extractor Discriminator Weight sharing (b) Adversarial discriminative methods Task loss Adversarial loss Source data Target data Classifier Feature extractor Discriminator Generator Adapted data GAN loss Feature extractor Weight Discriminator sharing (c) Adversarial generative methods Task loss Self-supervised loss Source data Target data Feature extractor Classifier Feature extractor Auxiliary head Weight sharing (d) Self-supervision-based methods Fig. 2. Classification of widely employed framework of different single-source deep unsupervised domain adaptation (DUDA) pipelines. Most existing methods can be obtained by employing different component values, slightly changing the architecture, or combining different pipelines.
model from a labeled source domain that can generalize well to a different (but related) unlabeled or sparsely labeled target domain. It belongs to the sample-efficient learning class [10], together with zero-shot learning, few-shot learning, and selfsupervised learning. We briefly compare domain adaptation with other methods in this category. While the unsupervised domain adaptation (UDA) does not require the annotations of target data, it usually needs a sufficient number of unlabeled target samples to train the model [11]. Compared to UDA, zero-shot learning does not need either the target data annotations or the unlabeled target samples [12, 13]. However, existing methods often require some auxiliary information, such as the attributes of the images, or the description of the classes [14, 15]. Further, zero-shot learning is trained on known/seen classes and tested on unknown/unseen classes, which demands the model to generalize from known/seen classes to unknown/unseen classes. Since the known/seen classes and the unknown/unseen classes are from different distributions, there is no concept of domain shift in zeroshot learning. Different from zero-shot learning, DA deals with the same learning tasks on different domains. Taking image classification as an example, both source data and target data have the same classes. Few-shot learning shares similar setting with zero-shot learning. The difference is that fewshot learning has a few (e.g. 5 or 10) annotated samples for the unknown/unseen classes [16‚Äì18]. Few-shot learning and zero-shot learning can also be grouped as low-shot learning.
Self-supervised learning (SSL) is a learning paradigm that captures the intrinsic patterns and properties of input data without using human-provided labels [19]. The basic idea of SSL is to construct some auxiliary tasks solely based on the data itself without using human-annotated labels and force the network to learn meaningful representations by performing the auxiliary tasks well. Typical self-supervised learning approaches generally involve two aspects: constructing auxiliary tasks and defining loss functions [20]. The auxiliary tasks are designed to encourage the model to learn meaningful representations of input data. The loss functions are defined to measure the difference between a model√¢AÀò Zs prediction and a fixed ¬¥ target, the similarities of sample pairs in a representation space (e.g., contrastive loss), or the difference between probability distributions (e.g., adversarial loss). Compared with domain adaptation, SSL does not specifically address the domain shift problem between different domains.
b) Domain Adaptation Challenges: Albeit DA is a very effective method, it is not without blemish. The main challenge for single-source UDA is domain shift [1], i.e., the difference between the source and target distributions that leads to unreliable predictions on the target domain. Typically, three types of domain shift are considered: covariate shift, label shift, and concept drift (see Section II for details).
As Figure 1 shows, the presence of domain shift causes the direct transfer of models trained on the source domain to perform poorly on the target domain. Figure 1 (a) shows an example of image-level object classification on the OfficeHome dataset [21]. When training a ResNet-50 model [22] on the target Clipart domain, we can obtain a promising 96.0% object classification accuracy. However, if we train the model on the source Art domain and directly test it on the target domain, the accuracy significantly drops to 34.9%.
Figure 1 (b) shows an example of simulation-to-real adaptation, which is a more realistic application with unlimited synthetic labeled data created by graphics and simulation infrastructure. For example, CARLA2 , GTA-V3 , and Autoware.AI4 are three popular simulators for autonomous driving research. While there are ongoing efforts to make simulations more realistic, it is very difficult to model all the characteristics of real data [23]. Using the FCN model [24] to conduct pixelwise segmentation on a real target dataset Cityscapes [3], direct transfer from synthetic GTA [25] only obtains a mean intersection-over-union (mIoU) of 21.7%, which is much lower 2http://www.carla.org 3https://www.rockstargames.com/V 4https://www.autoware.ai/ UNDER REVIEW 3 than the mIoU 62.6% of the model trained on real Cityscapes.
c) Focus of this Survey and Comparison with Other Surveys: There are many different domain adaptation settings (see Section II for details). Our survey focuses on the most prevalent one: single-source, single-target, homogeneous, and closed set adaptation without target labels. In this setting, there is one fully labeled source domain and one unlabeled target domain within the same modality, and the source and target domains share the same label set.
The early unsupervised domain adaptation (UDA) methods were mainly non-deep approaches, which aimed to match the feature distributions between the source domain and the target domain. Single-source UDA methods can be divided into two categories [26, 27]: sample re-weighting [28, 29] and intermediate subspace transformation [30‚Äì34].
With the advent of deep learning, the emphasis has shifted to end-to-end learning domain-invariant features. Typically, for single-source deep UDA (DUDA) [8, 35], a conjoined architecture with two streams is employed to represent the models for the source and target domains, respectively [36].
Besides the traditional task loss, such as cross-entropy loss for classification, based on the labeled source data, DUDA models are usually trained jointly with another loss to deal with domain shift, such as a discrepancy loss, adversarial loss, or self-supervision loss. The single-source DUDA methods are divided into four categories based on domain shift loss and generative/discriminative settings, as shown in Figure 2.
There have been several surveys on domain adaptation and transfer learning. In particular: [9] covers different transfer learning paradigms, such as self-taught learning and multi-task learning, but the domain adaptation part is not comprehensive; [26] deals for the most part with early methods with little discussion devoted to recent deep learning-based methods; [27] covers almost all categories of domain adaptation methods briefly but comparisons are scant; [37, 38] focus on shallow methods and only a few deep methods are reviewed; [39, 40] analyze multi-source domain adaptation respectively focusing on shallow and deep methods. Similar to [41‚Äì43], we focus on deep domain adaptation, but use a different taxonomy that provides different insights. There are also some blogs summarizing recent papers on different transfer learning methods5 and domain adaptation6 strategies. Compared to existing surveys and blogs, our survey has the following advantages/contributions: (1) we cover and compare the latest methods on deep unsupervised domain adaptation; (2) we provide an analysis that includes advantages/disadvantages of different categories and differences/connections among different methods in each category with summarizing tables; (3) we give suggestions and prospects for future directions to explore; (4) we systematically compare the results of existing methods on popular benchmark datasets; and (5) we discuss some important aspects that are overlooked in previous surveys, such as label shift and theory.
d) Organization of This Survey: In this survey, we review recent progress on visual domain adaptation, comparing 5https://github.com/jindongwang/transferlearning 6https://github.com/zhaoxin94/awesome-domain-adaptation advantages and disadvantages of different approaches in this class, and discussing future directions.
In particular: First, we define different DA strategies in Section II. Second, we summarize the available datasets for performing DA evaluation focusing on computer vision tasks in Section III. And then, we briefly introduce the theoretical view in Section IV-A, summarize and compare the representative approaches on different single-source DUDA categories, including discrepancy-based methods (Section IV-B), adversarial discriminative methods (Section IV-C), adversarial generative methods (Section IV-D), self-supervision-based methods (Section IV-E), and combinations and others (Section IV-F), followed by both qualitative and quantitative comparisons of different categories in Section IV-G and Section IV-H. Finally, we discuss potential future research directions in Section V.
II. DOMAIN ADAPTATION TAXONOMY We introduce a standard definition of the variables and models to enable effective comparisons and classification. Let x and y 7 respectively denote the input data and output label, drawn from a specific domain probability distribution P(x, y).
In typical domain adaptation, there is one source domain and one target domain. Suppose the source data and corresponding labels drawn from the source distribution PS(x, y) are XS and YS respectively, and the target data and corresponding labels drawn from the target distribution PT (x, y) are XT and YT respectively. The corresponding marginal distributions include PS(x), PS(y), PT (x), PT (y), and conditional distributions include PS(x|y), PS(y|x), PT (x|y), PT (y|x). Three typical sources of variation between the two domains considered in the literature include: 1) covariate shift, where PS(y | x) = PT (y | x) for all x, but PS(x) 6= PT (x); 2) label shift, where PS(x | y) = PT (x | y) for all y, but PS(y) 6= PT (y); 3) concept drift, where PS(x, y) 6= PT (x, y).
In addition, the source dataset is DS = {XS, YS} = {(x i S , yi S )} NS i=1, the target dataset is DT = {XT , YT } = {(x j T , y j T )} NT j=1, where NS and NT are the number of source samples and target samples respectively, x i S ‚àà RdS and x j T ‚àà RdT are referred as observations in the source domain and the target domain, and y i S and y j T are the corresponding class labels.
Suppose the number of labeled target samples is NT L; then, the DA problem can be classified into different categories: ‚Ä¢ unsupervised DA, when NT L = 0; ‚Ä¢ fully supervised DA, when NT L = NT ; ‚Ä¢ semi-supervised DA, otherwise.
Suppose the number of labeled source samples is NSL; then, DA can be classified into: ‚Ä¢ strongly supervised DA, when NSL = NS; ‚Ä¢ weakly supervised DA, otherwise.
Based on the representations, dS and dT , of the source and target domains (e.g. images vs. text), we can classify DA into: 7 In this paper we assume x is an image and y could be any label type (e.g.
object classes, bounding boxes, semantic segmentation, etc).
UNDER REVIEW 4 TABLE I CLASSIFICATION OF DOMAIN ADAPTATION (DA) STRATEGIES.
Standard Classification Definition Short description Representative methods target label unsupervised DA NT L = 0 target data is fully unlabeled [11, 23, 36, 44‚Äì131] fully supervised DA NT L = NT target data is fully labeled [41] semi-supervised DA 0 < NT L < NT target data is partially unlabeled [108, 132‚Äì134] source label strongly-supervised DA NSL = NS source data is fully/strongly labeled [11, 23, 36, 39, 40, 44‚Äì104, 107‚Äì151] weakly-supervised DA NSL < NT source data is weakly labeled [105, 106] homogeneity homogeneous DA dS = dT source and target data are observed in the same space [11, 23, 36, 44‚Äì106, 113‚Äì151] heterogeneous DA dS 6= dT source and target data are observed in different spaces [107‚Äì112] #sources single-source DA NS = 1 there is only one source domain [11, 23, 36, 44‚Äì134] multi-source DA NS > 1 there are multiple source domains [39, 40, 135‚Äì151] #targets single-target DA NT = 1 there is only one target domain [11, 23, 36, 44‚Äì112, 115‚Äì151] multi-target DA NT > 1 there are multiple target domains [113, 114] label set closed-set DA CS = CT the label sets of source and target domains are the same [11, 23, 36, 44‚Äì114, 123‚Äì151] open-set DA CS ‚à© CT ‚äÇ CT source label set is a proper subset of target label set [115‚Äì117] partial DA CS ‚à© CT ‚äÇ CS target label set is a proper subset of source label set [118‚Äì121] universal DA CS ? CT no prior knowledge of the label sets is available [122] target data domain adaptation with XT target data is known during training [11, 23, 36, 44‚Äì151] domain generalization without XT target data is unknown during training [152‚Äì154] ‚Äò#sources‚Äô and ‚Äò#targets‚Äô respectively represent the number of source domains and the number of target domains; NS, NT , NSL, NT L respectively denote the numbers of source samples, target samples, labeled source samples, and labeled target samples; dS, dT are the data dimensions of source data and target data respectively; CS, CT respectively represent the label set for the source and target domains; XT is the target data without labels.
‚Ä¢ homogeneous DA, when dS = dT ; ‚Ä¢ heterogeneous DA, otherwise.
Suppose the number of source domains is NS; then, the DA task can be categorized into: ‚Ä¢ single-source DA, when NS = 1; ‚Ä¢ multi-source DA, when NS > 1.
Similarly, let NT denote the number of target domains; we can then classify DA into: ‚Ä¢ single-target DA, when NT = 1; ‚Ä¢ multi-target DA, when NT > 1.
Let CS and CT respectively denote the label set for the source and target domains; then, we can define DA into the following different categories: ‚Ä¢ closed set DA, when CS = CT ; ‚Ä¢ open set DA, when CS ‚à© CT ‚äÇ CT ; ‚Ä¢ partial DA, when CS ‚à© CT ‚äÇ CS; ‚Ä¢ universal DA, when no prior knowledge of the label sets is available; where ‚à© and ‚äÇ indicate intersection and proper subset.
Although without labels, the target data is usually available during training in typical DA. If the target data is also unavailable, we often denote this task as domain generalization or zero-shot DA. Therefore, we have: ‚Ä¢ domain adaptation, when XT is available during training; ‚Ä¢ domain generalization or zero-shot DA, when XT is unavailable during training.
The classification of different DA categories are summarized in Table I. We focus on the review of recent unsupervised domain adaptation (UDA) methods under homogeneous, single-source, single-target, strongly-supervised, and closedset settings, i.e. NT L = 0, dS = dT , NS = 1, NT = 1, NSL = NS, CS = CT . The goal is to learn a model f that can correctly predict a sample from the target domain based on labeled {XS, YS} and unlabeled {XT }.
III. DATASETS In the early years, the datasets for DA were mainly collected from real world scenarios. Increasingly, large-scale synthetic datasets are being generated from simulation engines with labels automatically obtained, which induces large domain shift from the real world data. The released datasets are summarized in Table II.
Digit recognition. MNIST [155] is a dataset of handwritten digits with a training set of 60K examples and a test set of 10K examples. The digits have been size-normalized and centered in a fixed-size image. USPS [157] is also a dataset of handwritten digits with 7,291 examples for training and 2,007 examples for testing. MNIST-M [156] is created by combining MNIST digits with the patches randomly extracted from color photos of BSDS500 [169] as their background.
It contains 59,001 training and 90,001 test images. Synthetic Digits [156] consists of 500K images generated from Windows fonts by varying the text (that includes one-, two-, and threedigit numbers), positioning, orientation, background and stroke colors, and the amount of blur. SVHN [158] is obtained from house numbers in Google Street View images. It contains 73,257 digits for training, and 26,032 digits for testing.
Object classification. Office-31 [159] is a standard benchmark for DA. There are 4,110 images within 31 categories collected from office environments in 3 image domains: Amazon (A) with 2,817 images downloaded from amazon.com, Webcam (W), and DSLR (D), with 795 and 498 images taken by web camera and digital SLR camera with different photographical settings, respectively.
Office+Caltech [29] consists of the 10 overlapping categories shared by Office-31 [159] and Caltech-256 (C) [170].
Office-Home [21] consists of about 15,500 images from 65 categories of everyday objects in office and home settings.
There are 4 different domains: Artistic images (Ar), Clip Art (Cl), Product images (Pr) and Real-World images (Rw).
VisDA-2017 [160] is a challenging testbed for UDA with UNDER REVIEW 5 TABLE II RELEASED AND FREELY AVAILABLE DATASETS FOR DOMAIN ADAPTATION, WHERE ‚Äò# SAMPLES‚Äô REPRESENTS THE TOTAL NUMBER OF SAMPLES, ‚ÄôTYPE‚Äô INDICATES WHETHER THE DATA IS COLLECTED FROM SIMULATION OR REAL WORLD, ‚ÄôSYN‚Äô IS SHORT FOR ‚ÄôSYNTHETIC‚Äô, ‚ÄôCLA‚Äô, ‚ÄôREG‚Äô, ‚ÄôDET‚Äô, AND ‚ÄôSEG‚Äô ARE SHORT FOR ‚ÄôCLASSIFICATION‚Äô, ‚ÄôREGRESSION‚Äô, ‚ÄôDETECTION‚Äô, AND ‚ÄôSEGMENTATION‚Äô, RESPECTIVELY.
Task Dataset Ref # Samples Labels Resolution Type Task Short description digit recognition MNIST (M) [155] 70K 10 classes 28 √ó 28 real cla size-normalized and centered handwritten digits MNIST-M (M-M) [156] 149,002 10 classes 32 √ó 32 real cla MNIST digits with color photos as their background USPS (U) [157] 9,298 10 classes 16 √ó 16 real cla unconstrained handwritten digits SVHN (S) [158] 99,289 10 classes 32 √ó 32 real cla digits obtained from house numbers in Google Street View images Synthetic digits (SD) [156] 500K 10 classes 32 √ó 32 syn cla digits generated from Windows fonts by varying types of conditions object classification Office-31 (O) [159] 4,110 31 classes - real cla images from 3 domains: Amazon (A), Webcam (W) and DSLR (D) Office+Caltech (OC) [29] 2,533 10 classes - real cla shared categories of Office-31 and Caltech-256 (C) Office-Home (OH) [21] 15,500 65 classes - real cla images from 4 domains: Ar, Cl, Pr and Rw VisDA-2017 (V) [160] 280,157 12 classes - syn/real cla, seg about 280K images from synthetic data to real imagery DomainNet (DN) [148] 569,010 345 classes - syn/real cla the largest DA dataset for object classification with 6 domains pose estimation UnityEyes (UE) [23] 1.2M gaze degrees 640 √ó 480 syn reg synthetic images for eye gaze estimation MPIIGaze (MG) [161] 214K gaze degrees 36 √ó 60 syn reg real images for eye gaze estimation NYU (N) [162] 81,008 hand pose 224 √ó 224 syn/real reg images from both syn and real domains for hand pose estimation 3D point cloud KITTI-LiDAR (K-L) [163] 10,848 3 classes 64 √ó 512 real cla, seg LiDAR point cloud with point-wise labels for autonomous driving segmentation GTA-LiDAR (G-L) [44] 100K 2 classes 64 √ó 512 syn cla, seg LiDAR point cloud synthesized in GTA-V object detection Cityscapes (CS) [3] 3,475 8 classes 2048 √ó 1024 real det converted from Cityscapes with instance segmentation mask Foggy Cityscapes (FC) [164] 3,475 8 classes 2048 √ó 1024 syn det adding synthetic for to the original Cityscapes images SIM10k (SM) [165] 10,000 8 classes 1914 √ó 1052 syn det images synthesized in GTA-V for object detection KITTI-Obj (K-O) [163] 7,481 8 classes 1250 √ó 375 real det images collected from real urban scenes for object detection Syn2Real-D (S2R) [166] 248K 13 classes - syn/real det images from 3D CAD models and real-world detection datasets semantic segmentation Cityscapes (CS) [3] 5,000 30 classes 2048 √ó 1024 real seg images collected from real urban scenes for semantic segmentation BDDS (B) [167] 8,000 19 classes 1280 √ó 720 real seg images collected from real urban scenes for semantic segmentation GTA (G) [25] 24,966 19 classes 1914 √ó 1052 syn seg images synthesized in GTA-V for semantic segmentation SYNTHIA (SY) [168] 400K 16 classes 960 √ó 720 syn seg synthetic urban images for semantic segmentation the domain shift from synthetic data to real imagery. There are about 280K images from 12 categories, including a training set with 152,397 synthetic images, a validation set with 55,388 real-world images, and a test set with 72,372 real-world images. The dataset is used in the Visual Domain Adaptation Challenge, including classification and segmentation tracks.
DomainNet [148], the largest DA dataset to date for object classification, containing about 600K images from 6 domains: Clipart, Infograph, Painting, Quickdraw, Real, and Sketch.
There are 345 object categories altogether.
Pose estimation. UnityEyes [23] is a synthetic dataset with 1.2M images for eye gaze estimation. MPIIGaze [161] contains 214K real eye gaze images captured under extreme illumination conditions.
NYU [162] is a hand pose dataset with 72,757 training frames and 8,251 testing frames captured by 3 in 1 frontal and 2 side views. Each depth frame is labeled with hand pose information that is used to create a synthetic depth image.
3D point cloud segmentation. KITTI [163] is a realworld dataset for autonomous driving with images, LiDAR scans, and 3D bounding boxes organized in sequences. KITTILiDAR consists of 10,848 samples with point-wise labels obtained from 3D bounding boxes. Each point belongs to a car, pedestrian, or cyclist. 8,057 samples are used for training and the rest 2,791 samples are for testing.
GTA-LiDAR [44] contains 100K LiDAR point clouds synthesized in GTA-V using the method in [5] to do ImageLiDAR registration. The wide variety of scenes, car types, and traffic conditions, which ensures the diversity of the synthetic data. The categories car and pedestrian are synthesized.
Object detection. Cityscapes [3] is a dataset of real urban scenes containing 3,475 images with pixel-level annotations.
Since it is not designed for object detection, tightest rectangle of an instance segmentation mask is used as the ground truth bounding box [45].
Foggy Cityscapes [164] is a recently proposed synthetic foggy dataset simulating fog on real scenes. The depth maps provided in Cityscapes are used to simulate three intensity levels of fog [164]. Each foggy image is synthesized from an image with depth map from Cityscapes.
SIM10k [165] is a synthetic dataset collected from the computer game GTA-V. It contains 10,000 images with bounding box annotations for cars.
KITTI-Obj [163] is a real-world dataset consisting of 7,481 labeled images. The images are collected from various traffic situations, including freeways, rural and urban areas.
Syn2Real-D [166] is a dataset consisting of 248K images.
The synthetic images are collected from 3D CAD models while the real-world images are from COCO [171] and YouTube Bounding Boxes [172].
Semantic segmentation. Cityscapes [3] contains vehiclecentric urban street images collected from a moving vehicle in 50 cities from Germany and neighboring countries. There are 5,000 images with pixel-wise annotations, including a training set with 2,975 images, a validation set with 500 images, and a test set with 1,595 images. It is widely used in segmentation.
BDDS [167] contains thousands of real-world dashcam video frames with accurate pixel-wise annotations. It has a label space that is compatible with Cityscapes.
GTA [25] is collected in the high-fidelity rendered computer game GTA-V with pixel-wise semantic labels. It contains 24,966 images (video frames). There are 19 classes that are compatible with Cityscapes.
SYNTHIA [168] is a large synthetic dataset. To pair with Cityscapes, a subset, named SYNTHIA-RANDCITYSCAPES, was designed with 9,400 images which are automatically annotated with 16 compatible classes, one void class, and some unnamed classes.
IV. SINGLE-SOURCE DUDA In this section, we first introduce a theoretical view for domain adaptation. Second, we summarize different categories UNDER REVIEW 6 of single-source DUDA. Finally, we compare the advantages and disadvantages of these methods.
A. Theory Brief Many methods in the domain of machine learning are based on empirical evidence rather than well-founded theory. The ones that have solid theory background use statistics profusely.
Domain adaptation is no exception. However, upper bounds on the generalization target error by learning from the source data have been obtained. In a seminal paper, (author?) [173] provided a bound for domain adaptation on the target risk that generalizes the standard bound on the source risk. Informally, the theory says that if there exists a common hypothesis (classifier) that generalizes well on both the source and the target domains, the performance difference of any classifier on these two domains could be bounded by the distance between the data distributions of the two domains. The authors proposed H-divergence, a parametric pseudo-metric to measure the distance between two domains. Using this this pseudometric, two domains are considered close if there exists a binary classifier (a discriminator) that, upon seeing data from the two domains, can distinguish which domain the data comes from. This work formalizes the intuitive notion that reducing the two distributions while ensuring a low error on the source domain, yields accurate results in the target domain. Further, the theory justifies the basis of many recent DA algorithms that learn domain-invariant representations, using either domain adversarial classifier or discrepancy-based approaches.
(author?) [174] introduced a new divergence measure: the discrepancy distance, which was used to provide a generalization guarantee for the target domain. Compared with the H-divergence that can only be used in the setting of binary classification, the discrepancy distance could be used to achieve a generalization bound for target domain in regression setting as well. In a later work, (author?) [175] derived generalization bounds on the target error by making use of the robustness properties introduced in [176]. Extensions of the above theory to multi-source domain adaptation for both classification and regression problems also exist [146, 177].
B. Discrepancy-based Methods (Table III) Discrepancy-based methods explicitly measure the discrepancy between the source and target domains on corresponding activation layers of the two network streams. Long et al. [46] designed a Deep Adaptation Network, where the discrepancy is defined as the sum of the multiple kernel variant of maximum mean discrepancies (MK-MMD) between the fully connected (FC) layers. Sun et al. [47] proposed correlation alignment (CORAL) to minimize domain shift by aligning the second-order statistics of the source and target features of the last FL layer. Apart from the CORAL loss on the last FL layer, Zhuo et al. [36] also incorporated the CORAL loss on the last convolutional (conv) layer. To deal with the high dimension of convolutional layer activations, activationbased attention mapping is employed to distill it into low dimensional representations. The CORAL losses on both the last convolutional layer and the last FC layer are combined.
TABLE III COMPARISON OF DIFFERENT DISCREPANCY-BASED METHODS, WHERE ‚ÄòDISCREPANCY‚Äô INDICATES THE DISCREPANCY LOSS, ‚ÄòLOSS LEVEL‚Äô INDICATES WHAT LEVEL THE LOSS IS APPLIED TO, ‚ÄòLAYER‚Äô REPRESENTS THE LAYERS THAT THE LOSS FUNCTIONS ON, ‚ÄòWEIGHT‚Äô INDICATES WHETHER THE WEIGHTS OF THE TWO NETWORKS ARE SHARED OR NOT, ‚ÄòDISTRIBUTION‚Äô INDICATES WHAT TYPE OF DISTRIBUTION IS ALIGNED.
Ref Discrepancy Loss level Layer Weight Distribution [46] MK-MMD domain FL shared marginal [47] CORAL domain last FL shared marginal [36] CORAL domain last (conv, FL) shared marginal [48] weight, MMD domain all linear marginal [49] CMD domain all shared marginal [50] HoMM domain FL shared marginal [51] CDD class all except BN shared marginal [69] RKHS class all shared marginal [44] Geodesic domain all shared marginal [52] JMMD domain FL shared joint [53] Implicit domain BN shared marginal [54] Implicit domain weighted BN shared marginal [55] SWD domain all shared marginal [56] MEC domain DWT layer shared marginal Wu et al. [44] studied the UDA problem for 3D LiDAR point cloud segmentation from synthetic data to real world data.
At every batch of training, in addition to the focal loss to learn semantics from the point cloud on the synthetic batch, they employed the geodesic distance to penalize discrepancies between batch statistics from two domains. In recent papers, Zellinger et al. [49] proposed to match the higher order central moments of probability distributions by means of order-wise moment differences. They utilized the equivalent representation of probability distributions by moment sequences to define a new distance function, called Central Moment Discrepancy (CMD). Chen et al. [50] explored the benefits of using higherorder statistics (in this case mainly third-order and fourth-order statistics) for domain matching. They proposed a Higher-order Moment Matching (HoMM) method, and further extended the HoMM into reproducing kernel Hilbert spaces (RKHS).
Some other types of divergence are also designed to align the source and target domains. Lee et al. [55] designed sliced Wasserstein discrepancy (SWD) to capture the natural notion of dissimilarity between the outputs of task-specific classifiers.
It provides a geometrically meaningful guidance to detect target samples that are far from the support of the source and enables efficient distribution alignment in an end-to-end trainable fashion. Roy et al. [56] proposed domain alignment layers which implement feature whitening for the purpose of matching source and target feature distributions. Additionally, they leveraged the unlabeled target data by proposing the Min-Entropy Consensus loss, which regularizes training while avoiding the adoption of many user-defined hyper-parameters.
Instead of explicitly modeling the discrepancy between the source and the target domains, some papers implicitly minimize domain discrepancy by aligning the Batch Normalization (BN) statistics. Li et al. [53] proposed to adopt domain specific normalization for different domains. The proposed Adaptive BN (AdaBN) replaces the moving average mean and variance of all BN layers in the task network trained on the source domain with the mean and variance estimated from the target mini-batches. AdaBN [53] and other DUDA methods define a prior on which layers should be adapted. Instead, Cariucci UNDER REVIEW 7 TABLE IV COMPARISON OF DIFFERENT ADVERSARIAL DISCRIMINATIVE MODELS, WHERE ‚ÄòEN‚Äô IS SHORT FOR ENCODER. ADVERSARIAL LEVEL REFERS TO THE LEVEL OF ALIGNMENT FOR THE DISCRIMINATOR INPUT, EITHER GLOBALLY OR CLASS-WISELY.
Ref Adversarial level En weight Discriminator input Discriminator type Discriminator loss [57] global shared Fs, Ft feature discriminator GAN loss [58] global shared Fs, Ft gradient reversal layers Cross-Entropy [11] global unshared Fs, Ft, yÀús, yÀút feature, output discriminator GAN loss [59] class, global shared Fs, Ft, yÀús, yÀút feature discriminator GAN loss [60] global shared (Fs + Fnoise), Ft feature discriminator GAN loss [61] global shared yÀús, yÀút, output discriminator GAN loss [62] global shared Fs, Ft, yÀús, yÀút conditional discriminator GAN loss, Conditional Entropy [63] global shared idomain, yÀús, yÀút joint discriminator Cross-Entropy [64] global unshared Fs, Ft, FÀÜs, FÀÜt feature, prototypical discriminator GAN loss [65] global shared Fs, Ft feature discriminator GAN loss [66] global shared Fs, Ft, Fm, xs, xt, xm output discriminator GAN loss [67] class shared Fs, Ft joint discriminator GAN loss [68] global shared xs, xt gradient reversal layers Binary Cross-Entropy of confusion matrix [45] global, instance shared Fs, Ft feature discriminator GAN loss [129] global shared Fs, Ft feature discriminator GAN loss [131] global shared Fs, Ft gradient reversal layers Cross-Entropy [125] global shared Fs, Ft feature discriminators GAN loss [126] global shared Fs, Ft gradient reversal layers Cross Entropy [127] global shared Fs, Ft gradient reversal layers Cross Entropy Fs, Ft, Fnoise : Extracted features of source image, target image, or input noise; Fm : mixed feature of Fs and Ft; yÀús, yÀút : Predicted labels of source or target images; idomain : index of domain; xs, xt: source and target images; xm : mixed image of xs and xt.
et al. [54] proposed to learn automatically which layers of the network should be aligned and the corresponding alignment degree. The Auto-DomaIn Alignment Layer (AutoDIAL) is embedded multiple times to align the learned feature representations of the source and target domains at different levels.
These BN-based methods have fewer parameters to tune, higher computational efficiency, and competitive performance.
The methods described above measure the domain discrepancy at the domain level, which neglects the information concerning the classes from which the samples are drawn and thus may lead to misalignment and poor performance.
Kang et al. [51] proposed Contrastive Adaptation Network, which optimizes a new metric, Contrastive Domain Discrepancy (CDD), by explicitly minimizing the intra-class discrepancy and maximizing the inter-class domain discrepancy. The source and target samples of the same underlying class are drawn closer, while the samples from different classes are pushed apart. Pan et al. [69] recently proposed Transferrable Prototypical Networks, which perform domain alignment such that prototypes for each class in the source and target domains are close in the embedding space and the predictions from prototypes separately on source and target data are similar.
Most of the papers mentioned above consider aligning the marginal distributions in the feature space. When confronted with complex tasks, these approaches would fail when the label distributions are drastically different between source and target domains. The joint alignment of distributions DS = (XS, YS) and DT = (XT , YT ) is considered in [52] under the assumptions that PS(x) 6= PT (x) and PS(y|x) 6= PT (y|x).
The joint distributions across domains is projected to a Reproducing Kernel Hilbert Space (RKHS) H and MMD is used as the distance metric. During the joint distribution alignment, the distribution shift PS(x) and PT (x), PS(y|x) and PT (y|x) are significantly reduced.
The above methods all adopt weight-sharing between the two streams of the Siamese architecture [178, 179] that attempts to reduce the impact of domain shift by learning domain-invariant features. However, domain invariance may be detrimental to discriminative power. On the contrary, Rozantsev et al. [48] proposed to explicitly model the domain shift and relaxed the weight-sharing constraint to a linear correlation. They jointly optimized a weight regularizer, representing the loss between corresponding layers of the two streams, and an unsupervised regularizer, encoding the MMD measure and favoring similar distributions of the source and target representations.
C. Adversarial Discriminative Models (Table IV) Adversarial discriminative models usually employ an adversarial objective with respect to a domain discriminator to encourage domain confusion (see Table IV). In the early-stage of adversarial discriminative models, the domain adversarial training of neural networks is proposed to learn domain invariant and task discriminative representations [58]. It is directly derived from the seminal theoretical works of Ben et al. [173] and directly optimizes the H-divergence between source and target. By deriving the generalization bound on the target risk and obtaining an empirical formulation of the objective, Ganin et al. [58] proposed the Domain-Adversarial Neural Networks (DANN) algorithm. From this point of view, the adversarial discriminative models are originally similar to the discrepancy-based models. Recently, a couple of adversarial discriminative models were proposed with different algorithms and network architectures, thus differing from the discrepancybased methods.
Suppose mS and mT are the representation mappings of the source and target domains, respectively, and d is a domain discriminator, which classifies whether a data point is drawn from the source or the target domain. The adversarial discriminator is trained typically based on an adversarial loss Lad . The loss Lam used to train representation mapping is different in existing methods. The Domain-Adversarial Neural Network [58] optimizes the mapping to minimize the discriminator loss directly Lam = ‚àíLad , which might be problematic, since the UNDER REVIEW 8 discriminator converges quickly during training, causing the gradients to vanish. A gradient reversal layer was proposed to achieve domain adversarial training with a single feed-forward network with standard backpropagation and stochastic gradient descent. Tzeng et al. [11] proposed Adversarial Discriminative Domain Adaptation (ADDA), using an inverted label GAN loss to split the optimization process into two independent objectives for the generator and discriminator.
Besides aligning marginal distributions, several methods also align conditional or joint distributions. Long et al.
[62] considered aligning conditional distribution across domains, and proposed Conditional Domain Adversarial Network (CDAN). Based on DANN [58], they used conditional discriminator D(f √óg) with improved discriminability, where f is feature extractor and g is classifier, to capture the cross-covariance between feature representations and classifier predictions. To extend joint distribution alignment, Du et al.
[67] used dual adversarial strategy to train a dual-discriminator to pit against each other. Cicek et al. [63] also aimed for joint distribution P(d, y) alignment over domain d and label y by a joint predictor and aligned its output with classifier‚Äôs prediction.After analyzing the drawbacks of feature-level alignment methods, Liu et al. [65] proposed Transferable Adversarial Training (TAT), not only adapting feature representations from difference domains, but also generating transferable examples to make the classifier learn a more robust decision boundary.
Xu et al. [66] explored two common limitations in current adversarial-based methods. Sampling from source and target domains separately is insufficient to ensure domain-invariance at the whole latent space, and does not give the discriminator a hard label to judge real and fake samples. They proposed a mixed version of the discriminator to guarantee domaininvariance in a more continuous latent space, thus improving the robustness of models performance. Chen et al. [68] adopted the concept of self-training. They analyzed the noise of pseudo-labels in the confusion matrix and proposed correspondingly an adversarial-learned loss to accurately estimate the confusion matrix. In this way, their proposed method inherits the strength of both adversarial learning and selftraining paradigm.
Hoffman et al. [57] made the very first effort for domain adaptation in semantic segmentation. They employed a pixellevel adversarial loss to enforce the network to extract domaininvariant features for semantic segmentation and further applied category-specific constraints, e.g. pixel percentage histograms. Instead of only performing domain adversarial globally, Chen et al. [59] proposed to perform feature alignment jointly at the global and class-wise levels by leveraging soft labels from source and target-domain data. Hong et al. [60] proposed to learn a conditional generator to transform features of synthetic images to real-image like features, and perform domain adversarial training on the learned features. However, the proposed method is network-specific and only applied to the FCN model structure. While previous works mostly perform feature alignment in the middle of a network, Tsai et al. [61] adopted adversarial learning in the output space. To further enhance the adapted model, they constructed a multilevel adversarial network to effectively perform output space domain adaptation at different feature levels. To address DA in object detection, [126, 129] applied multi-level domain alignment with adversarial training, and Chen et al. [45] performed domain alignment on both image level and instance level. Weak alignment model was introduce in [127] which focused the adversarial alignment loss on images that are globally similar, putting less emphasis on aligning images which are globally dissimilar. Zhu et al. [125] instead proposed to perform adversarial learning on region level for domain alignment. Recently, Zhent et al. [131] proposed a coaraseto-fine feature adaptation approach for object detection. Different from image level or instance level feature alignment, foreground regions are extracted by attention mechanism, and aligned through multi-layer adversarial learning. Based on prototypical representations, Hu et al. [64] recently proposed a Prototypical Adversarial Learning scheme to align both feature representations and intermediate prototypes across domains.
D. Adversarial Generative Models (Table V) Adversarial generative models combine the domain discriminative model with a generative component generally based on generative adversarial nets (GANs) [181], which include a generator g and a discriminator d. g takes random noise z as input to generate a virtual image, while d takes the output of g and real images x as input to classify whether an image is real or generated. The learning process is that d tries to maximize the probability of correctly classifying real and generated images, while g tries to generate images to maximize the probability of d making a mistake. The Coupled Generative Adversarial Networks (CoGAN) [180] is composed of a tuple of GANs, and each is responsible for synthesizing images in one domain. CoGAN corresponds to a constrained min-max game of two teams, each with two players.
Instead of taking random noise as input, the generator of more recent GAN based methods is usually conditioned on the source data. Shrivastava et al. [23] proposed simulated and unsupervised learning (SimGAN) to improve the realism of a simulator‚Äôs output using unlabeled real data. The discriminator‚Äôs loss in SimGAN is the same as is used in a traditional GAN, while a self-regularization loss is added in the refiner (generator) loss to ensure that the refined data do not change much, which aims to preserve the annotation information. The generator in pixel-level DA [70] is conditioned on both a noise vector and an image from the source domain. To penalize large low-level differences between the source and generated images for foreground pixels only, the model learns to minimize a masked Pairwise Mean Squared Error (PMSE) which only calculates the masked pixels (foreground) of the source and the generated images. Sankaranarayanan et al. [73] proposed to learn a mutual feature embedding for source and target images, and to generate intermediate domain images from source and target embeddings. They also designed a multiclass discriminator to encourage the model to extract more class-discriminative features.
To overcome the under-constrained nature of GAN, [182] proposed CycleGAN with a cycle-consistency constraint.
Based on the CycleGAN loss, some effective adaptation UNDER REVIEW 9 TABLE V COMPARISON OF DIFFERENT ADVERSARIAL GENERATIVE MODELS. ‚ÄòWEIGHT‚Äô INDICATES WHETHER THE WEIGHTS OF DIFFERENT GANS ARE SHARED.
Ref Architecture & loss Input of GAN Weight Generative level Discriminative level Specific objective [180] CoGAN z partially shared pixel-level pixel-level joint distribution learning without paired images [23] GAN with new Lg xS - pixel-level pixel-level self-regularization [70] GAN, masked-PMSE z, xS - pixel-level pixel-level masked-PMSE minimization [71] CycleGAN, semantic, feature xS, xT , fea unshared pixel-level pixel-level, feature-level semantic consistency enforcement [72] CycleGAN, attention map xS, xT unshared pixel-level pixel-level, feature-level adversarial attention alignment [73] GAN with class labels supervision xS, xT shared pixel-level pixel-level class-consistent generation [74] CycleGAN with class labels supervision xS, xT shared pixel-level pixel-level, feature-level attribute-conditioned, photometric transformation [75] CycleGAN with conditional loss xS, xT shared pixel-level pixel-level, feature-level conditioned on classifier prediction [76] CycleGAN with Lc xS, xT shared pixel-level pixel-level channel-wise statistics feature alignment [77] Coupled GAN, VAE, cycle-consisteny xS, xT partially shared pixel-level pixel-level joint-distribution alignment in latent space [128] GAN, constraint loss Lcon xS, xT shared pixel-level pixel-level, feature-level domain diversification [130] CycleGAN, feature xS, xT shared pixel-level feature-level pixel-level and feature-level alignment methods were introduced. Hoffman et al. [71] proposed discriminatively-trained Cycle-Consistent Adversarial Domain Adaptation (CyCADA), which adapts representations at both the pixel-level and feature-level, enforces cycle-consistency, and leverages a task loss to perform semantic segmentation adaptation. Similarly, Russo et al. [78] introduced bidirectional image translation mapping and proposed classconsistency loss. While CycleGAN [182] can only translate low-level appearance, e.g. texture, [74] realized multiple view-point transformation combining with key-point detection network. Similarly, Tzeng et al. [87] performed domain adaptation on object detection using pixel-level alignment and feature-level alignment. Extending previous CycleGANbased works [71, 87, 182], Li et al. [75] proposed cycleconsistent conditional adversarial transfer networks (3CATN) to improve adversarial training and feature generation process by conditioning on the classifier prediction. Instead of using a discriminator, Wu et al. [76] explored channel-wise statistics alignment of CNN features to guide the generation process.
Liu et al. [77] combined CoGAN [180] with Variational Autoencoder (VAE) [183] to perform unsupervised image-toimage translation. A shared latent space between source and target domains is inferred to align the joint distributions of different domains. And then training data closer to the target domain can be sampled from the shared latent space. Besides the CycleGAN loss, Kang et al. [72] proposed to impose the attention alignment penalty to reduce the discrepancy of attention maps across domains. To make the attention mechanism invariant to domain shift, the target network is trained with a mixture of real and synthetic data from both source and target domains. Hsu et al. [130] leveraged CycleGAN together with feature-level alignment for object detection adaptation. Kim et al. [128] further proposed to generate diversified intermediate domains to help domain-invariant representation learning for object detection. A multi-domain discriminator is leveraged to encourage the feature to be indistinguishable among the domains.
E. Self-supervision-based Methods (Table VI) Self-supervision based methods incorporate auxiliary selfsupervised learning task(s) into the original task network.
Training the self-supervision task jointly with the original task network is helpful to bring the source and target domains closer. Ghifary et al. [184] designed a three-layer Multi-task Autoencoder (MTAE) architecture to transform the original image into analogs in multiple related domains. The hiddeninput and hidden-output weights represent shared and domainspecific parameters, respectively. The learned features are then used as input to a classifier. The category-level correspondence across domains is required. Self-domain and between-domain reconstruction tasks are introduced as the self-supervision task and are performed during training. Deep reconstruction classification network (DRCN) [79] combines a convolutional supervised network for source label prediction with a deconvolutional unsupervised network for target data reconstruction. The feature mapping parameters of the two streams are shared, while the labeling parameters of the supervised network and the decoding parameters of the unsupervised network are learned individually.
MTAE requires that the number of samples of corresponding categories in the two domains should be the same. After the sample selection procedure, some important information may be missing. Further, the output of the algorithm is learned features, based on which a classifier (multi-class Support Vector Machine with a linear kernel in this paper) needs to be trained. DRCN employs an end-to-end strategy, without the requirement of aligned pairs. The above two methods use the same encoder to extract domain-invariant features, ignoring the individual characteristics of each domain. Bousmalis et al. [80] explicitly learned to extract image representations that are partitioned into two subspaces. One component is private to each domain, which aims to capture domain-specific properties, such as background. The other is shared across domains with the goal of capturing shared representations by using autoencoders and explicit loss functions, i.e. scaleinvariant mean-square error (SIMSE).
Except for the reconstruction task, more recent selfsupervision tasks (e.g. image rotation prediction and jigsaw prediction) have been used for DA [81‚Äì83]. Xu et al. [83] suggested using self-supervision pretext tasks (e.g. image rotation, patch location prediction) over a feature extractor. Feng et al. [84] proposed to use self-supervision pretext tasks as part of their framework for domain generalization. Carlucci et al.
[82] proposed to solve domain adaptation/generalization by introducing a jigsaw puzzle as a self-supervision task. Images are decomposed into 9 patches which are then randomly shuffled and used to form images of the same dimension of the original ones. The Maximal Hamming distance algorithm is used to define a set of patch permutations and assign an UNDER REVIEW 10 TABLE VI COMPARISON OF DIFFERENT SELF-SUPERVISION-BASED METHODS, WHERE ‚ÄòEN‚Äô AND ‚ÄòDE‚Äô ARE SHORT FOR ENCODER AND DECODER, ‚ÄòNEn‚Äô AND ‚ÄòNDe‚Äô RESPECTIVELY INDICATE THE NUMBER OF ENCODERS AND DECODERS, ‚ÄòLOSS‚Äô INDICATES THE EMPLOYED SELF-SUPERVISION LOSS, ‚ÄòS‚Äô AND ‚ÄòT‚Äô IN THE ‚ÄòDOMAIN‚Äô COLUMN REPRESENT SOURCE DOMAIN AND TARGET DOMAIN IN WHICH THE RECONSTRUCTION IS PERFORMED.
Ref NEn En base net NDe Domain De weight Self-supervision tasks Loss [184] 1 shared 1 S, T unshared Reconstruction L2 [79] 1 shared 1 T ‚Äì Reconstruction L2 [80] 2 shared/unshared 1 S, T shared Reconstruction SIMSE [81] 1 shared 3 S, T shared Image rotation, Patch Location, Flip prediction Cross-Entropy (CE) [82] 1 shared 1 S, T shared Jigsaw Puzzle CE [83] 1 shared 1 T ‚Äì Image rotation, Spatial-aware rotation prediction CE [84] 1 shared 1 S, T shared MI minimization & maximization Mutual Information (MI) [85] 1 shared 0 S, T ‚Äì Instance Discrimination & Cross-domain entropy minimization CE & Entropy [86] 1 shared 1 S, T shared Region Reconstruction L2 index to each of them. The convolutional network is optimized to satisfy two objectives: object recognition on the ordered images and jigsaw classification, namely the permutation index recognition, on the shuffled images. Sun et al. [81] further proposed to perform domain adaptation by jointly learning multiple self-supervision tasks. Source and target images share the same convolutional feature encoder, and the extracted features are then fed into different self-supervision task heads: image rotation prediction, patch location prediction, and flip prediction. Since images from different domains normally have many low-level visual differences, e.g. brightness, texture, etc., self-supervision tasks aiming to predict pixel values of the original images are usually not quite helpful. Because of this, self-supervision tasks that predict high-level structural labels are more favorable for domain adaptation. Kim et al. [85] proposed a cross-domain self-supervised learning approach for DA. It captures apparent visual similarities with both in-domain and across-domain self-supervision. Consequently, they could perform DA with only few source labels. Selfsupervised learning has also been introduced into point-cloud adaptation [86], in which region reconstruction is introduced as a new pretext task.
F. Combinations and Others Some techniques combine several of the above-mentioned methods to jointly explore their advantages. Zhang et al. [88] performed adaptation in both the visual appearance-level and representation-level. Leveraging the unpaired image-to-image translation framework [182], the method proposed by Murez et al. [89] requires that the extracted features are able to reconstruct the images in both domains. In addition, they also aligned the extracted features in both domains.
Finding invariant representations alone is clearly not a sufficient condition for the success of domain adaptation. Zhao et al. [90] gave a simple counterexample where invariant representations lead to large joint error on source and target domains. So far, most methods focus on covariate shift which occurs on standard datasets but fails in most practical applications. For instance, when transferring knowledge from synthetic to real images [160], the supports of the input distributions are actually disjoint. Similar to covariate shift, label shift is also a long-standing problem in machine learning, but only a few works in domain adaptation have focused on solving it until recently. In this line of work, [90‚Äì94] proposed generalization bounds for this scenario and focused on detection and alignment by estimating the density ratio P(ys)/P(yt) and doing importance re-sampling on Y space.
Recently, pseudo-labeling has been exploited in a number of DA methods. Pan et al. [69] and Hu et al. [64] assigned pseudo-labels to images in the target domain and then performed domain alignment based on prototypes. These methods are mostly used for image classification. Zou et al.
[95] proposed to utilize pseudo-labeling in self-training for semantic segmentation, in which pseudo-labels are generated from high-confidence predictions. However, since pseudolabels are noisy, overconfident label belief can be paced on wrong classes, leading to propagated errors. In order to solve this issue, Zou et al. [96] proposed a confidence regularized self-training framework, in which pseudo-labels are treated as continuous latent variables jointly optimized via alternating optimization. Label regularization and model regularization are proposed as two types of confidence regularizations.
Ensemble methods have also been used for DA. [97, 98] originally developed ensemble methods for semi-supervised learning. Laine et al. [97] performed ensembling by averaging over past predictions for each example, while Tarvainen et al.
[98] performed ensemble by leveraging past network weights.
These ensemble approaches require high randomness in either inputs or network models, which can be provided by data augmentation, varying augmentation parameters, and utilizing dropout. French et al. [99] extended these methods for unsupervised domain adaptation. Images are first processed with stochastic data augmentation and then fed into both networks.
The student network is trained with gradient descent while the teacher network updates its weights using an exponential moving average of the student network‚Äôs weights. Stochastic weight averaging further improves the adaptation results as shown in [100]. Recently, Cai et al. [123] proposed Mean Teacher with Object Relations (MTOR) for object detection which integrates object relations into the consistency cost between teacher and student modules.
Other techniques are quite different from what we have mentioned above. Zhang et al. [101] proposed a curriculumstyle learning approach to minimize the domain gap in semantic segmentation. This method solves easy tasks first in order to infer some necessary properties about the target domain and then the network predictions in the target domain are enforced to follow those inferred properties during the training process.
Wang et al. [102] proposed a Manifold Embedded Distribution UNDER REVIEW 11 TABLE VII COMPARISON OF DIFFERENT SINGLE-SOURCE DUDA CATEGORIES. (THE MORE STARS THE METHOD HAS, THE BETTER IT IS. ) Theory guarantee Efficiency Task scalability Data scalability Data dependency Optimizability Performance Discrepancy-based methods FFF FFF F FF FFF FFF FF Adversarial discriminative methods FF FF FFF FFF F F FFF Adversarial generative methods F F FF F F F FFF Self-supervision methods F FF FFF FFF FF FFF FF TABLE VIII PERFORMANCE COMPARISON (CLASSIFICATION ACCURACY IN %) OF DIFFERENT METHODS ON DIGIT DATASET FOR DIGIT RECOGNITION.
‚ÄòBACKBONE‚Äô DENOTES THE BASE NETWORK ARCHITECTURE, ‚ÄòM‚Äô, ‚ÄòM-M‚Äô, ‚ÄòU‚Äô, ‚ÄòS‚Äô ARE DIFFERENT DOMAINS (SEE SECTION III FOR DETAILS), AND ‚Äò‚Äì>‚Äô REPRESENTS THE ADAPTATION FROM ONE SOURCE DOMAIN TO ANOTHER TARGET DOMAIN. THE COLUMN ‚ÄòC‚Äô INDICATES WHICH CATEGORY THE METHOD BELONGS TO, WHERE ‚ÄòD‚Äô, ‚ÄòA‚Äô, ‚ÄòG‚Äô, ‚ÄòS‚Äô, ‚ÄòO‚Äô ARE RESPECTIVELY SHORT FOR DISCREPANCY-BASED, ADVERSARIAL DISCRIMINATIVE, ADVERSARIAL GENERATIVE, SELF-SUPERVISION-BASED METHODS, AND OTHERS (THE SAME BELOW).
Backbone Method Venue C M‚Äì>M-M M‚Äì>U U‚Äì>M S‚Äì>M M‚Äì>S AlexNet SWDA [48] TPAMI 2018 D - 60.7 67.3 - - Custom DANN [58] JMLR 2016 A 76.7 - - 73.9 - DRCN [79] ECCV 2016 S - 91.8 73.7 82.0 40.1 DSN [80] NeurIPS 2016 S 83.2 - - 82.7 - PixelDA [70] CVPR 2017 G 98.2 95.9 - - - UNIT [77] NeurIPS 2017 G - 96.0 93.6 90.5 - CyCADA [71] ICML 2018 G - 95.6 96.5 90.4 SEDA [99] ICLR 2018 O - 98.2 99.6 99.3 97.0 MCD [103] CVPR 2018 O - 94.2 94.1 96.2 - SWD [55] CVPR 2019 D - 98.1 97.1 98.9 - DWT [56] CVPR 2019 D - 99.1 98.8 97.8 28.9 RCA [63] ICCV 2019 A 99.5 - - 99.3 89.2 ResNet26 SSDA [81] arXiv 2019 S 98.9 96.5 90.2 85.8 61.3 ResNet50 CDAN [62] NeurIPS 2018 A - 95.6 98.0 89.2 - 3CATN [75] ACM MM 2019 G - 96.1 98.3 92.5 - LeNet ADDA [11] CVPR 2017 A - 89.4 90.1 76.0 - I2I [89] CVPR 2018 O - 98.8 97.6 90.1 - TPN [69] CVPR 2019 O - 92.1 94.1 93.0 - HoMM [50] AAAI 2020 D - - 99.1 99.0 - Alignment approach which learns a domain-invariant classifier in Grassmann manifold with structural risk minimization, while performing dynamic distribution alignment to quantitatively account for the relative importance of marginal and conditional distributions. Saito et al. [103] introduced a new approach that attempts to align the distributions of the source and target domains by utilizing the task-specific decision boundaries. They proposed to maximize the discrepancy between two classifiers‚Äô outputs to detect target samples that are far from the support of the source. Khodabandeh et al.
[124] recently proposed to address DA from the perspective of robust learning and showed that the problem may be ormulated as training with noisy labels. Chen et al. [104] proposed a target guided distillation approach to learn the real image style, which is achieved by training the segmentation model to imitate a pre-trained real style model using real images.
They further took advantage of the intrinsic spatial structure presented in urban scene images, and proposed a spatial-aware adaptation scheme to effectively align the distribution of two domains.
G. Qualitative Comparison (Table VII) To thoroughly review the various single-source DUDA methods, we use the following qualitative criteria: 1) Theory guarantee: if the target risk has upper bound; and if the TABLE IX PERFORMANCE COMPARISON (CLASSIFICATION ACCURACY IN %) OF DIFFERENT METHODS ON OFFICE-31 DATASETS FOR OBJECT CLASSIFICATION. ‚ÄòA‚Äô, ‚ÄòD‚Äô, ‚ÄòW‚Äô ARE DIFFERENT DOMAINS IN THE OFFICE-31 DATASET, AND ‚ÄòAVG‚Äô IS THE AVERAGE PERFORMANCE OF DIFFERENT ADAPTATION SETTINGS (THE SAME BELOW).
Backbone Method Venue C A‚Äì>W D‚Äì>W W‚Äì>D A‚Äì>D D‚Äì>A W‚Äì>A Avg AlexNet DANN [58] JMLR 2016 A 73.0 96.4 99.2 - - - - SWDA [48] TPAMI 2018 D 76.0 96.7 99.6 - - - - CORAL [47] DACVA 2017 D 66.4 95.7 99.2 66.8 52.8 51.5 72.1 DAN [46] JMLR 2015 D 68.5 96.0 99.0 67.0 54.0 53.1 72.9 DUCDA [36] ACM MM 2017 D 68.3 96.2 99.7 68.3 53.6 51.6 73.0 DRCN [79] ECCV 2016 S 68.7 96.4 99.0 66.8 56.0 54.9 73.6 JMMD [52] ICML 2017 D 75.2 96.6 99.6 72.8 57.5 56.3 76.3 AutoDIAL [54] ICCV 2017 D 75.5 96.6 99.5 73.6 58.1 59.4 77.1 CDAN [62] NeurIPS 2018 A 78.3 97.2 100.0 76.3 57.3 57.3 77.7 DM-ADA [66] AAAI 2020 A 83.9 99.8 99.9 77.5 64.6 64.0 81.6 ResNet-50 SSDA [83] Access 2019 S 88.6 98.0 100.0 85.7 68.0 65.5 84.3 JMMD [52] ICML 2017 D 86.0 96.7 99.7 85.1 69.2 70.7 84.6 HoMM [50] AAAI 2020 D 90.8 99.3 100.0 87.9 69.3 69.5 86.1 GTA [73] CVPR 2018 G 89.5 97.9 99.8 87.7 72.8 71.4 86.5 CRST [96] ICCV 2019 O 89.4 98.9 100.0 88.7 72.6 70.9 86.8 DAAA [72] ECCV 2018 G 86.8 99.3 100.0 88.8 74.3 73.9 87.2 CDAN [62] NeurIPS 2018 A 94.1 98.6 100.0 92.9 71.0 69.3 87.7 TAT [65] ICML 2019 A 92.5 99.3 100.0 93.2 73.1 72.1 88.4 AL2DA [68] AAAI 2020 A 95.6 97.7 100.0 94 72.2 72.5 88.7 3CATN [75] ACM MM 2019 G 95.3 99.3 100.0 94.1 73.1 71.5 88.9 PANDA [64] arXiv 2020 A 94.9 97.8 99.8 94.2 73.9 72.8 88.9 CAN [51] CVPR 2019 D 94.5 99.1 99.8 95 78.0 77.0 90.6 VGG16 CMD [49] ICLR 2017 D 77.0 96.3 99.2 79.6 63.8 63.3 79.9 Inception ABN-DA [53] PR 2018 D 75.4 96.2 99.6 72.7 59.0 60.5 77.2 AutoDIAL [54] ICCV 2017 D 84.2 97.9 99.9 82.3 64.6 64.2 82.2 ResNet-34 I2I [89] CVPR 2018 O 75.3 96.5 99.6 71.1 50.1 52.1 74.1 upper bound can be minimized by the algorithm. 2) Efficiency: the computation cost of the training and inference of the algorithm. 3) Task scalability: if the algorithm is applicable to complex tasks, such as semantic segmentation and object detection. 4) Data scalability: if the algorithm is applicable to large and complex datasets with rather diversified images.
1) Data dependency: if the algorithm can be well trained with small datasets. 6) Optimizability: if the algorithm is easy to train and requires less hyper-parameter tuning. 7) Performance: how well the algorithm performs.
Discrepancy-based methods usually define a distance measurement between the source and target distributions. Based on this definition, an upper bound of the target risk can be derived and domain adaptation algorithms can be designed to minimize this upper bound. Compared with other DUDA categories, many of the existing discrepancy-based methods have better theoretical guarantees. Since most discrepancy-based methods do not add significantly large blocks onto the backbone network, the whole network architectures are usually not very complicated. On the one hand, the computation efficiency of the discrepancy-based methods is usually higher than other categories and the training of the network does not highly rely on large datasets. On the other hand, these methods are not as applicable to large and complex datasets with more diversified images as other categories. In terms of optimizability, since the networks are not very complicated, they are easier to train and UNDER REVIEW 12 TABLE X PERFORMANCE COMPARISON (CLASSIFICATION ACCURACY IN %) OF DIFFERENT METHODS ON OFFICE-HOME DATASET FOR OBJECT CLASSIFICATION.
‚ÄòAR‚Äô, ‚ÄòCL‚Äô, ‚ÄòPR‚Äô, ‚ÄòRW‚Äô ARE DIFFERENT DOMAINS IN THE OFFICE-HOME DATASET.
BackBone Method Venue C Ar‚Äì>Cl Ar‚Äì>Pr Ar‚Äì>Rw Cl‚Äì>Ar Cl‚Äì>Pr Cl‚Äì>Rw Pr‚Äì>Ar Pr‚Äì>Cl Pr‚Äì>Rw Rw‚Äì>Ar Rw‚Äì>Cl Rw‚Äì>Pr Avg AlexNet CDAN [62] NeurIPS 2018 A 38.1 50.3 60.3 39.7 56.4 57.8 35.5 43.1 63.2 48.4 48.5 71.1 51.0 ResNet-50 HoMM [50] AAAI 2020 D - 64.7 71.8 - - 66.1 - - 74.5 - - 81.2 - DWT-MEC [56] CVPR 2019 D 50.3 72.1 77.0 59.6 69.3 70.2 58.3 48.1 77.3 69.3 53.6 82.0 65.6 CDAN [62] NeurIPS 2018 A 50.7 70.6 76.0 57.6 70.0 70.0 57.4 50.9 77.3 70.9 56.7 81.6 65.8 TAT [65] ICML 2019 A 51.6 69.5 75.4 59.4 69.5 68.6 59.5 50.5 76.8 70.9 56.6 81.6 65.8 AL2DA [68] AAAI 2020 A 53.7 70.1 76.4 60.2 72.6 71.5 56.8 51.9 77.1 70.2 56.3 82.1 66.6 PANDA [64] arXiv 2020 A 52.4 73.4 79.0 64.2 74.2 73.2 63.0 53.0 79.5 73.4 56.7 83.5 68.8 TABLE XI PERFORMANCE COMPARISON (CLASSIFICATION ACCURACY IN %) OF DIFFERENT METHODS ON VISDA-2017 DATASET FOR OBJECT CLASSIFICATION. THE SIMULATION DOMAIN (SIM) AND THE REAL-WORLD DOMAIN (REAL) ARE RESPECTIVELY USED AS SOURCE AND TARGET.
BackBone Method Venue C Sim->Real ResNet-50 DAN [46] ICML 2015 D 63.7 GTA [73] CVPR 2018 G 69.5 CDAN [62] NeurIPS 2018 A 70.0 TAT [65] ICML 2019 A 71.9 3CATN [75] ACM MM 2019 G 73.2 ResNet-101 DAN [46] ICML 2015 D 62.8 DM-ADA [66] AAAI 2020 A 75.6 SWD [55] CVPR 2019 D 76.4 CRST [96] ICCV2019 O 78.1 PANDA [64] arXiv 2020 A 78.3 self-ensembling [99] ICLR 2018 O 82.8 CAN [51] CVPR 2019 D 87.2 require less hyperparameter tuning. Most of the discrepancybased methods learn image-level representations, instead of pixel-level ones, thus they are not as applicable to complex tasks, such as semantic segmentation, as other categories. It is difficult for most discrepancy-based methods to achieve satisfying performance on complex datasets and tasks.
Adversarial discriminative approaches are the most widely used methods to solve DA problems and achieve remarkable results. Several theoretical studies on these methods focus on the investigation of generalization bound and risk analysis.
These methods have competitive computational efficiency and task scalability. In terms of data scalability, they work well across different kinds of datasets. Due to the reliance on the convergence of a min-max game between the discriminator and the feature extractor, they do not always work well on small datasets and are also relatively difficult to optimize.
There is usually no good theoretical support behind adversarial generative approaches since they mainly leverage GAN or other kinds of generative models to reduce the visual gap between source and target domains. However, they usually perform well on many complex tasks with high dimensional solution space, such as semantic segmentation and object detection. It is also because of their reliance on the generative models that they usually require the source and target domains to have homogeneous visual patterns and cannot easily scale to more complex datasets. Since they rely on generative models to build pattern transformation between source and target domains, they require large-scale datasets to robustly train the generative model. Correspondingly, these approaches also require more computing resources and a more complicated optimization process.
TABLE XII PERFORMANCE COMPARISON (IN %) OF DIFFERENT METHODS FROM CITYSCAPES TO KITTI FOR OBJECT DETECTION. THE 4TH TO THE 8TH COLUMNS INDICATE THE AVERAGE PRECISION (AP) FOR THE 5 DIFFERENT CLASSES, AND THE LAST COLUMN IS THE MEAN AVERAGE PRECISION (MAP).
BackBone Method Venue C Person Rider Car Truck Train mAP VGG-16 DAF [45] CVPR 2018 A 40.9 16.1 70.3 23.6 21.2 34.4 MDA [129] ICCVW 2019 A 53.3 24.5 72.2 28.7 25.3 40.7 CFFA [131] arXiv 2020 A 50.4 29.7 73.6 29.7 21.6 41.0 Despite the apparent difference, both discrepancy-based methods and adversarial methods can be understood as approaches that attempt to align the marginal feature distributions of both domains. While both methods are intuitive and have seen empirical success in several cases, fundamental limitation exists for both lines of work.
In a recent paper [90], the authors proved an informationtheoretic lower bound on the joint error of methods based on learning domain-invariant representations, showing that when the label distributions of the two domains differ, any algorithm has to achieve a large error on at least one of the two domains. Since only source error could be minimized due to the availability of labeled samples, this implies an increasing error on the target domain. Furthermore, the better the distribution alignment, the worse the joint error. In a concurrent work, (author?) [185] also identified the insufficiency of learning domain-invariant representation for successful adaptation. They further analyzed the information loss of non-invertible transformations, and proposed a generalization upper bound that directly takes it into account.
While most of the work we discussed so far focuses on learning domain-invariant representations, methods based on estimating the importance ratio of density functions between target and source domains are also abundant in the literature [28, 186‚Äì189]. Most of these approaches exhibit provable generalization guarantees under the covariate shift assumption.
An interesting avenue for future research is combining the distribution alignment method using deep networks for feature learning with importance ratio reweighting. Note that, different from traditional methods where the importance ratio is estimated between the data density functions, recent work has explored the alternative direction where the importance ratio between the marginal label distributions of the two domains is estimated instead [92, 94]. The fundamental limitation of domain-invariant representations is the potential discrepancy UNDER REVIEW 13 TABLE XIII PERFORMANCE COMPARISON (IN %) OF DIFFERENT METHODS FROM CITYSCAPES TO FOGGYCITYSCAPES FOR OBJECT DETECTION. THE 4TH TO THE 11ST COLUMNS INDICATE THE AVERAGE PRECISION (AP) FOR THE 8 DIFFERENT CLASSES.
BackBone Method Venue C Bus Bicycle Car Motor Person Rider Train Truck mAP ResNet-50 MTOR [123] CVPR 2019 O 38.6 35.6 44.0 28.3 30.6 41.4 40.6 21.9 35.1 Inception-v2 RLDA [124] ICCV 2019 O 45.3 36.0 49.2 26.9 35.1 42.2 27.0 30.0 36.5 VGG-16 DAF [45] CVPR 2018 A 35.3 27.1 40.5 20.0 25.0 31.0 20.2 22.1 27.6 SCDA [125] CVPR 2019 A 39.0 33.6 48.5 28.0 33.5 38.0 23.3 26.5 33.8 MAF [126] ICCV 2019 A 39.9 33.9 43.9 29.2 28.2 39.5 33.3 23.8 34.0 SWDA [127] CVPR 2019 A 36.2 35.3 43.5 30.0 29.9 42.3 32.6 24.5 34.3 DD-MRL [128] CVPR 2019 G 38.4 32.2 44.3 28.4 30.8 40.5 34.5 27.2 34.6 MDA [129] ICCVW 2019 A 41.8 36.5 44.8 30.5 33.2 44.2 28.7 28.2 36.0 PDA [130] WACV 2020 G 44.1 35.9 54.4 29.1 36.0 45.5 25.8 24.3 36.9 CFFA [131] arXiv 2020 A 43.2 37.4 52.1 34.7 34.0 46.9 29.9 30.8 38.6 between the marginal label distributions. To overcome such lower bound, one could use the importance ratio between label distributions to compensate for such label discrepancy, as explored in several recent work [91, 190].
Compared with other methods, self-supervision-based methods do not have a strong theoretical guarantee since these methods are mostly based on the intuition that by forcing the CNN encoder to perform the desired task on the source domain and the pretext tasks on the target domain, the CNN encoder could extract domain-invariant features for both. In terms of computation cost, self-supervision-based methods perform the self-supervision tasks with additional heads, which are normally light-weight CNNs. They normally have more computation cost than discrepancy-based methods, while having less computation cost than adversarial generative methods.
Self-supervision-based methods do not have assumptions on the downstream task, and are applicable to complex tasks.
In terms of data scalability, self-supervision-based methods are robust and applicable to complex datasets. The selfsupervision tasks are normally simple tasks which are easy to train along with the downstream task network. Finally, selfsupervision-based methods usually have better performance than discrepancy-based methods, but are less performant than adversarial discriminative and generative methods.
H. Quantitative Comparison (Table VIII to Table XV) In this section, we quantitatively compare different categories of single-source DUDA methods in three visual tasks, i.e. image classification, object detection, and semantic segmentation. First, we introduce detailed experimental settings, including datasets with their properties, and evaluation metrics.
Second, we analyze the results.
1) Image Classification: We compare the classification performance of different methods on 4 different datasets, Digit, Office-31, Office-Home, and VisDA-2017. The first three datasets contain several domains of images. A DA method is evaluated by performing adaptation from each domain to every other domain in the dataset, and averaging all adaptation performances. Classification accuracy is used as the evaluation metric.
Digit and Office-31 are relatively basic datasets for DA.
Because images in these datasets are mostly centered objects with simple backgrounds, many methods could achieve high adaptation accuracy, making it hard to compare them.
However, these datasets are still widely used since they are convenient for testing new ideas. Office-Home contains more domains (4) and the 12 source-to-target adaptation settings provide more diverse tests to mitigate the possibility of overfitting. VisDA-2017 is a challenging large-scale dataset with one simulation domain and one real-world domain.
2) Object Detection: We compare the detection performance of different methods on Cityscapes‚ÜíKITTI and Cityscapes‚ÜíFoggy Cityscapes. Each dataset contains bounding boxes of different categories. We use mean Average Precision (mAP) as the evaluation metric.
Cityscapes and KITTI are both real-world datasets, but collected from different cities. The scene layouts of the images in the two domains are different, which can test the ability to bridge the domain gap caused by both appearance and scene layout differences. Cityscapes and KITTI only have 5 shared categories in the adaptation setting. Foggy Cityscapes is a synthetic dataset simulating fog on Cityscapes images.
Cityscapes and Foggy Cityscapes have 8 classes of objects; since they have the same scene layouts, this adaptation task focuses on testing the appearance adaptation ability of a DA method.
3) Semantic Segmentation: We compare the segmentation performance of different methods on GTA‚ÜíCityscapes and SYNTHIA‚ÜíCityscapes. Mean intersection-over-union (mIoU) is utilized as the evaluation metric.
GTA and SYNTHIA are both synthetic datasets, while Cityscapes is a real-world dataset. Both GTA‚ÜíCityscapes and SYNTHIA‚ÜíCityscapes test the performance of simulation-toreal segmentation adaptation methods. Images in GTA and Cityscapes are taken from the dashcams, while images in SYNTHIA are taken from various points of view. Images in GTA have higher level of fidelity compared to images in SYNTHIA. Consequently, SYNTHIA has a larger domain gap than Cityscapes, and it can also test the adaptation method on the domain gap caused by different point of view angles.
4) Result Analysis: All the experiment result comparisons are shown in Table VIII, IX, X, XI (image classification); Table XII, XIII (object detection); and Table XIV, XV (semantic segmentation). For each backbone, the methods are sorted by average classification accuracy, mAP and mIoU.
The results show that, compared with object detection and semantic segmentation, it is easier for the methods under UNDER REVIEW 14 TABLE XIV PERFORMANCE COMPARISON (IN %) OF DIFFERENT METHODS FROM GTA TO CITYSCAPES FOR SEMANTIC SEGMENTATION. THE 4TH TO THE 22ND COLUMNS INDICATE THE CLASS-WISE INTERSECTION-OVER-UNION (CWIOU) FOR THE 19 DIFFERENT CLASSES, AND THE LAST COLUMN IS THE MEAN INTERSECTION-OVER-UNION (MIOU).
BackBone Method Venue C road sidewalk building wall fence pole traffic light traffic sign vegettion terrain sky person rider car truck bus train motorbike bicycle mIoU VGG16 FCN-Wild [57] arXiv 2016 A 70.4 32.4 62.1 14.9 5.4 10.9 14.2 2.7 79.2 21.3 64.6 44.1 4.2 70.4 8 7.3 0.0 3.5 0.0 27.1 MCD [103] CVPR 2018 O 86.4 8.5 76.1 18.6 9.7 14.9 7.8 0.6 82.8 32.7 71.4 25.2 1.1 76.3 16.1 17.1 1.4 0.2 0.0 28.8 CDA [101] ICCV 2017 O 74.9 22.0 71.7 6.0 11.9 8.4 16.3 11.1 75.7 13.3 66.5 38.0 9.3 55.2 18.8 18.9 0.0 16.8 14.6 28.9 AdaptSegNet [61] CVPR 2018 A 87.3 29.8 78.6 21.1 18,2 22.5 21.5 11.0 79.7 29.6 71.3 46.8 6.5 80.1 23.0 26.9 0.0 10.6 0.3 35.0 CyCADA [71] ICML 2018 G 85.2 37.2 76.5 21.8 15.0 23.8 22.9 21.5 80.5 31.3 60.7 50.5 9.0 76.9 17.1 28.2 4.5 9.8 0.0 35.4 ROAD [104] CVPR 2018 O 85.4 31.2 78.6 27.9 22.2 21.9 23.7 11.4 80.7 29.3 68.9 48.5 14.1 78.0 19.1 23.8 9.4 8.3 0.0 35.9 DCAN [76] ECCV 2018 G 82.3 26.7 77.4 23.7 20.5 20.4 30.3 15.9 80.9 25.4 69.5 52.6 11.1 79.6 24.9 21.2 1.3 17.0 6.7 36.2 CGAN [60] CVPR 2018 A 89.2 49.0 70.7 13.5 10.9 38.5 29.4 33.7 77.9 37.6 65.8 75.1 32.4 77.8 39.2 45.2 0.0 25.5 35.4 44.5 ResNet-101 DCAN [76] ECCV 2018 G 88.5 37.4 79.3 24.8 16.5 21.3 26.3 17.4 80.8 30.9 77.6 50.2 19.2 77.7 21.6 27.1 2.7 14.3 18.1 38.5 ROAD [104] CVPR 2018 O 76.3 36.1 69.6 28.6 22.4 28.6 29.3 14.8 82.3 35.3 72.9 54.4 17.8 78.9 27.7 30.3 4.0 24.9 12.6 39.4 UDA-SS [81] arXiv 2019 S 86.6 37.8 80.8 29.7 16.4 28.9 30.9 22.2 83.8 37.1 76.9 60.1 7.8 84.1 30.8 32.1 1.2 23.2 13.3 41.2 SSDA [83] Access 2019 S 87.6 25.7 77.5 19.8 16.8 29.0 32.1 20.5 79.9 32.9 75.3 58.2 26.0 79.0 23.3 31.6 2.1 26.9 37.7 41.2 AdaptSegNet [61] CVPR 2018 A 86.5 36.0 79.9 23.4 23.3 23.9 35.2 14.8 83.4 33.3 75.6 58.5 27.6 73.7 32.5 35.4 3.9 30.1 28.1 42.4 SWD [55] CVPR 2019 D 92.0 46.4 82.4 24.8 24.0 35.1 33.4 34.2 83.6 30.4 80.9 56.9 21.9 82.0 24.4 28.7 6.1 25.0 33.6 44.5 PANDA [64] arXiv 2020 A 92.4 51.3 82.9 31.8 24.9 32.6 35.8 20.4 84.5 38.7 79.8 60.0 25.8 85.1 33.7 44.1 9.0 27.5 22.6 46.5 FCAN [88] CVPR 2018 O - - - - - - - - - - - - - - - - - - - 47.8 DRN105 MCD [103] CVPR 2018 O 90.3 31.0 78.5 19.7 17.3 28.6 30.9 16.1 83.7 30.0 69.1 58.5 19.6 81.5 23.8 30.0 5.7 25.7 14.3 39.7 ResNet-34 I2I [89] CVPR 2018 O 85.3 38.0 71.3 18.6 16.0 18.7 12.0 4.5 72.0 43.4 63.7 43.1 3.3 76.7 14.4 12.8 0.3 9.8 0.6 31.8 ResNet-38 CBST [95] ECCV 2018 O 88.0 56.2 77.0 27.4 22.4 40.7 47.3 40.9 82.4 21.6 60.3 50.2 20.4 83.8 35.0 51.0 15.2 20.6 37.0 46.2 CRST [96] ICCV 2019 O 91.7 45.1 80.9 29.0 23.4 43.8 47.1 40.9 84.0 20.0 60.6 64.0 31.9 85.8 39.5 48.7 25.0 38.0 47.0 49.8 DRN26 CyCADA [71] ICML 2018 G 79.1 33.1 77.9 23.4 17.3 32.1 33.3 31.8 81.5 26.7 69.0 62.8 14.7 74.5 20.9 25.6 6.9 18.8 20.4 39.5 PSPNet DCAN [76] ECCV 2018 G 85.0 30.8 81.3 25.8 21.2 22.2 25.4 26.6 83.4 36.7 76.2 58.9 24.9 80.7 29.5 42.9 2.5 26.9 11.6 41.7 TABLE XV PERFORMANCE COMPARISON (IN %) OF DIFFERENT METHODS FROM SYNTHIA TO CITYSCAPES FOR SEMANTIC SEGMENTATION. THE 4TH TO THE 19TH COLUMNS INDICATE THE CWIOU FOR THE 16 DIFFERENT CLASSES, AND THE LAST TWO COLUMNS ARE THE MIOU OVER ALL THE 16 CLASSES AND OVER 13 CLASSES EXCLUDING THE 3 CLASSES MARKED WITH *.
BackBone Method Venue C road sidewalk building wall* fence* pole* traffic light traffic sign vegetation sky person rider car bus motorbike bicycle mIoU mIoU* VGG16 AdaptSegNet [61] CVPR 2018 A 78.9 29.2 75.5 - - - 0.1 4.8 72.6 76.7 43.4 8.8 71.1 16.0 3.6 8.4 - 37.6 FCN-Wild [57] arXiv 2016 A 11.5 19.6 30.8 4.4 0.0 20.3 0.1 11.7 42.3 68.7 51.2 3.8 54.0 3.2 0.2 0.6 17.0 22.9 CDA [101] ICCV 2017 O 65.2 26.1 74.9 0.1 0.5 10.7 3.7 3.0 76.1 70.6 47.1 8.2 43.2 20.7 0.7 13.1 29.0 34.8 DCAN [76] ECCV 2018 G 79.9 30.4 70.8 1.6 0.6 22.3 6.7 23.0 76.9 73.9 41.9 16.7 61.7 11.5 10.3 38.6 35.4 41.8 ROAD [104] CVPR 2018 O 77.7 30.0 77.5 9.6 0.3 25.8 10.3 15.6 77.6 79.8 44.5 16.6 67.8 14.5 7.0 23.8 36.2 41.7 CGAN [60] CVPR 2018 A 85.0 25.8 73.5 3.4 3.0 31.5 19.5 21.3 67.4 69.4 68.5 25.0 76.5 41.6 17.9 29.5 41.2 47.8 VGG16-Dilated FCN NMD [59] ICCV 2017 A 62.7 25.6 78.3 - - - 1.2 5.4 81.3 81.0 37.4 6.4 63.5 16.1 1.2 4.6 - 35.7 ResNet-38 CBST [95] ECCV 2018 O 53.6 23.7 75.0 12.5 0.3 36.4 23.5 26.3 84.8 74.7 67.2 17.5 84.5 28.4 15.2 55.8 42.5 48.5 ResNet-101 AdaptSegNet [61] CVPR 2018 A 84.3 42.7 77.5 - - - 4.7 7.0 77.9 82.5 54.3 21.0 72.3 32.2 18.9 32.3 - 46.7 SWD [55] CVPR 2019 D 82.4 33.2 82.5 - - - 22.6 19.7 83.7 78.8 44.0 17.9 75.4 30.2 14.4 39.9 - 48.1 PANDA [64] arXiv 2020 A 88.1 44.2 81.1 - - - 10.0 11.1 80.3 84.3 42.8 21.6 82.5 34.6 16.9 38.7 - 48.9 DCAN [76] ECCV 2018 G 81.5 33.4 72.4 7.9 0.2 20.0 8.6 10.5 71.0 68.7 51.5 18.7 75.3 22.7 12.8 28.1 36.5 42.7 CRST [96] ICCV 2019 O 67.7 32.2 73.9 10.7 1.6 37.4 22.2 31.2 80.8 80.5 60.8 29.1 82.8 25.0 19.4 45.3 43.8 50.1 PSPNet DCAN [76] ECCV 2018 G 82.8 36.4 75.7 5.1 0.1 25.8 8.0 18.7 74.7 76.9 51.1 15.9 77.7 24.8 4.1 37.3 38.4 44.9 analysis to achieve better performance on the image classification task. Since classification is a relatively simple task, not requiring many local details for the global class prediction, no specific category performs significantly better than the others. For object detection and semantic segmentation, most of the published work utilize adversarial discriminative or adversarial generative methods since these two tasks require massive detailed local information about the images. Adversarial learning-based methods are powerful in performing local feature alignment while discrepancy-based and selfsupervision-based methods are less capable of capturing local information, leading to inferior performance on object detection and semantic segmentation tasks.
V. FUTURE DIRECTIONS Existing DUDA methods have achieved promising performance on many computer vision tasks, such as object classification and semantic segmentation. However, there is still a large performance gap between existing methods and the upper bound (train and test both on target domain). To help address the remaining challenges, we provide some possible improvements over the state-of-the-art methods. In addition,we present more practical settings of DA, new applications of DA and brave new perspectives in DA.
A. New Methodologies of DA Incorporating Previous Knowledge. As domain shift is usually caused by the imaging process, such as illumination changes [26], incorporating prior knowledge into the adaptation process may lead to a performance increase. For UNDER REVIEW 15 adversarial methods, imposing multi-level constraints jointly in the adaptation, such as low-level appearances, mid-level features, and high-level semantics, can better preserve the structure and attributes of the source data and thus perform better. Designing an effective and direct metric to evaluate the quality of adaptation, instead of testing the performance on the target domain, would accelerate the training process of GANs.
Meta Learning Across Domains. Meta learning algorithms provides a learning to learn paradigm that is effective at learning meta models with the capability of solving new tasks in a fast manner. However, they require sufficient tasks for meta model training and the optimized model can only solve new tasks similar to the training ones. These limitations make them suffer from performance decline in presence of insufficiency of training tasks in the target domains and task heterogeneity, where the source tasks present different characteristics from the target tasks [191]. Besides the above challenge, there may be data distribution shift between the source tasks and target tasks, exposing more severe challenges to existing meta learning algorithms. Cross-domain meta learning provides promising solutions to address these challenges by essentially learning more transferable representations [153, 192].
Contrastive Learning for DA. DUDA methods [193, 194] are recently focusing on the disentanglement [195] of the features into domain-invariant and domain-specific ones based on data variations. Domain-invariant features play an important part in reducing the noisy information from each domain, thus making learned features discriminative of the category.
Current approaches of contrastive learning for domain adaptation are highly dependent on the design of specific tasks.
For example, different DA tasks may rely on different pretext tasks. Therefore, a potential research direction is to design a common pretext task. These methods are often criticized for their computational cost since a large number of negative samples have to be selected for comparison with every single positive sample. Thus, an approach to decrease computational complexity is needed.
B. More Practical Settings of DA Multi-modal DA. The labeled source domains may contain multi-modal data. For example, synthetic data generated by simulators (CARLA and GTA-V, etc.) may be of different modalities, such as LiDAR, RaDAR, and image. Other examples include the audio channel and visual channel of videos and the textual and visual information of social posts. Similar to multi-modal recognition [196, 197] and feature-level fusion in image classification and retrieval [198, 199], we believe that jointly combining and fusing different modalities to explore the combinations would boost the performance of DA. Another advantage of multi-modal DA is that even if some modalities are missing, the DA system can still work by leveraging information from other available modalities. For example, while the cameras for autonomous driving cannot capture images well at night, the LiDAR scanners are robust under almost all lighting conditions [4]. How to design effective fusion strategies is the main challenge. The simplest ways are to directly employ early fusion at the feature level or late fusion at the decision level. But to deal well with the incomplete data issue, fusion at the model-level, such as graph convolutional network [200], is probably a better choice.
Multi-task DA. To the best of our knowledge, all the domain adaptation methods proposed so far only focus on a single task (e.g. semantic segmentation, robotic grasping, image classification) with single-modal input (e.g. images).
However, in many scenarios, several tasks need to be performed on the same data simultaneously (e.g. semantic segmentation and traffic sign identification for a driving image).
Separately adapting each task would be redundant in terms of computation, since the networks for both models may rely on the same set of features. So how to adapt multiple tasks simultaneously and efficiently is a promising direction to explore. One straightforward solution is to find a common feature representation that is beneficial for all the tasks. In order to guide learning towards an optimal shared feature space, methods based on adversarial learning may be used with novel designs.
Continual Learning and Adaptation. Many machine learning models (e.g. semantic segmentation models) are trained on a fixed dataset and then deployed onto a real system, with the assumption that the data at test time has a similar distribution as the training data. However, this is often not the case. Imagine a segmentation model trained on images taken in the US with mostly sunny weather conditions. The cars with the trained model are sold all over the world, and different cars will be running under different domains (e.g.
different cities, weathers, time of day, etc.). In order for the network to perform well all the time, continual learning and adaptation needs to be performed. Basically, the network is expected to have the ability to learn continually from a steam of experiential data, building on what has been learned previously, and adapting to varying new domains [201]. Some methods [202‚Äì204] try to limit the extent of weight sharing across experiences by focusing on preserving past knowledge.
A method is proposed in [205] to adapt to changing environments for semantic segmentation. However, the method requires synthesizing new images on the fly, which is not computationally efficient. Methods such as [206] may be used to find a compact representation of the whole dataset, which may be more efficient to fine-tune the model without forgetting the learned knowledge. Learning representations that are generalizable to different domains could make the network more robust against target domain change. [207, 208] proposed to use style transfer to randomize the input domains for better generalization performance.
Federated Domain Adaptation. Data generated by IoT devices poses unique challenges for training machine learning models. Users√¢AÀò Z profile data typically contains sensitive ¬¥ information, thus cannot leave its hosting device for the sake of privacy preservation. Due to the growing storage and computational power of these devices and concerns about data privacy, it is increasingly preferable to store them in a decentralized way on individual devices rather than hosting them in a central storage. Federated Learning (FL) provides a privacy-preserving mechanism to leverage such decentralized data and computation resources to train machine learning UNDER REVIEW 16 models. The main idea behind federated learning is to have each node learn on its own local data and not share either the data or the model parameters. FL improves data privacy and efficiency in machine learning performed over networks of distributed devices, such as mobile phones, IoT and wearable devices, etc. While FL achieves better privacy and efficiency, existing methods ignore the fact that the data on each node are collected in a non-i.i.d manner, leading to domain shift between nodes [193]. Models trained with federated learning can still fail to generalize to new devices due to the problem of domain shift. Thus it is of great importance to develop domain adaptation algorithms for federated learning [209].
Such algorithms should be able to align the representations learned among different source and target devices.
DA on the Edge. Nowadays, many vision-based perception models are deployed in edge devices, e.g. mobile phones, autonomous cars, and security cameras. These edge devices are usually deployed in different environments, with substantial need for domain adaptation. Different networks need to be personalized via learning on the users‚Äô private data. Sending all the user data to the server, and training millions of networks for all the users would be very expensive. Instead, training networks on the device not only decreases computational complexity, but it also protects privacy since the collected data need not leave the device. While the edge devices normally have a limited budget in terms of computation and power, almost all the current state-of-the-art DA methods, e.g. the adversarial generative methods, require training on high-end GPUs for days. Invertible neural networks [210, 211] are beneficial to mitigate the memory limitation problem. Other methods, such as quantization, pruning, neural architecture search, and software-hardware co-design, can also be used for efficient on-device training. Performing DA on the edge with efficient deep learning techniques is a practical and fruitful research area to explore.
C. New Applications of DA Robotics. Reinforcement learning (RL) algorithms are typically trained in simulation environments. There are two main reasons for this: first, RL algorithms normally require many interactions with the environment, while getting data from the real-world is relatively slow compared to simulation environments that can be sped up; second, training an agent in the real-world would damage the environment as well as the agent itself especially when the policy is not fully well learned. However, if we want to apply the policy learned in simulation into real-world, the domain difference needs to be handled. Methods such as domain randomization [212] have been proposed to mitigate the visual difference of the domains.
Normally, the source and target environments/domains are similar in terms of dynamics of both the environments and agents. An interesting direction is how to transfer if we know the detailed difference of the dynamics.
Video Analysis. Current methods mainly focus on adapting images from the source domain to the target domain. Adapting videos is more challenging and worth studying. Effectively exploring the temporal correlation of videos may significantly improve the DA performance. Existing video style transfer methods [213‚Äì215] may fail to work for DA, since the semantics of generated videos cannot be guaranteed to be preserved.
Imposing some semantic constraint may help to solve this problem. Further, maintaining the temporal consistency [216] is an important factor. Audio is also an important channel in videos, which is not considered in these methods.
Subjective Attributes. Existing DA methods work on objective tasks, such as object classification and semantic segmentation, while the adaptation for the understanding of subjective attributes, such as personality [217], aesthetics and emotions [218, 219], has been rarely explored. There are many other challenges to adapt these subjective attributes.
Take visual emotion for example: although the transferred images with pixel-level alignment may not change the highlevel semantics, the emotion may still be changed [6, 7].
Employing emotion-specific distance measure, such as Mikels‚Äô emotion wheel [7], may help to tackle this problem. Further, emotions may be evoked by different features, such as lowlevel artistic elements (e.g. color) for abstract paintings and high-level semantics for natural images [199]. First determining the image style and then conducting adaptation with corresponding semantic consistency may perform better.
D. Brave New Perspectives DA in the Wild. So far all the domain adaptation works mainly focus on a neat setting, however, domain adaptation problems in the real world can be a pretty complex combination of different ‚Äúclean‚Äù settings. For example, in a practical domain adaptation setting, there may be several source domains available: some source domains have no labeled examples, some have few labeled samples, and some have abundant labeled samples. At the same time, the label spaces of the source domains and target domains may not be exactly the same. There may also be multiple target domains, with some target domains that have classes not existing in any source domains. Solving practical DA problems in the real-world remains an under-explored field.
Model Robustness of DA. Due to the wide success of deep neural networks and their unexpected vulnerability of adversarial examples, there has been much attention placed on evaluating and quantifying the robustness of neural networks [220, 221]. However, all the current DA work only focus on boosting the performance on the target domain, without any consideration on the robustness of the adapted model. Investigating how to perform domain adaptation while maintaining the robustness of the model on the target domain is an interesting direction to explore.
Neural Architecture Search for DA. Existing domain adaptation models usually manually design a specific neural network architecture based the proposed algorithm. However, there is not much work to automatically learn the optimal network architecture to address the domain shift issue. Neural architecture search (NAS) [222] is an emerging direction that automatically looks for the optimal neural network architecture for better performance or higher computational efficiency.
With the success of NAS, we suggest the research on automatically learning optimal network architecture that can be UNDER REVIEW 17 adapted to different domains. For instance, when detecting vehicles from traffic videos, the model can automatically and dynamically learn different network architectures for videos from different weather, e.g., sunny, rainy, cloudy, and snowy or different locations, e.g., London, New York, Rome and Tokyo. With different network architectures, the model can learn better generalized representation to different domains.
Learning Common Sense for DA. Most of the existing domain adaptation models try to learn a generalized representation between the source and target domains. However, they do not discover the knowledge behind the visual tasks. We argue that human beings have better domain generalization capability because they can learn the ‚Äúcommon sense‚Äù behind the tasks and infer prediction in different domains. To imitate the human√¢AÀò Zs capability in domain generalization, we suggest ¬¥ to investigate learning ‚Äúcommon sense‚Äù for domain adaptation.
For instance, when the model learns that a computer screen is usually placed on a desk, it can have better performance when detecting the computer screen and the desk, no matter under what illumination, colorization, and camera views. By learning ‚Äúcommon sense‚Äù, models can be better generalized to different domains.
VI. CONCLUSION This paper provides an overview of recent developments in deep unsupervised domain adaptation (DUDA) with the intent of offering a tool for researchers and practitioners to obtain a perspective on the field. Because of the vast literature on the subject, we decided to focus on homogeneous, single-source, single-target, strongly-supervised, and closed-set settings. We classified these methods into different categories, summarized the representative ones, and compared them, supported by experimental results. We believe that DUDA will continue to be an active and promising research area. We also suggested a number of research directions with a discussion of their main challenges and of some possible solutions.


</div>
</details>

<details>
<summary>KOR</summary>
<div markdown="1">


 
</div>
</details>
